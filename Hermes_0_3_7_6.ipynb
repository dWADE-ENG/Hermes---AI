{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WR_rxRE2VCL3"
      },
      "source": [
        "# HERMES 0.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2Daw6wtpWUoF"
      },
      "outputs": [],
      "source": [
        "#import signal\n",
        "#os.kill(os.getpid(), signal.SIGKILL)\n",
        "#if \"COLAB_GPU\" in os.environ or \"COLAB_BACKEND_VERSION\" in os.environ:\n",
        "#    os.system(\"rm -rf /content/*\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pj_6OIcsWjRn",
        "outputId": "2414953e-f0f4-4ea3-9152-6ac7b3f04401"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.12/dist-packages (2.0.8)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Collecting segmentation-models-pytorch\n",
            "  Downloading segmentation_models_pytorch-0.5.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.12/dist-packages (from albumentations) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from albumentations) (1.16.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from albumentations) (6.0.3)\n",
            "Requirement already satisfied: pydantic>=2.9.2 in /usr/local/lib/python3.12/dist-packages (from albumentations) (2.11.10)\n",
            "Requirement already satisfied: albucore==0.0.24 in /usr/local/lib/python3.12/dist-packages (from albumentations) (0.0.24)\n",
            "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.12/dist-packages (from albumentations) (4.12.0.88)\n",
            "Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.12/dist-packages (from albucore==0.0.24->albumentations) (4.2.3)\n",
            "Requirement already satisfied: simsimd>=5.9.2 in /usr/local/lib/python3.12/dist-packages (from albucore==0.0.24->albumentations) (6.5.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.24 in /usr/local/lib/python3.12/dist-packages (from segmentation-models-pytorch) (0.36.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from segmentation-models-pytorch) (11.3.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.12/dist-packages (from segmentation-models-pytorch) (0.6.2)\n",
            "Requirement already satisfied: timm>=0.9 in /usr/local/lib/python3.12/dist-packages (from segmentation-models-pytorch) (1.0.22)\n",
            "Requirement already satisfied: torch>=1.8 in /usr/local/lib/python3.12/dist-packages (from segmentation-models-pytorch) (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (3.4.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (25.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (2.32.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9.2->albumentations) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9.2->albumentations) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9.2->albumentations) (0.4.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8->segmentation-models-pytorch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8->segmentation-models-pytorch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (2025.10.5)\n",
            "Downloading segmentation_models_pytorch-0.5.0-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: segmentation-models-pytorch\n",
            "Successfully installed segmentation-models-pytorch-0.5.0\n",
            "Requirement already satisfied: docutils<0.22,>=0.20 in /usr/local/lib/python3.12/dist-packages (0.21.2)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m129.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m149.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m570.5/570.5 kB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sphinx 8.2.3 requires docutils<0.22,>=0.20, but you have docutils 0.19 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting rasterio\n",
            "  Downloading rasterio-1.4.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.1 kB)\n",
            "Collecting affine (from rasterio)\n",
            "  Downloading affine-2.4.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.12/dist-packages (from rasterio) (25.4.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from rasterio) (2025.10.5)\n",
            "Requirement already satisfied: click>=4.0 in /usr/local/lib/python3.12/dist-packages (from rasterio) (8.3.0)\n",
            "Collecting cligj>=0.5 (from rasterio)\n",
            "  Downloading cligj-0.7.2-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.12/dist-packages (from rasterio) (2.0.2)\n",
            "Collecting click-plugins (from rasterio)\n",
            "  Downloading click_plugins-1.1.1.2-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from rasterio) (3.2.5)\n",
            "Downloading rasterio-1.4.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (22.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.3/22.3 MB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cligj-0.7.2-py3-none-any.whl (7.1 kB)\n",
            "Downloading affine-2.4.0-py3-none-any.whl (15 kB)\n",
            "Downloading click_plugins-1.1.1.2-py2.py3-none-any.whl (11 kB)\n",
            "Installing collected packages: cligj, click-plugins, affine, rasterio\n",
            "Successfully installed affine-2.4.0 click-plugins-1.1.1.2 cligj-0.7.2 rasterio-1.4.3\n"
          ]
        }
      ],
      "source": [
        "!pip install albumentations tqdm segmentation-models-pytorch torchvision\n",
        "!pip install --upgrade \"docutils>=0.20,<0.22\"\n",
        "!pip install awscli -q\n",
        "!pip install rasterio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBVFudt5U7Y5"
      },
      "source": [
        "## IMPORTS AND DEPENDENCIES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "J00SrcNPVyO6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "from torch.nn.parallel import DataParallel\n",
        "\n",
        "#Computer Vision and image processing\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "#Torchvision for pretrained models and transforms\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.models.segmentation import deeplabv3_resnet50, deeplabv3_resnet101\n",
        "#Data augmentation library\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import segmentation_models_pytorch as smp\n",
        "\n",
        "#Utilities\n",
        "import gc\n",
        "import os\n",
        "import glob\n",
        "import time\n",
        "import zipfile\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import json\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "import rasterio as rio\n",
        "from tqdm.auto import tqdm\n",
        "from itertools import cycle\n",
        "from functools import lru_cache\n",
        "from collections import defaultdict\n",
        "from datetime import timedelta\n",
        "import psutil\n",
        "\n",
        "os.environ.update({\n",
        "    \"GDAL_DISABLE_READDIR_ON_OPEN\": \"EMPTY_DIR\",\n",
        "    \"GDAL_CACHEMAX\": \"2048\",\n",
        "    \"CPL_VSIL_CURL_ALLOWED_EXTENSIONS\": \".tif,.tiff\"\n",
        "})\n",
        "\n",
        "#Optional: Weights & Biases for experiment tracking\n",
        "#try:\n",
        "  #import wandb\n",
        "  #WANDB_AVAILABLE = True\n",
        "#except ImportError:\n",
        "  #WANDB_AVAILABLE = False\n",
        "  #print(\"Warning: Weights & Biases not installed. Please install with `pip install wandb`\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iT9TVIsFU3DE"
      },
      "source": [
        "## Config\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xBet-zahUlez"
      },
      "outputs": [],
      "source": [
        "#Set random seeds\n",
        "def set_seed(seed=42):\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "  torch.backends.cuda.matmul.allow_tf32 = True\n",
        "  torch.backends.cudnn.allow_tf32 = True\n",
        "  torch.backends.cudnn.benchmark = True\n",
        "\n",
        "def prepare_model(model, config=None):\n",
        "    \"\"\"Prepare model for optimized training.\"\"\"\n",
        "    model = model.to(memory_format=torch.channels_last)\n",
        "\n",
        "    if config and getattr(config, \"compile_model\", False) and hasattr(torch, \"compile\"):\n",
        "        try:\n",
        "            model = torch.compile(model)\n",
        "            print(\"✓ Model compiled via torch.compile()\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Skipping compile due to: {e}\")\n",
        "    else:\n",
        "        print(\"ℹ️ Skipping torch.compile (disabled or unsupported).\")\n",
        "\n",
        "    return model\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "#Configuration and Parameters\n",
        "\n",
        "#Optimized for A100\n",
        "class Config:\n",
        "  #model architecture\n",
        "  backbone = 'resnet101' # May later upgrade to resnet152 or Vision Transformer\n",
        "  num_classes_flood = 2\n",
        "  num_classes_damage = 4\n",
        "\n",
        "  mixed_precision = True # Essential for A100\n",
        "\n",
        "  #training parameters\n",
        "  batch_size = 32\n",
        "  accumulation_steps = 4\n",
        "  effective_batch_size = batch_size * accumulation_steps\n",
        "  num_epochs = 50\n",
        "  learning_rate = 1e-4\n",
        "  weight_decay = 1e-4\n",
        "  gradient_clip = 1.0\n",
        "  warmup_epochs = 2\n",
        "\n",
        "\n",
        "  # loss weights\n",
        "  ce_weight = 0.3\n",
        "  dice_weight = 0.4\n",
        "  focal_weight = 0.3\n",
        "\n",
        "  # task weights for multi-tasking learning\n",
        "  flood_task_weight = 0.6\n",
        "  damage_task_weight = 0.4\n",
        "\n",
        "  #Memory Management\n",
        "  gradient_checkpointing = False\n",
        "  empty_cache_freq = 50\n",
        "  compile_model = True\n",
        "\n",
        "  #Multi-Scale Training\n",
        "  multi_scale_training = True\n",
        "  scale_range = (0.75, 1.25)\n",
        "\n",
        "\n",
        "  # Create directories\n",
        "  def __init__(self, base_dir=\"/content\", use_tf32=True):\n",
        "    #Core device and optimization setting\n",
        "    self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    self.use_tf32 = use_tf32\n",
        "\n",
        "    #data loading\n",
        "    self.gradient_checkpoint = True\n",
        "    self.num_workers = 0\n",
        "    self.pin_memory = False\n",
        "    self.prefetch_factor = 4\n",
        "    self.persistent_workers = True\n",
        "    self.spacenet_cache_size=50\n",
        "    self.preload_count = 10\n",
        "\n",
        "    # image dimensions\n",
        "    self.tile_size = 512\n",
        "\n",
        "    #Directory structure\n",
        "    base = Path(base_dir)\n",
        "    self.data_root = base / \"data\"\n",
        "    self.checkpoint_dir = base / \"checkpoints\"\n",
        "    self.cache_dir = base / \"cache\"\n",
        "    self.results_dir = base / \"results\"\n",
        "\n",
        "\n",
        "    #Directory creation\n",
        "    for d in [self.data_root, self.checkpoint_dir, self.cache_dir, self.results_dir]:\n",
        "        d.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "\n",
        "\n",
        "    # Enable A100-specific optimizations\n",
        "    if torch.cuda.is_available():\n",
        "      # Enable TF32 for A100\n",
        "      torch.backends.cuda.matmul.allow_tf32 = self.use_tf32\n",
        "      torch.backends.cudnn.allow_tf32 = self.use_tf32\n",
        "\n",
        "      # Set cudnn benchmarking for A100\n",
        "      torch.backends.cudnn.benchmark = True\n",
        "      torch.backends.cudnn.enabled = True\n",
        "\n",
        "      # Check if we actually have an A100\n",
        "      gpu_name = torch.cuda.get_device_name(0)\n",
        "      if 'A100' in gpu_name:\n",
        "          print(f\"✓ A100 detected: {gpu_name}\")\n",
        "          print(f\"  Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "      else:\n",
        "          print(f\"⚠ Warning: GPU is {gpu_name}, not A100. Adjusting config...\")\n",
        "          self.batch_size = 8\n",
        "          self.tile_size = 512\n",
        "    else:\n",
        "      print(\"⚠ Warning: No GPU detected. Adjusting config...\")\n",
        "\n",
        "    print(f\"Directories initialized in {base_dir}\")\n",
        "    print(f\"Device: {self.device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Tny2SCTyFLY0",
        "outputId": "a7fc0c86-1dc8-42dc-a5bd-febfef308fb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         13.06G 100%    2.43MB/s    1:25:30 (xfr#7031, to-chk=0/7046)\n",
            "         23.73G 100%    3.47MB/s    1:48:37 (xfr#8989, to-chk=0/8999)\n",
            "          1.68G 100%    6.20MB/s    0:04:19 (xfr#610, to-chk=0/618)\n"
          ]
        }
      ],
      "source": [
        "#Mapping Datasets\n",
        "\n",
        "# Create destination directories\n",
        "!mkdir -p /content/data/FloodNet\n",
        "!mkdir -p /content/data/RescueNet\n",
        "!mkdir -p /content/data/SpaceNet\n",
        "\n",
        "# Copy datasets from Google Drive to Colab local SSD (with progress bar)\n",
        "!rsync -ah --info=progress2 \"/content/drive/MyDrive/G.E.M.S./FloodNet/\" \"/content/data/FloodNet/\"\n",
        "!rsync -ah --info=progress2 \"/content/drive/MyDrive/G.E.M.S./RescueNet/\" \"/content/data/RescueNet/\"\n",
        "!rsync -ah --info=progress2 \"/content/drive/MyDrive/G.E.M.S./sn8/\" \"/content/data/SpaceNet/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "IVeWK3MCoo1X"
      },
      "outputs": [],
      "source": [
        "#Pulling FloodNet dataset from GitHub DropBox due to file size at ~12GB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "-aG659vxGq-t"
      },
      "outputs": [],
      "source": [
        "#!wget \"https://www.dropbox.com/scl/fo/k33qdif15ns2qv2jdxvhx/ANGaa8iPRhvlrvcKXjnmNRc?rlkey=ao2493wzl1cltonowjdbrnp7f&e=3&st=6lg4ncwc&dl=1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "WWkKOToKG0fn"
      },
      "outputs": [],
      "source": [
        "# import zipfile\n",
        "\n",
        "# zip_path = '/content/ANGaa8iPRhvlrvcKXjnmNRc?rlkey=ao2493wzl1cltonowjdbrnp7f&e=3&st=6lg4ncwc&dl=1'\n",
        "# extract_path = '/content/FloodNet'\n",
        "\n",
        "# with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "#   zip_ref.extractall(extract_path)\n",
        "\n",
        "#   print(\"Extraction complete.\")\n",
        "#   print(os.listdir('/content/FloodNet'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "G09dsszhHBUZ"
      },
      "outputs": [],
      "source": [
        "#Defining dataset pathways\n",
        "FloodNet_train_img_dir = '/content/data/FloodNet/FloodNet-Supervised_v1.0/train/train-org-img'\n",
        "FloodNet_train_mask_dir = '/content/data/FloodNet/FloodNet-Supervised_v1.0/train/train-label-img'\n",
        "\n",
        "\n",
        "FloodNet_val_img_dir ='/content/data/FloodNet/FloodNet-Supervised_v1.0/val/val-org-img'\n",
        "FloodNet_val_mask_dir = '/content/data/FloodNet/FloodNet-Supervised_v1.0/val/val-label-img'\n",
        "\n",
        "FloodNet_test_img_dir = '/content/data/FloodNet/FloodNet-Supervised_v1.0/test/test-org-img'\n",
        "FloodNet_test_mask_dir = '/content/data/FloodNet/FloodNet-Supervised_v1.0/test/test-label-img'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "55E7uZP5Ikmy"
      },
      "outputs": [],
      "source": [
        "# os.remove(\"/content/ANGaa8iPRhvlrvcKXjnmNRc?rlkey=ao2493wzl1cltonowjdbrnp7f&e=3&st=6lg4ncwc&dl=1\")\n",
        "# print(\"Zip File Deleted\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "q5htIfoogw0N"
      },
      "outputs": [],
      "source": [
        "#with zipfile.ZipFile(\"/content/drive/MyDrive/G.E.M.S./.zip\", \"r\") as zip_ref:\n",
        "  #zip_ref.extractall(\"/content/SN8\")\n",
        "\n",
        "#print(\"Extraction complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "yB7J5MuAhM0W"
      },
      "outputs": [],
      "source": [
        "SpaceNet8_train_img_dir = '/content/data/SpaceNet/images/train'\n",
        "SpaceNet8_train_mask_dir = '/content/data/SpaceNet/masks/train'\n",
        "\n",
        "SpaceNet8_val_img_dir = '/content/data/SpaceNet/images/val'\n",
        "SpaceNet8_val_mask_dir = '/content/data/SpaceNet/masks/val'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "WLW2rNaPIpWN"
      },
      "outputs": [],
      "source": [
        "#!pip install gdown\n",
        "# import gdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "UX04bC9zIszd"
      },
      "outputs": [],
      "source": [
        "#!gdown --fuzzy \"https://drive.google.com/file/d/1iRkEX9LQ8Hi-38QMyaReFJ8wXDDyYJAg/view?usp=sharing\" -O RescueNet.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "atY77jh0DhDA"
      },
      "outputs": [],
      "source": [
        "# import zipfile\n",
        "\n",
        "# with zipfile.ZipFile(\"/content/drive/MyDrive/G.E.M.S./RescueNet.zip\", \"r\") as zip_ref:\n",
        "#      zip_ref.extractall(\"/content/RescueNet\")\n",
        "# \\\n",
        "# print(\"Extraction complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMT0inR5dj39",
        "outputId": "ab6a9e53-ba5d-4572-e8cf-ec65aabd9516"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['FloodNet', 'RescueNet', 'SpaceNet']\n"
          ]
        }
      ],
      "source": [
        "print(os.listdir('/content/data'))\n",
        "#print(os.listdir('/content/RescueNet'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "yTDX_9BiD8hw"
      },
      "outputs": [],
      "source": [
        "RescueNet_train_img_dir = '/content/data/RescueNet/train/train-org-img'\n",
        "RescueNet_train_mask_dir = '/content/data/RescueNet/train/train-label-img'\n",
        "\n",
        "RescueNet_val_img_dir = '/content/data/RescueNet/val/val-org-img'\n",
        "RescueNet_val_mask_dir = '/content/data/RescueNet/val/val-label-img'\n",
        "\n",
        "RescueNet_test_img_dir = '/content/data/RescueNet/test/test-org-img'\n",
        "RescueNet_test_mask_dir = '/content/data/RescueNet/test/test-label-img'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "MXb9EZZCEbE7"
      },
      "outputs": [],
      "source": [
        "#os.remove(\"/content/RescueNet.zip\")\n",
        "#print(\"Files Deleted\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "uNOwuNO5YDt3"
      },
      "outputs": [],
      "source": [
        "sn8_path = \"/content/data/SpaceNet\"\n",
        "sn8_train_manifest = \"/content/data/SpaceNet/manifests/train_manifest.csv\"\n",
        "sn8_val_manifest = \"/content/data/SpaceNet/manifests/val_manifest.csv\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmUApdU4TuzB"
      },
      "source": [
        "## Loading Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "LvMldvFTE2SU"
      },
      "outputs": [],
      "source": [
        "class FloodNetDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset class for FloodNet flood detection data\n",
        "\n",
        "    Expected file structure:\n",
        "    - Images: original disaster images\n",
        "    - Masks: binary masks (0: non-flooded, 1: flooded)\n",
        "    \"\"\"\n",
        "    FLOODNET_COLORS = {\n",
        "        (0,0,0):0, #Unlabeled\n",
        "        (128,0,0):1, #Building-flooded\n",
        "        (0,128,0):2, # Buildings-non-flooded\n",
        "        (128,128,0):3, #Road-flooded\n",
        "        (0,0,128):4, #Road-non-flooded\n",
        "        (128,0,128):5, #Water\n",
        "        (0,128,128):6, #Tree\n",
        "        (128,128,128):7,#Vehicle\n",
        "        (64,0,0):8, #Pool\n",
        "        (192,0,0):9, #Grass\n",
        "\n",
        "    }\n",
        "\n",
        "\n",
        "    def __init__(self, img_dir, mask_dir, transform=None, binary_flood=True):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img_dir: Directory containing input images\n",
        "            mask_dir: Directory containing segmentation masks\n",
        "            transform: Albumentations transform pipeline\n",
        "        \"\"\"\n",
        "        self.img_dir = img_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.img_names = sorted(os.listdir(img_dir))\n",
        "        self.transform = transform\n",
        "        self.binary_flood= binary_flood\n",
        "\n",
        "        # Filter images that have corresponding masks\n",
        "        all_imgs = sorted(os.listdir(img_dir))\n",
        "        self.img_names = []\n",
        "\n",
        "        for img_name in all_imgs:\n",
        "          base_name = img_name.rsplit(\".\", 1)[0]\n",
        "          pattern = os.path.join(mask_dir, base_name + \"_*.png\")\n",
        "          if glob.glob(pattern):\n",
        "            self.img_names.append(img_name)\n",
        "          else:\n",
        "            print(f\"Warning: No mask found for {img_name}, excluding from dataset\")\n",
        "\n",
        "        print(f\"FloodNet dataset: {len(all_imgs)} images, {len(self.img_names)} with masks\")\n",
        "\n",
        "\n",
        "    def rgb_to_class(self, mask_rgb):\n",
        "        \"\"\"Convert RGB mask to class indices\"\"\"\n",
        "        h, w = mask_rgb.shape[:2]\n",
        "        mask_class = np.zeros((h,w), dtype=np.uint8)\n",
        "\n",
        "        for rgb, class_id in self.FLOODNET_COLORS.items():\n",
        "            #Find pixels matching this RGB value\n",
        "            matches = np.all(mask_rgb == rgb, axis=-1)\n",
        "            mask_class[matches] = class_id\n",
        "\n",
        "        return mask_class\n",
        "\n",
        "    def create_flood_mask(self, class_mask):\n",
        "      \"\"\"Create binary flood/non-_flood mask from classes\"\"\"\n",
        "      flood_mask = np.zeros_like(class_mask, dtype=np.uint8)\n",
        "      flooded_classes = [1,3,5] # Building-flooded\n",
        "\n",
        "\n",
        "      for class_id in flooded_classes:\n",
        "        flood_mask[class_mask == class_id] = 1\n",
        "\n",
        "      return flood_mask\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_names)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Load image and mask pair with error handling\"\"\"\n",
        "        img_name = self.img_names[idx]\n",
        "\n",
        "        # Extract base filename without extension\n",
        "        base_name = img_name.rsplit(\".\", 1)[0]\n",
        "\n",
        "        # Find matching mask (FloodNet naming pattern)\n",
        "        pattern = os.path.join(self.mask_dir, base_name + \"_*.png\")\n",
        "        matching_masks = glob.glob(pattern)\n",
        "\n",
        "        if len(matching_masks) == 0:\n",
        "            raise FileNotFoundError(f\"No matching mask for: {img_name}\")\n",
        "\n",
        "        mask_path = matching_masks[0]\n",
        "        img_path = os.path.join(self.img_dir, img_name)\n",
        "\n",
        "        # Load image (OpenCV loads as BGR, convert to RGB)\n",
        "        image = cv2.imread(img_path)\n",
        "        if image is None:\n",
        "            raise FileNotFoundError(f\"Failed to load image: {img_path}\")\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Load mask as grayscale\n",
        "        mask_rgb = cv2.imread(mask_path)\n",
        "        if mask_rgb is None:\n",
        "            raise FileNotFoundError(f\"Failed to load mask: {mask_path}\")\n",
        "        mask_rgb = cv2.cvtColor(mask_rgb, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Convert RGB mask to class indices\n",
        "        class_mask = self.rgb_to_class(mask_rgb)\n",
        "\n",
        "        # Create appropriate output mask\n",
        "        if self.binary_flood:\n",
        "          mask = self.create_flood_mask(class_mask)\n",
        "        else:\n",
        "          mask = class_mask\n",
        "\n",
        "\n",
        "        # Apply augmentations\n",
        "        if self.transform:\n",
        "            augmented = self.transform(image=image, mask=mask)\n",
        "            image = augmented[\"image\"]\n",
        "            mask = augmented[\"mask\"]\n",
        "\n",
        "        return image, mask.long()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "hwZHhRF4PKVk"
      },
      "outputs": [],
      "source": [
        "class RescueNetDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset class for RescueNet damage assessment data\n",
        "\n",
        "    Expected file structure:\n",
        "    - Images: original disaster images\n",
        "    - Masks: multi-class masks (0-3 for damage levels)\n",
        "    \"\"\"\n",
        "    def __init__(self, img_dir, mask_dir, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img_dir: Directory containing input images\n",
        "            mask_dir: Directory containing segmentation masks\n",
        "            transform: Albumentations transform pipeline\n",
        "        \"\"\"\n",
        "        self.img_dir = img_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.img_names = sorted(os.listdir(img_dir))\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_names)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Load image and mask pair with error handling\"\"\"\n",
        "        img_name = self.img_names[idx]\n",
        "\n",
        "        # Extract base filename without extension\n",
        "        base_name = img_name.rsplit(\".\", 1)[0]\n",
        "\n",
        "        # Find matching mask (RescueNet naming pattern)\n",
        "        pattern = os.path.join(self.mask_dir, base_name + \"_lab*.png\")\n",
        "        matching_masks = glob.glob(pattern)\n",
        "\n",
        "        if len(matching_masks) == 0:\n",
        "            raise FileNotFoundError(f\"No matching mask for: {img_name}\")\n",
        "\n",
        "        mask_path = matching_masks[0]\n",
        "        img_path = os.path.join(self.img_dir, img_name)\n",
        "\n",
        "        # Load image\n",
        "        image = cv2.imread(img_path)\n",
        "        if image is None:\n",
        "            raise FileNotFoundError(f\"Failed to load image: {img_path}\")\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Load mask\n",
        "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "        if mask is None:\n",
        "            raise FileNotFoundError(f\"Failed to load mask: {mask_path}\")\n",
        "\n",
        "        # Ensure mask values are in valid range (0-3)\n",
        "        mask = np.clip(mask, 0, 3).astype(np.uint8)\n",
        "\n",
        "        # Apply augmentations\n",
        "        if self.transform:\n",
        "            augmented = self.transform(image=image, mask=mask)\n",
        "            image = augmented[\"image\"]\n",
        "            mask = augmented[\"mask\"]\n",
        "\n",
        "        return image, mask.long()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "tBV4kCq4lJtL"
      },
      "outputs": [],
      "source": [
        "class SpaceNet8Dataset(Dataset):\n",
        "    def __init__(self, manifest_csv, augment=None,\n",
        "                 cache_size=50, disaster_focus=True,\n",
        "                 preload_critical_samples=True, config=None):\n",
        "        self.df = pd.read_csv(manifest_csv)\n",
        "        self.augment = augment\n",
        "        self.disaster_focus = disaster_focus\n",
        "        self.preload_critical_samples = preload_critical_samples\n",
        "        self.config = config\n",
        "\n",
        "        # Quick LRU cache for frequently accessed files\n",
        "        from functools import lru_cache\n",
        "        self._load_file = lru_cache(maxsize=cache_size)(self._load_file_uncached)\n",
        "\n",
        "        if disaster_focus:\n",
        "            self._prioritize_disaster_samples()  # Added underscore\n",
        "\n",
        "        self.gpu_preloaded = {}\n",
        "        if preload_critical_samples and torch.cuda.is_available():\n",
        "            self._preload_disaster_samples(self.config.preload_count)  # Added underscore\n",
        "\n",
        "        print(f\"Disaster-focused SpaceNet8: {len(self.df)} samples\")\n",
        "        print(f\"Cache size: {cache_size}, GPU preloaded: {len(self.gpu_preloaded)}\")\n",
        "\n",
        "    def _prioritize_disaster_samples(self):\n",
        "        \"\"\"Prioritize samples likely to contain flood/damage for training efficiency\"\"\"\n",
        "        file_sizes = []\n",
        "        for _, row in self.df.iterrows():\n",
        "            try:\n",
        "                mask_size = os.path.getsize(row[\"mask_path\"]) if os.path.exists(row[\"mask_path\"]) else 0\n",
        "                file_sizes.append(mask_size)\n",
        "            except:\n",
        "                file_sizes.append(0)\n",
        "\n",
        "        self.df[\"mask_size\"] = file_sizes\n",
        "        self.df = self.df.sort_values(by='mask_size', ascending=False).reset_index(drop=True)\n",
        "        print(f\"Prioritized {len(self.df)} samples by disaster content likelihood\")\n",
        "\n",
        "    def _preload_disaster_samples(self, preload_count):\n",
        "      \"\"\"Preload disaster-rich samples to GPU for faster training\"\"\"\n",
        "      print(f\"Preloading top {preload_count} disaster samples to A100 GPU memory...\")\n",
        "\n",
        "      # ADDED: Check available GPU memory\n",
        "      if torch.cuda.is_available():\n",
        "        available_mem = torch.cuda.get_device_properties(0).total_memory * 0.3  # Use max 30% for preload\n",
        "        estimated_sample_size = self.config.tile_size * self.config.tile_size * 3 * 4  # RGB float32\n",
        "        max_preload = int(available_mem / estimated_sample_size)\n",
        "        preload_count = min(preload_count, max_preload, len(self.df))\n",
        "        print(f\"  Adjusted preload count to {preload_count} based on available memory\")\n",
        "\n",
        "      for idx in range(preload_count):\n",
        "        row = self.df.iloc[idx]\n",
        "        try:\n",
        "            img, mask = self._load_file_uncached(row[\"post_path\"], row[\"mask_path\"])\n",
        "\n",
        "            img_tensor = torch.from_numpy(img).float() / 255.0\n",
        "            mask_tensor = torch.from_numpy(mask).long()\n",
        "\n",
        "            self.gpu_preloaded[idx] = (\n",
        "                img_tensor.cuda(non_blocking=True),\n",
        "                mask_tensor.cuda(non_blocking=True)\n",
        "            )\n",
        "        except RuntimeError as e:  # ADDED: Handle OOM during preload\n",
        "            if \"out of memory\" in str(e).lower():\n",
        "                print(f\"  OOM during preload at sample {idx}, stopping preload\")\n",
        "                torch.cuda.empty_cache()\n",
        "                break\n",
        "            raise e\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to preload sample {idx}: {e}\")\n",
        "\n",
        "      print(f\"Successfully preloaded {len(self.gpu_preloaded)} disaster-rich samples to GPU\")\n",
        "\n",
        "    def _load_file_uncached(self, post_path, mask_path):\n",
        "        \"\"\"Load and cache raw file for optimization\"\"\"\n",
        "        try:\n",
        "            with rio.Env(\n",
        "                GDAL_DISABLE_READDIR_ON_OPEN=\"EMPTY_DIR\",\n",
        "                GDAL_CACHEMAX=2048,\n",
        "                CPL_VSIL_CURL_ALLOWED_EXTENSIONS='.tif,.tiff',  # Fixed comma\n",
        "                GDAL_NUM_THREADS='ALL_CPUS'\n",
        "            ):\n",
        "                with rio.open(post_path) as src:\n",
        "                    # (C,H,W) order - FIXED INDENTATION\n",
        "                    img = src.read([1, 2, 3])\n",
        "                with rio.open(mask_path) as src:\n",
        "                    mask = src.read(1)\n",
        "            return img.astype(np.uint8), mask.astype(np.int64)  # Fixed dtype\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to load file {os.path.basename(post_path)}: {e}\")\n",
        "            return (np.zeros((3, self.config.tile_size, self.config.tile_size), dtype=np.uint8),\n",
        "                    np.zeros((self.config.tile_size, self.config.tile_size), dtype=np.int64))  # Fixed dtype\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Check GPU preloaded samples first (fastest)\n",
        "        if idx in self.gpu_preloaded:\n",
        "            img_tensor, mask_tensor = self.gpu_preloaded[idx]\n",
        "\n",
        "            if self.augment:\n",
        "                img_np = img_tensor.cpu().numpy().transpose(1, 2, 0)  # Fixed comma\n",
        "                mask_np = mask_tensor.cpu().numpy()\n",
        "                augmented = self.augment(image=img_np, mask=mask_np)\n",
        "                return augmented[\"image\"], augmented[\"mask\"]\n",
        "\n",
        "            return img_tensor, mask_tensor\n",
        "\n",
        "        # If not in cache, load from disk\n",
        "        row = self.df.iloc[idx]\n",
        "        img, mask = self._load_file(row[\"post_path\"], row[\"mask_path\"])\n",
        "\n",
        "        # Convert to HWC for Albumentations\n",
        "        img = np.moveaxis(img, 0, -1)\n",
        "        binary_flood_mask = (mask > 0).astype(np.int64)\n",
        "\n",
        "        # Apply augmentation if given\n",
        "        if self.augment:\n",
        "            augmented = self.augment(image=img, mask=binary_flood_mask)  # Fixed mask variable\n",
        "            return augmented[\"image\"], augmented[\"mask\"]\n",
        "\n",
        "        # Back to CHW + normalize\n",
        "        else:\n",
        "            img_tensor = torch.from_numpy(np.moveaxis(img, -1, 0)).float() / 255.0\n",
        "            mask_tensor = torch.from_numpy(binary_flood_mask).long()\n",
        "            return img_tensor, mask_tensor\n",
        "\n",
        "    def get_raw_mask(self, idx):\n",
        "        \"\"\"Get raw mask without augmentation for weight computation\"\"\"\n",
        "        row = self.df.iloc[idx]\n",
        "        try:\n",
        "            with rio.open(row[\"mask_path\"]) as src:\n",
        "                mask = src.read(1)\n",
        "            return mask.astype(np.int64)\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to load mask {idx}: {e}\")\n",
        "            return np.zeros((self.config.tile_size, self.config.tile_size), dtype=np.int64)\n",
        "\n",
        "class SpaceNet8CompatWrapper(Dataset):\n",
        "    \"\"\"Wrapper to make SpaceNet8 compatible with FloodNet\"\"\"\n",
        "    def __init__(self, spacenet_dataset):\n",
        "        self.spacenet_dataset = spacenet_dataset\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.spacenet_dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_tensor, mask_tensor = self.spacenet_dataset[idx]\n",
        "        # Ensure mask is binary for flood detection (0=no flood, 1=flood)\n",
        "        # SpaceNet8 uses: 0=no building, 1=no damage, 2=minor, 3=major, 4=destroyed\n",
        "        # Convert to binary: 0=no flood, 1=flood (any building damage indicates potential flooding)\n",
        "        binary_mask = (mask_tensor > 0).long()\n",
        "\n",
        "        return img_tensor, binary_mask\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "7QaMm0bRv4ep"
      },
      "outputs": [],
      "source": [
        "def compute_class_weights(dataset, num_classes):\n",
        "    \"\"\"Fast vectorized class weight computation - bypasses augmentation\"\"\"\n",
        "    import rasterio as rio\n",
        "\n",
        "    class_counts = torch.zeros(num_classes, dtype=torch.float64)\n",
        "\n",
        "    print(\"Fast class weight computation (bypassing augmentation)...\")\n",
        "\n",
        "    # Direct mask access without augmentation\n",
        "    if hasattr(dataset, 'df'):  # For SpaceNet8Dataset\n",
        "        for idx in tqdm(range(len(dataset)), desc=\"Analyzing masks\"):\n",
        "            row = dataset.df.iloc[idx]\n",
        "            try:\n",
        "                # Read mask directly - skip augmentation pipeline\n",
        "                with rio.open(row[\"mask_path\"]) as src:\n",
        "                    mask = src.read(1)\n",
        "                # Vectorized histogram\n",
        "                mask_flat = torch.from_numpy(mask.flatten()).long()\n",
        "                counts = torch.bincount(mask_flat, minlength=num_classes)\n",
        "                class_counts += counts\n",
        "            except Exception as e:\n",
        "                continue\n",
        "    else:  # For other dataset types\n",
        "        # Fallback to original but optimized\n",
        "        for idx in tqdm(range(len(dataset)), desc=\"Analyzing masks\"):\n",
        "            try:\n",
        "                # Get raw mask without augmentation\n",
        "                if hasattr(dataset, 'get_raw_mask'):\n",
        "                    mask = dataset.get_raw_mask(idx)\n",
        "                else:\n",
        "                    # Temporarily disable augmentation\n",
        "                    orig_augment = dataset.augment\n",
        "                    dataset.augment = None\n",
        "                    _, mask = dataset[idx]\n",
        "                    dataset.augment = orig_augment\n",
        "\n",
        "                if isinstance(mask, torch.Tensor):\n",
        "                    mask_flat = mask.flatten()\n",
        "                else:\n",
        "                    mask_flat = torch.from_numpy(mask.flatten()).long()\n",
        "\n",
        "                counts = torch.bincount(mask_flat, minlength=num_classes)\n",
        "                class_counts += counts\n",
        "            except Exception as e:\n",
        "                continue\n",
        "\n",
        "    # Compute weights\n",
        "    total = class_counts.sum()\n",
        "    weights = total / (num_classes * class_counts.clamp(min=1))\n",
        "    weights = weights / weights.sum() * num_classes  # Normalize\n",
        "\n",
        "    return weights.float()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKWSLSXXSpYf"
      },
      "source": [
        "## Data Augmentation Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Us6gA-NJSt-x"
      },
      "outputs": [],
      "source": [
        "def get_training_augmentation(config, training=True):\n",
        "    \"\"\"\n",
        "    Training augmentation pipeline with various geometric and color transforms\n",
        "    \"\"\"\n",
        "    return A.Compose([\n",
        "        # Resize to target dimensions\n",
        "        A.Resize(config.tile_size, config.tile_size),\n",
        "\n",
        "        # Geometric transforms\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.VerticalFlip(p=0.1),  # Less common but useful for aerial images\n",
        "        A.RandomRotate90(p=0.5),\n",
        "\n",
        "        # Slight rotations and shifts\n",
        "        A.ShiftScaleRotate(\n",
        "            shift_limit=0.1,\n",
        "            scale_limit=0.1,\n",
        "            rotate_limit=15,\n",
        "            border_mode=cv2.BORDER_REFLECT,\n",
        "            p=0.5\n",
        "        ),\n",
        "\n",
        "        # Color augmentations\n",
        "        A.RandomBrightnessContrast(\n",
        "            brightness_limit=0.2,\n",
        "            contrast_limit=0.2,\n",
        "            p=0.5\n",
        "        ),\n",
        "        A.ColorJitter(\n",
        "            brightness=0.1,\n",
        "            contrast=0.1,\n",
        "            saturation=0.1,\n",
        "            hue=0.05,\n",
        "            p=0.3\n",
        "        ),\n",
        "\n",
        "        # Weather effects (useful for disaster scenarios)\n",
        "        A.RandomRain(p=0.1),\n",
        "        A.RandomFog(p=0.1),\n",
        "\n",
        "        # Noise and blur\n",
        "        A.GaussNoise(per_channel=True, p=0.2),\n",
        "        A.GaussianBlur(blur_limit=(3, 7), p=0.2),\n",
        "\n",
        "        # Normalize with ImageNet statistics\n",
        "        A.Normalize(\n",
        "            mean=(0.485, 0.456, 0.406),\n",
        "            std=(0.229, 0.224, 0.225)\n",
        "        ),\n",
        "\n",
        "        # Convert to PyTorch tensor\n",
        "        ToTensorV2()\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "AetmH_CoTA10"
      },
      "outputs": [],
      "source": [
        "def get_validation_augmentation(config):\n",
        "    \"\"\"\n",
        "    Validation augmentation pipeline (only essential transforms)\n",
        "    \"\"\"\n",
        "    return A.Compose([\n",
        "        A.Resize(config.tile_size, config.tile_size),\n",
        "        A.Normalize(\n",
        "            mean=(0.485, 0.456, 0.406),\n",
        "            std=(0.229, 0.224, 0.225)\n",
        "        ),\n",
        "        ToTensorV2()\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "wTOtyNZPTC1w"
      },
      "outputs": [],
      "source": [
        "def get_test_augmentation():\n",
        "    \"\"\"\n",
        "    Test augmentation pipeline (same as validation)\n",
        "    \"\"\"\n",
        "    return get_validation_augmentation()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Me6035kVTH9I"
      },
      "source": [
        "## Attention models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "LS4o6fvJTHjX"
      },
      "outputs": [],
      "source": [
        "class ChannelAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Channel attention module to focus on important feature channels\n",
        "    Squeeze-and-Excitation style attention mechanism\n",
        "    \"\"\"\n",
        "    def __init__(self, channels, reduction=16):\n",
        "        super().__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
        "\n",
        "        # Shared MLP\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(channels, channels // reduction, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(channels // reduction, channels, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _, _ = x.size()\n",
        "\n",
        "        # Average pooling path\n",
        "        avg_out = self.fc(self.avg_pool(x).view(b, c))\n",
        "\n",
        "        # Max pooling path\n",
        "        max_out = self.fc(self.max_pool(x).view(b, c))\n",
        "\n",
        "        # Combine and apply attention\n",
        "        attention = avg_out + max_out\n",
        "        return x * attention.view(b, c, 1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "83_aRdS6TRdC"
      },
      "outputs": [],
      "source": [
        "class SpatialAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Spatial attention module to focus on important spatial regions\n",
        "    \"\"\"\n",
        "    def __init__(self, kernel_size=7):\n",
        "        super().__init__()\n",
        "        padding = kernel_size // 2\n",
        "        self.conv = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Channel-wise average and max\n",
        "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
        "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
        "\n",
        "        # Concatenate and convolve\n",
        "        x_cat = torch.cat([avg_out, max_out], dim=1)\n",
        "        attention = self.sigmoid(self.conv(x_cat))\n",
        "\n",
        "        return x * attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDnVRkb0TYu4"
      },
      "source": [
        "# Multi-Task Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "K-JJXAoyTYQC"
      },
      "outputs": [],
      "source": [
        "class EnhancedDisasterModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Enhanced multi-task segmentation model for disaster assessment\n",
        "\n",
        "    Features:\n",
        "    - Shared backbone for feature extraction\n",
        "    - Task-specific heads with attention mechanisms\n",
        "    - Feature fusion for cross-task learning\n",
        "    - Deep supervision options\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes_flood=2, num_classes_damage=4, backbone='resnet101', config=None):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.memory_format = torch.channels_last if torch.cuda.is_available() else torch.contiguous_format\n",
        "\n",
        "        # Initialize backbone based on configuration\n",
        "        if backbone == 'resnet101':\n",
        "            base_model = deeplabv3_resnet101(pretrained=True)\n",
        "            print(\"Using ResNet101 backbone\")\n",
        "        else:\n",
        "            base_model = deeplabv3_resnet50(pretrained=True)\n",
        "            print(\"Using ResNet50 backbone\")\n",
        "\n",
        "        # Extract backbone and ASPP module\n",
        "        self.backbone = base_model.backbone\n",
        "        self.aspp = base_model.classifier[0]\n",
        "\n",
        "        # Attention mechanisms for feature refinement\n",
        "        self.channel_attention = ChannelAttention(256)\n",
        "        self.spatial_attention = SpatialAttention()\n",
        "\n",
        "        # Task-specific feature extraction branches\n",
        "        # Flood detection branch\n",
        "        self.flood_branch = nn.Sequential(\n",
        "            nn.Conv2d(256, 512, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout2d(0.3),\n",
        "\n",
        "            nn.Conv2d(512, 256, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout2d(0.2),\n",
        "\n",
        "            nn.Conv2d(256, 128, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        # Damage assessment branch\n",
        "        self.damage_branch = nn.Sequential(\n",
        "            nn.Conv2d(256, 512, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout2d(0.3),\n",
        "\n",
        "            nn.Conv2d(512, 256, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout2d(0.2),\n",
        "\n",
        "            nn.Conv2d(256, 128, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        # Classification heads\n",
        "        self.flood_classifier = nn.Conv2d(128, num_classes_flood, 1)\n",
        "        self.damage_classifier = nn.Conv2d(128, num_classes_damage, 1)\n",
        "\n",
        "        # Optional: Cross-task feature fusion\n",
        "        self.enable_fusion = True\n",
        "        if self.enable_fusion:\n",
        "            self.fusion_conv = nn.Sequential(\n",
        "                nn.Conv2d(256, 128, 1, bias=False),\n",
        "                nn.BatchNorm2d(128),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Conv2d(128, 256, 1, bias=False),\n",
        "                nn.BatchNorm2d(256),\n",
        "                nn.Sigmoid()\n",
        "            )\n",
        "        if hasattr(config, 'gradient_checkpoint') and config.gradient_checkpoint:\n",
        "            if hasattr(self.backbone, 'gradient_checkpointing_enable'):\n",
        "                self.backbone.gradient_checkpointing_enable()\n",
        "                print(\"Gradient checkpointing enabled - trading compute for memory\")\n",
        "\n",
        "\n",
        "    def forward(self, x, return_features=False):\n",
        "        \"\"\"\n",
        "        Forward pass through the network\n",
        "\n",
        "        Args:\n",
        "            x: Input tensor [B, 3, H, W]\n",
        "            return_features: Whether to return intermediate features\n",
        "\n",
        "        Returns:\n",
        "            flood_out: Flood segmentation output [B, 2, H, W]\n",
        "            damage_out: Damage segmentation output [B, 4, H, W]\n",
        "            features (optional): Intermediate features for visualization\n",
        "        \"\"\"\n",
        "        # Store input shape for upsampling\n",
        "        input_shape = x.shape[-2:]\n",
        "\n",
        "        # Extract multi-level features from backbone\n",
        "        features = self.backbone(x)\n",
        "\n",
        "        # Get high-level features\n",
        "        x = features['out']\n",
        "\n",
        "        # Apply ASPP for multi-scale context\n",
        "        x = self.aspp(x)\n",
        "\n",
        "        # Apply attention mechanisms\n",
        "        x = self.channel_attention(x)\n",
        "        x = self.spatial_attention(x)\n",
        "\n",
        "        # Task-specific processing\n",
        "        flood_features = self.flood_branch(x)\n",
        "        damage_features = self.damage_branch(x)\n",
        "\n",
        "        # Optional cross-task feature fusion\n",
        "        if self.enable_fusion:\n",
        "            # Concatenate task features\n",
        "            combined = torch.cat([flood_features, damage_features], dim=1)\n",
        "\n",
        "            # Generate fusion weights\n",
        "            fusion_weights = self.fusion_conv(combined)\n",
        "\n",
        "            flood_channels = flood_features.shape[1]\n",
        "            damage_channels = damage_features.shape[1]\n",
        "\n",
        "            # Apply fusion\n",
        "            flood_features = flood_features + fusion_weights[:, :flood_channels] * damage_features\n",
        "            damage_features = damage_features + fusion_weights[:, flood_channels:flood_channels+damage_channels] * flood_features\n",
        "\n",
        "        # Generate predictions\n",
        "        flood_out = self.flood_classifier(flood_features)\n",
        "        damage_out = self.damage_classifier(damage_features)\n",
        "\n",
        "        # Upsample to original resolution\n",
        "        flood_out = F.interpolate(\n",
        "            flood_out, size=input_shape,\n",
        "            mode='bilinear', align_corners=False\n",
        "        )\n",
        "        damage_out = F.interpolate(\n",
        "            damage_out, size=input_shape,\n",
        "            mode='bilinear', align_corners=False\n",
        "        )\n",
        "\n",
        "        if return_features:\n",
        "            return flood_out, damage_out, flood_features, damage_features\n",
        "\n",
        "        return flood_out, damage_out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x22JpKxYVbSh"
      },
      "source": [
        "## Custom Loss Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "MeVFNwxBVfM0"
      },
      "outputs": [],
      "source": [
        "class DisasterFocusedLoss(nn.Module):\n",
        "    \"\"\"Loss function optimized for disaster mapping class imbalances\"\"\"\n",
        "\n",
        "    def __init__(self, ce_weight=0.3, dice_weight=0.4, focal_weight=0.3,\n",
        "                 class_weights=None, focal_alpha=0.25, focal_gamma=2.0):\n",
        "        super().__init__()\n",
        "        self.ce_weight = ce_weight\n",
        "        self.dice_weight = dice_weight\n",
        "        self.focal_weight = focal_weight\n",
        "        self.focal_alpha = focal_alpha\n",
        "        self.focal_gamma = focal_gamma\n",
        "\n",
        "        self.ce_loss = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        # Cross-entropy with disaster class weighting\n",
        "        ce = self.ce_loss(pred, target)\n",
        "\n",
        "        # Dice loss - critical for disaster boundary accuracy\n",
        "        dice = self._dice_loss_disaster(pred, target)\n",
        "\n",
        "        # Focal loss - handle severe class imbalance in disaster data\n",
        "        focal = self._focal_loss_disaster(pred, target)\n",
        "\n",
        "        total_loss = (\n",
        "            self.ce_weight * ce +\n",
        "            self.dice_weight * dice +\n",
        "            self.focal_weight * focal\n",
        "        )\n",
        "\n",
        "        return total_loss, {\n",
        "            'ce': ce.item(),\n",
        "            'dice': dice.item(),\n",
        "            'focal': focal.item()\n",
        "        }\n",
        "\n",
        "    def _dice_loss_disaster(self, pred, target):\n",
        "        \"\"\"Dice loss optimized for disaster mapping accuracy\"\"\"\n",
        "        pred_soft = F.softmax(pred, dim=1)\n",
        "        target_one_hot = F.one_hot(target, pred.shape[1]).permute(0, 3, 1, 2).float()\n",
        "\n",
        "        # Flatten for efficient computation\n",
        "        pred_flat = pred_soft.reshape(pred_soft.shape[0], pred_soft.shape[1], -1)\n",
        "        target_flat = target_one_hot.reshape(target_one_hot.shape[0], target_one_hot.shape[1], -1)\n",
        "\n",
        "        intersection = (pred_flat * target_flat).sum(dim=2)\n",
        "        union = pred_flat.sum(dim=2) + target_flat.sum(dim=2)\n",
        "\n",
        "        # Smooth dice with small epsilon for disaster data stability\n",
        "        dice = (2 * intersection + 1e-6) / (union + 1e-6)\n",
        "\n",
        "        return 1 - dice.mean()\n",
        "\n",
        "    def _focal_loss_disaster(self, pred, target):\n",
        "        \"\"\"Focal loss with disaster-specific parameters\"\"\"\n",
        "        ce_loss = F.cross_entropy(pred, target, reduction='none')\n",
        "        pt = torch.exp(-ce_loss)\n",
        "\n",
        "        # Disaster-tuned focal loss\n",
        "        focal_loss = self.focal_alpha * (1 - pt) ** self.focal_gamma * ce_loss\n",
        "\n",
        "        return focal_loss.mean()\n",
        "\n",
        "dice_loss_flood = smp.losses.DiceLoss(mode='binary')\n",
        "\n",
        "bce_loss_flood = nn.BCEWithLogitsLoss()\n",
        "\n",
        "def criterion_flood(pred, target):\n",
        "  return dice_loss_flood(pred, target) + bce_loss_flood(pred, target)\n",
        "\n",
        "criterion_damage = DisasterFocusedLoss(\n",
        "    ce_weight=0.3,\n",
        "    dice_weight=0.4,\n",
        "    focal_weight=0.3,\n",
        "    focal_alpha=0.25,\n",
        "    focal_gamma=2.0\n",
        ")\n",
        "\n",
        "def joint_loss(preds, targets):\n",
        "  \"\"\"\n",
        "  preds: dict with {\"flood\": flood_out, \"damage\": damage_out}\n",
        "  targets: dict with {\"flood\": flood_mask, \"damage\": damage_mask}\n",
        "  \"\"\"\n",
        "  # Flood: binary dice + BCE\n",
        "  loss_flood = criterion_flood(preds[\"flood\"], targets[\"flood\"])\n",
        "\n",
        "  # Damage: multi-class dice + focal loss\n",
        "  loss_damage, components = criterion_damage(preds[\"damage\"], targets[\"damage\"])\n",
        "\n",
        "  total_loss = 0.5 * loss_flood + 0.5 * loss_damage\n",
        "  return total_loss, {\"flood\": loss_flood.item(), **components}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5TL0-McWGOp"
      },
      "source": [
        "## Evaluation Metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "5hgto8UdWLlE"
      },
      "outputs": [],
      "source": [
        "class MetricCalculator:\n",
        "    \"\"\"\n",
        "    Optimized vectorized segmentation metrics calculator\n",
        "    \"\"\"\n",
        "    @staticmethod\n",
        "    def calculate_iou(pred, target, num_classes):\n",
        "        \"\"\"\n",
        "        Vectorized IoU calculation - much faster\n",
        "        \"\"\"\n",
        "        if pred.dim() > 1:\n",
        "            pred = pred.view(-1)\n",
        "            target = target.view(-1)\n",
        "\n",
        "        # Vectorized computation for all classes at once\n",
        "        ious = []\n",
        "        for cls in range(num_classes):\n",
        "            pred_mask = (pred == cls)\n",
        "            target_mask = (target == cls)\n",
        "\n",
        "            intersection = (pred_mask & target_mask).sum().float()\n",
        "            union = (pred_mask | target_mask).sum().float()\n",
        "\n",
        "            if union == 0:\n",
        "                ious.append(float('nan'))\n",
        "            else:\n",
        "                ious.append((intersection / union).item())\n",
        "\n",
        "        return ious\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_metrics_batch(preds, targets, num_classes):\n",
        "        \"\"\"\n",
        "        Calculate IoU and Dice for entire batch at once - NEW METHOD\n",
        "        \"\"\"\n",
        "        batch_size = preds.shape[0]\n",
        "        device = preds.device\n",
        "\n",
        "        # Flatten spatial dimensions\n",
        "        preds_flat = preds.view(batch_size, -1)\n",
        "        targets_flat = targets.view(batch_size, -1)\n",
        "\n",
        "        # Preallocate results\n",
        "        ious = torch.zeros(batch_size, num_classes, device=device)\n",
        "        dices = torch.zeros(batch_size, num_classes, device=device)\n",
        "\n",
        "        for cls in range(num_classes):\n",
        "            pred_mask = (preds_flat == cls).float()\n",
        "            target_mask = (targets_flat == cls).float()\n",
        "\n",
        "            intersection = (pred_mask * target_mask).sum(dim=1)\n",
        "            union = ((pred_mask + target_mask) > 0).float().sum(dim=1)\n",
        "            pred_sum = pred_mask.sum(dim=1)\n",
        "            target_sum = target_mask.sum(dim=1)\n",
        "\n",
        "            # IoU\n",
        "            valid_union = union > 0\n",
        "            ious[valid_union, cls] = intersection[valid_union] / union[valid_union]\n",
        "            ious[~valid_union, cls] = float('nan')\n",
        "\n",
        "            # Dice\n",
        "            dice_denom = pred_sum + target_sum\n",
        "            valid_dice = dice_denom > 0\n",
        "            dices[valid_dice, cls] = 2 * intersection[valid_dice] / dice_denom[valid_dice]\n",
        "            dices[~valid_dice, cls] = float('nan')\n",
        "\n",
        "        return ious, dices\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_dice(pred, target, num_classes):\n",
        "        \"\"\"\n",
        "        Vectorized Dice calculation\n",
        "        \"\"\"\n",
        "        if pred.dim() > 1:\n",
        "            pred = pred.view(-1)\n",
        "            target = target.view(-1)\n",
        "\n",
        "        dices = []\n",
        "        for cls in range(num_classes):\n",
        "            pred_mask = (pred == cls).float()\n",
        "            target_mask = (target == cls).float()\n",
        "\n",
        "            intersection = (pred_mask * target_mask).sum()\n",
        "            dice_denom = pred_mask.sum() + target_mask.sum()\n",
        "\n",
        "            if dice_denom == 0:\n",
        "                dices.append(float('nan'))\n",
        "            else:\n",
        "                dices.append((2 * intersection / dice_denom).item())\n",
        "\n",
        "        return dices\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_pixel_accuracy(pred, target):\n",
        "        \"\"\"\n",
        "        Calculate overall pixel accuracy - optimized\n",
        "        \"\"\"\n",
        "        return (pred == target).float().mean().item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7m1MqUlg9nf"
      },
      "source": [
        "## Pretraining Flood model with SpaceNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "YrFLRTslg45h"
      },
      "outputs": [],
      "source": [
        "# Create a separate pretraining function\n",
        "def pretrain_flood_on_spacenet(model, spacenet_loader, num_epochs=10):\n",
        "    print(\"Pretraining flood detection on SpaceNet8...\")\n",
        "\n",
        "    # Only optimize flood-related parameters\n",
        "    pretrain_optimizer = torch.optim.Adam([\n",
        "        {'params': model.backbone.parameters(), 'lr': 1e-5},\n",
        "        {'params': model.flood_branch.parameters(), 'lr': 5e-5},\n",
        "        {'params': model.flood_classifier.parameters(), 'lr': 5e-5}\n",
        "    ])\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "      for images, masks in tqdm(spacenet_loader):\n",
        "        flood_out, _ = model(images)\n",
        "        loss = F.cross_entropy(flood_out, masks)\n",
        "\n",
        "        pretrain_optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        pretrain_optimizer.step()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This should be BEFORE creating the trainer, not during training\n",
        "if not class_weights_computed:  # Add a flag to ensure it's only done once\n",
        "    print(\"Computing class weights (this may take a minute)...\")\n",
        "    flood_class_weights = compute_class_weights(flood_train_dataset, num_classes=2)\n",
        "    damage_class_weights = compute_class_weights(damage_train_dataset, num_classes=4)\n",
        "    class_weights_computed = True\n",
        "    print(\"Class weights computed!\")"
      ],
      "metadata": {
        "id": "Q6HSimvNIwlu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNFpPUyRWSbn"
      },
      "source": [
        "## Optimized Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "i98iJ4O1WRe0"
      },
      "outputs": [],
      "source": [
        "class OptimizedTrainer:\n",
        "    def __init__(self, model, config, device='cuda'):\n",
        "        self.model = model\n",
        "        self.config = config\n",
        "        self.device = config.device\n",
        "\n",
        "        # A100 optimizations\n",
        "        self._enable_optimizations()\n",
        "\n",
        "        # Mixed precision training\n",
        "        self.scaler = torch.amp.GradScaler('cuda', init_scale=2**10, growth_interval=100)\n",
        "\n",
        "        # Optimizer with task-specific learning rates\n",
        "        self.optimizer = self._create_disaster_optimizer()\n",
        "\n",
        "        # Loss functions\n",
        "        self.flood_loss_fn = self._create_disaster_loss('flood')\n",
        "        self.damage_loss_fn = self._create_disaster_loss('damage')\n",
        "\n",
        "        # Metrics tracking\n",
        "        self.metric_calculator = MetricCalculator()\n",
        "        self.disaster_metrics = defaultdict(list)\n",
        "        self.best_disaster_iou = {'flood': 0.0, 'damage': 0.0}\n",
        "\n",
        "        # Training history tracking\n",
        "        self.train_losses = {'flood': [], 'damage': [], 'total': []}\n",
        "        self.val_metrics = {'flood_iou': [], 'damage_iou': []}\n",
        "        self.best_flood_iou = 0.0\n",
        "        self.best_damage_iou = 0.0\n",
        "        self.best_combined_score = 0.0\n",
        "\n",
        "        print(\"Disaster mapping trainer initialized for A100\")\n",
        "\n",
        "    def _enable_optimizations(self):\n",
        "        \"\"\"Enable A100-specific optimizations\"\"\"\n",
        "        torch.backends.cudnn.allow_tf32 = True\n",
        "        torch.backends.cuda.matmul.allow_tf32 = True\n",
        "        torch.backends.cudnn.benchmark = True\n",
        "        torch.cuda.set_per_process_memory_fraction(0.8)\n",
        "        print(\"A100 optimizations enabled\")\n",
        "\n",
        "    def _create_disaster_optimizer(self):\n",
        "        \"\"\"Create optimizer with disaster-focused learning rates\"\"\"\n",
        "        param_groups = [\n",
        "            {\n",
        "                'params': [p for n, p in self.model.backbone.named_parameters()],\n",
        "                'lr': self.config.learning_rate * 0.1,\n",
        "                'weight_decay': self.config.weight_decay\n",
        "            },\n",
        "            {\n",
        "                'params': [p for n, p in self.model.aspp.named_parameters()],\n",
        "                'lr': self.config.learning_rate * 0.3,\n",
        "                'weight_decay': self.config.weight_decay * 0.5\n",
        "            },\n",
        "            {\n",
        "                'params': list(self.model.flood_branch.parameters()) +\n",
        "                          list(self.model.flood_classifier.parameters()),\n",
        "                'lr': self.config.learning_rate * 1.2,\n",
        "                'weight_decay': self.config.weight_decay * 0.8\n",
        "            },\n",
        "            {\n",
        "                'params': list(self.model.damage_branch.parameters()) +\n",
        "                          list(self.model.damage_classifier.parameters()),\n",
        "                'lr': self.config.learning_rate * 1.5,\n",
        "                'weight_decay': self.config.weight_decay * 0.6\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        return torch.optim.AdamW(\n",
        "            param_groups,\n",
        "            eps=1e-8,\n",
        "            betas=(0.9, 0.999),\n",
        "            amsgrad=True\n",
        "        )\n",
        "\n",
        "    def _create_disaster_loss(self, task):\n",
        "        \"\"\"Create disaster-focused loss function\"\"\"\n",
        "        if task == 'flood':\n",
        "            class_weights = torch.tensor([0.2, 3.5]).cuda()\n",
        "            alpha = 0.8\n",
        "        else:\n",
        "            class_weights = torch.tensor([0.3, 1.5, 2.2, 2.8]).cuda()\n",
        "            alpha = 0.6\n",
        "\n",
        "        return DisasterFocusedLoss(\n",
        "            ce_weight=0.3,\n",
        "            dice_weight=0.4,\n",
        "            focal_weight=0.3,\n",
        "            class_weights=class_weights,\n",
        "            focal_alpha=alpha\n",
        "        )\n",
        "\n",
        "    def train_epoch(self, flood_loader, damage_loader, epoch):\n",
        "      \"\"\"A100-optimized training epoch for disaster mapping\"\"\"\n",
        "      import time\n",
        "      self.model.train()\n",
        "\n",
        "      flood_cycle = cycle(flood_loader)\n",
        "      damage_cycle = cycle(damage_loader)\n",
        "\n",
        "      num_iterations = max(len(flood_loader), len(damage_loader))\n",
        "      epoch_metrics = defaultdict(float)\n",
        "\n",
        "      print(f\"Disaster mapping training epoch {epoch}: {num_iterations} iterations\")\n",
        "\n",
        "      # Initialize gradient accumulation\n",
        "      self.optimizer.zero_grad(set_to_none=True)\n",
        "      pbar = tqdm(range(num_iterations), desc=f\"Epoch {epoch+1}\", leave=False)\n",
        "\n",
        "      times = {'data_load': [], 'forward':[], 'total':[]}\n",
        "\n",
        "      for i in pbar:\n",
        "          print(f\"DEBUG: Starting iteration {i}\")\n",
        "          iter_start = time.time()\n",
        "          try:\n",
        "              print(f\"DEBUG: Loading flood batch...\")\n",
        "              data_start = time.time()\n",
        "              flood_batch = next(flood_cycle)\n",
        "              print(f\"DEBUG: Flood batch loaded in {time.time()-data_start:.2f}s\")\n",
        "\n",
        "              print(f\"DEBUG: Loading damage batch...\")\n",
        "              damage_batch = next(damage_cycle)\n",
        "              print(f\"DEBUG: Damage batch loaded\")\n",
        "\n",
        "              times['data_load'].append(time.time() - data_start)\n",
        "\n",
        "              forward_start = time.time()\n",
        "              metrics = self._simultaneous_disaster_step(flood_batch, damage_batch, i)\n",
        "\n",
        "              times['forward'].append(time.time() - forward_start)\n",
        "\n",
        "              # Accumulate metrics\n",
        "              for key, value in metrics.items():\n",
        "                  epoch_metrics[key] += value\n",
        "\n",
        "              # Accumulation boundary -> optimizer step\n",
        "              if (i + 1) % self.config.accumulation_steps == 0:\n",
        "                  self._disaster_gradient_update()\n",
        "\n",
        "              times['total'].append(time.time() - iter_start)\n",
        "\n",
        "            # Progress reporting\n",
        "              if (i + 1) % 20 == 0:\n",
        "                  if len(times['total']) >= 20:\n",
        "                      avg_data = sum(times['data_load'][-20:]) / 20\n",
        "                      avg_forward = sum(times['forward'][-20:]) / 20\n",
        "                      avg_total = sum(times['total'][-20:]) / 20\n",
        "\n",
        "                      print(f\"\\n⏱️  Timing (last 20 iters):\")\n",
        "                      print(f\"   Data load: {avg_data:.3f}s ({avg_data/avg_total*100:.1f}%)\")\n",
        "                      print(f\"   Forward+backward: {avg_forward:.3f}s ({avg_forward/avg_total*100:.1f}%)\")\n",
        "                      print(f\"   Total: {avg_total:.3f}s/iter\")\n",
        "\n",
        "                  flood_loss = epoch_metrics['flood_loss'] / (i + 1)\n",
        "                  damage_loss = epoch_metrics['damage_loss'] / (i + 1)\n",
        "                  mem_gb = (torch.cuda.max_memory_allocated() / 1e9) if torch.cuda.is_available() else 0\n",
        "                  pbar.set_postfix({\n",
        "                  'flood_loss': f\"{flood_loss:.4f}\",\n",
        "                  'damage_loss': f\"{damage_loss:.4f}\",\n",
        "                  'mem': f\"{mem_gb:.1f}GB\"\n",
        "                  })\n",
        "\n",
        "                  if (i + 1) % 50 == 0:\n",
        "                      torch.cuda.empty_cache()\n",
        "\n",
        "          except RuntimeError as e:\n",
        "            # Robust OOM handling (skip batch, keep training)\n",
        "            if \"out of memory\" in str(e).lower():\n",
        "                print(f\"⚠️ GPU OOM at iter {i}, skipping...\")\n",
        "                if torch.cuda.is_available():\n",
        "                    torch.cuda.empty_cache()\n",
        "                # Clear any partial grads from the failed step\n",
        "                self.optimizer.zero_grad(set_to_none=True)\n",
        "                continue\n",
        "            # Re-raise non-OOM errors\n",
        "            raise\n",
        "\n",
        "      # Flush remaining accumulated grads if loop ended mid-accumulation\n",
        "      if num_iterations % self.config.accumulation_steps != 0:\n",
        "          self._disaster_gradient_update()\n",
        "\n",
        "      # Epoch averages\n",
        "      epoch_flood_loss = epoch_metrics['flood_loss'] / max(1, num_iterations)\n",
        "      epoch_damage_loss = epoch_metrics['damage_loss'] / max(1, num_iterations)\n",
        "      epoch_total_loss = epoch_flood_loss + epoch_damage_loss\n",
        "\n",
        "      # Record history\n",
        "      self.train_losses['flood'].append(epoch_flood_loss)\n",
        "      self.train_losses['damage'].append(epoch_damage_loss)\n",
        "      self.train_losses['total'].append(epoch_total_loss)\n",
        "\n",
        "      return epoch_flood_loss, epoch_damage_loss, epoch_total_loss\n",
        "\n",
        "    def _simultaneous_disaster_step(self, flood_batch, damage_batch, step_idx):\n",
        "        \"\"\"Process both disaster tasks simultaneously on A100\"\"\"\n",
        "        flood_imgs, flood_masks = flood_batch\n",
        "        damage_imgs, damage_masks = damage_batch\n",
        "\n",
        "        # Move to GPU with non-blocking transfer\n",
        "        flood_imgs = flood_imgs.to(self.device, non_blocking=True)\n",
        "        flood_masks = flood_masks.to(self.device, non_blocking=True)\n",
        "        damage_imgs = damage_imgs.to(self.device, non_blocking=True)\n",
        "        damage_masks = damage_masks.to(self.device, non_blocking=True)\n",
        "\n",
        "        metrics = {}\n",
        "\n",
        "        with torch.amp.autocast('cuda', enabled=True):\n",
        "            # Process flood task\n",
        "            flood_out_1, _ = self.model(flood_imgs)\n",
        "            flood_loss, flood_components = self.flood_loss_fn(flood_out_1, flood_masks)\n",
        "\n",
        "            # Process damage task\n",
        "            _, damage_out_2 = self.model(damage_imgs)\n",
        "            damage_loss, damage_components = self.damage_loss_fn(damage_out_2, damage_masks)\n",
        "\n",
        "            # Combined loss with task weighting\n",
        "            total_loss = (\n",
        "                self.config.flood_task_weight * flood_loss +\n",
        "                self.config.damage_task_weight * damage_loss\n",
        "            ) / self.config.accumulation_steps\n",
        "\n",
        "        # Backward pass\n",
        "        self.scaler.scale(total_loss).backward()\n",
        "\n",
        "        # Collect metrics\n",
        "        metrics.update({\n",
        "            'flood_loss': flood_loss.item(),\n",
        "            'damage_loss': damage_loss.item(),\n",
        "            'total_loss': total_loss.item() * self.config.accumulation_steps,\n",
        "            'flood_dice': flood_components.get('dice', 0),\n",
        "            'damage_dice': damage_components.get('dice', 0),\n",
        "        })\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def _disaster_gradient_update(self):\n",
        "        \"\"\"Gradient update optimized for disaster mapping\"\"\"\n",
        "        # Unscale gradients\n",
        "        self.scaler.unscale_(self.optimizer)\n",
        "\n",
        "        # Gradient clipping for stability\n",
        "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.gradient_clip)\n",
        "\n",
        "        # Step optimizer\n",
        "        self.scaler.step(self.optimizer)\n",
        "        self.scaler.update()\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "    def validate(self, loader, task, num_classes):\n",
        "        \"\"\"Optimized validation with batch-level metric computation\"\"\"\n",
        "        self.model.eval()\n",
        "\n",
        "        total_loss = 0\n",
        "        all_ious = []\n",
        "        all_dice = []\n",
        "        all_accuracies = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, masks in tqdm(loader, desc=f'Validating {task}'):\n",
        "                images = images.to(self.device, non_blocking=True)\n",
        "                masks = masks.to(self.device, non_blocking=True)\n",
        "\n",
        "                with torch.amp.autocast('cuda', enabled=True):\n",
        "                    flood_out, damage_out = self.model(images)\n",
        "\n",
        "                    if task == 'flood':\n",
        "                        output = flood_out\n",
        "                        loss, _ = self.flood_loss_fn(output, masks)\n",
        "                    else:\n",
        "                        output = damage_out\n",
        "                        loss, _ = self.damage_loss_fn(output, masks)\n",
        "\n",
        "            total_loss += loss.item() * images.size(0)  # Weight by batch size\n",
        "\n",
        "            # Get predictions\n",
        "            preds = torch.argmax(output, dim=1)\n",
        "\n",
        "            # BATCH-LEVEL METRICS - Much faster!\n",
        "            batch_ious, batch_dice = self.metric_calculator.calculate_metrics_batch(\n",
        "            preds, masks, num_classes\n",
        "            )\n",
        "\n",
        "            #Pixel accuracy for batch\n",
        "            batch_acc = (preds == masks).float().mean(dim=(1, 2))  # Per-sample accuracy\n",
        "\n",
        "            # Store results (keep on GPU until end)\n",
        "            all_ious.append(batch_ious)\n",
        "            all_dice.append(batch_dice)\n",
        "            all_accuracies.append(batch_acc)\n",
        "\n",
        "        # Aggregate all metrics at once (more efficient)\n",
        "        all_ious = torch.cat(all_ious, dim=0)\n",
        "        all_dice = torch.cat(all_dice, dim=0)\n",
        "        all_accuracies = torch.cat(all_accuracies, dim=0)\n",
        "\n",
        "        # Move to CPU and compute statistics\n",
        "        all_ious_cpu = all_ious.cpu().numpy()\n",
        "        all_dice_cpu = all_dice.cpu().numpy()\n",
        "\n",
        "          # Calculate averages\n",
        "        avg_loss = total_loss / len(loader.dataset)\n",
        "        class_ious = np.nanmean(all_ious_cpu, axis=0)\n",
        "        class_dice = np.nanmean(all_dice_cpu, axis=0)\n",
        "        mean_iou = np.nanmean(class_ious)\n",
        "        mean_dice = np.nanmean(class_dice)\n",
        "        mean_accuracy = all_accuracies.mean().item()\n",
        "\n",
        "        # Print validation results\n",
        "        print(f\"\\n{task.upper()} Validation Results:\")\n",
        "        print(f\"Average Loss: {avg_loss:.4f}\")\n",
        "        print(f\"Mean IoU: {mean_iou:.4f}\")\n",
        "        print(f\"Mean Dice: {mean_dice:.4f}\")\n",
        "        print(f\"Pixel Accuracy: {mean_accuracy:.4f}\")\n",
        "\n",
        "        # Per-class metrics\n",
        "        for i in range(num_classes):\n",
        "            class_name = self._get_class_name(task, i)\n",
        "        print(f\"  {class_name} - IoU: {class_ious[i]:.4f}, Dice: {class_dice[i]:.4f}\")\n",
        "\n",
        "        # Update validation metrics history\n",
        "        if task == 'flood':\n",
        "            self.val_metrics['flood_iou'].append(mean_iou)\n",
        "        else:\n",
        "            self.val_metrics['damage_iou'].append(mean_iou)\n",
        "\n",
        "        return avg_loss, mean_iou, class_ious\n",
        "\n",
        "    def _get_class_name(self, task, class_idx):\n",
        "        \"\"\"Get human-readable class names for disaster mapping\"\"\"\n",
        "        if task == 'flood':\n",
        "            return ['Non-Flooded', 'Flooded'][class_idx]\n",
        "        else:\n",
        "            return ['No Damage', 'Minor', 'Major', 'Destroyed'][class_idx]\n",
        "\n",
        "    def test(self, loader, task, num_classes):\n",
        "        \"\"\"Test model - same as validate for disaster mapping\"\"\"\n",
        "        return self.validate(loader, task, num_classes)\n",
        "\n",
        "class CheckpointManager:\n",
        "  \"Save and Resume checkpoints\"\n",
        "\n",
        "  def __init__(self, model, config, checkpoint_dir):\n",
        "    self.model = model\n",
        "    self.config = config\n",
        "    self.device = config.device\n",
        "    self.checkpoint_dir = Path(checkpoint_dir)\n",
        "    self.checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    self.train_losses = {'flood': [], 'damage': [], 'total': []}\n",
        "    self.val_metrics = {'flood_iou': [], 'damage_iou': []}\n",
        "    self.best_flood_iou = 0.0\n",
        "    self.best_damage_iou = 0.0\n",
        "    self.optimizer = None\n",
        "\n",
        "\n",
        "  def save_checkpoint(\n",
        "      self,model, optimizer, scaler, epoch,\n",
        "      flood_metrics, damage_metrics, is_best=False,\n",
        "      train_losses=None, val_metrics=None,\n",
        "      best_flood_iou=None, best_damage_iou=None):\n",
        "\n",
        "    if train_losses is not None:\n",
        "      self.train_losses = train_losses\n",
        "    if val_metrics is not None:\n",
        "      self.val_metrics = val_metrics\n",
        "    if best_flood_iou is not None:\n",
        "      self.best_flood_iou = best_flood_iou\n",
        "    if best_damage_iou is not None:\n",
        "      self.best_damage_iou = best_damage_iou\n",
        "    self.optimizer = optimizer  # Store for plotting\n",
        "\n",
        "    checkpoint = {\n",
        "      'epoch': epoch,\n",
        "      'model_state_dict': model.state_dict(),\n",
        "      'optimizer_state_dict': optimizer.state_dict(),\n",
        "      'scaler_state_dict': scaler.state_dict(),\n",
        "      'train_losses': self.train_losses,\n",
        "      'val_metrics': self.val_metrics,\n",
        "      'flood_metrics': flood_metrics,\n",
        "      'damage_metrics': damage_metrics,\n",
        "      'best_flood_iou': self.best_flood_iou,\n",
        "      'best_damage_iou': self.best_damage_iou,\n",
        "      'config': self.config.__dict__\n",
        "    }\n",
        "\n",
        "    # Save regular checkpoint\n",
        "    checkpoint_path = self.checkpoint_dir / f'checkpoint_epoch_{epoch:04d}.pt'\n",
        "    torch.save(checkpoint, checkpoint_path)\n",
        "    print(f\"Saved checkpoint: {checkpoint_path}\")\n",
        "\n",
        "    # Save best model\n",
        "    if is_best:\n",
        "      best_path = self.checkpoint_dir / f'best_model.pt'\n",
        "      torch.save(checkpoint, best_path)\n",
        "      print(f\"Saved best model: {best_path}\")\n",
        "\n",
        "  def load_checkpoint(self, model, optimizer, scaler, checkpoint_name='best_model.pt'):\n",
        "    \"\"\"Load checkpoint and restore model, optimizer, and scaler state.\"\"\"\n",
        "    checkpoint_path = self.checkpoint_dir / checkpoint_name\n",
        "    print(f\"Loading checkpoint from {checkpoint_path}\")\n",
        "\n",
        "    # Ensure proper device type\n",
        "    device = torch.device(self.device if isinstance(self.device, str) else self.device)\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "\n",
        "    # Restore weights\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    scaler.load_state_dict(checkpoint['scaler_state_dict'])\n",
        "\n",
        "    # Restore training history\n",
        "    self.train_losses = checkpoint.get('train_losses', {'flood': [], 'damage': [], 'total': []})\n",
        "    self.val_metrics = checkpoint.get('val_metrics', {'flood_iou': [], 'damage_iou': []})\n",
        "    self.best_flood_iou = checkpoint.get('best_flood_iou', 0.0)\n",
        "    self.best_damage_iou = checkpoint.get('best_damage_iou', 0.0)\n",
        "\n",
        "    epoch = checkpoint.get('epoch', 0)\n",
        "    print(f\"Resumed from epoch {epoch}\")\n",
        "    print(f\"Best Flood IoU: {self.best_flood_iou:.4f}\")\n",
        "    print(f\"Best Damage IoU: {self.best_damage_iou:.4f}\")\n",
        "\n",
        "    return epoch\n",
        "\n",
        "  def _cleanup_old_checkpoints(self):\n",
        "      \"\"\"Keep only recent checkpoints\"\"\"\n",
        "      checkpoints = sorted(self.checkpoint_dir.glob('checkpoint_epoch_*.pt'))\n",
        "      for old_checkpoint in checkpoints[:-3]:\n",
        "          old_checkpoint.unlink()\n",
        "\n",
        "  def plot_training_history(self):\n",
        "      \"\"\"Generate training history plots\"\"\"\n",
        "      fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "      # Loss plot\n",
        "      ax = axes[0, 0]\n",
        "      epochs = range(1, len(self.train_losses['total']) + 1)\n",
        "      ax.plot(epochs, self.train_losses['flood'], 'b-', label='Flood Loss')\n",
        "      ax.plot(epochs, self.train_losses['damage'], 'r-', label='Damage Loss')\n",
        "      ax.plot(epochs, self.train_losses['total'], 'g--', label='Total Loss')\n",
        "      ax.set_xlabel('Epoch')\n",
        "      ax.set_ylabel('Loss')\n",
        "      ax.set_title('Training Loss History')\n",
        "      ax.legend()\n",
        "      ax.grid(True)\n",
        "\n",
        "      # IoU plot\n",
        "      ax = axes[0, 1]\n",
        "      if self.val_metrics['flood_iou']:\n",
        "          epochs_val = range(1, len(self.val_metrics['flood_iou']) + 1)\n",
        "          ax.plot(epochs_val, self.val_metrics['flood_iou'], 'b-o', label='Flood IoU')\n",
        "          ax.plot(epochs_val, self.val_metrics['damage_iou'], 'r-o', label='Damage IoU')\n",
        "          ax.set_xlabel('Epoch')\n",
        "          ax.set_ylabel('IoU')\n",
        "          ax.set_title('Validation IoU History')\n",
        "          ax.legend()\n",
        "          ax.grid(True)\n",
        "\n",
        "      # Learning rate plot\n",
        "      ax = axes[1, 0]\n",
        "      lrs = [group['lr'] for group in self.optimizer.param_groups]\n",
        "      ax.plot([lrs[0]] * len(epochs), 'b-', label='Backbone LR')\n",
        "      ax.plot([lrs[-1]] * len(epochs), 'r-', label='Head LR')\n",
        "      ax.set_xlabel('Epoch')\n",
        "      ax.set_ylabel('Learning Rate')\n",
        "      ax.set_title('Learning Rate Schedule')\n",
        "      ax.legend()\n",
        "      ax.set_yscale('log')\n",
        "      ax.grid(True)\n",
        "\n",
        "      # Summary text\n",
        "      ax = axes[1, 1]\n",
        "      ax.axis('off')\n",
        "\n",
        "      # Fix the string formatting\n",
        "      final_flood_iou = f\"{self.val_metrics['flood_iou'][-1]:.4f}\" if self.val_metrics['flood_iou'] else \"N/A\"\n",
        "      final_damage_iou = f\"{self.val_metrics['damage_iou'][-1]:.4f}\" if self.val_metrics['damage_iou'] else \"N/A\"\n",
        "      final_flood_loss = f\"{self.train_losses['flood'][-1]:.4f}\" if self.train_losses['flood'] else \"N/A\"\n",
        "      final_damage_loss = f\"{self.train_losses['damage'][-1]:.4f}\" if self.train_losses['damage'] else \"N/A\"\n",
        "\n",
        "      summary_text = f\"\"\"\n",
        "      Training Summary:\n",
        "\n",
        "      Total Epochs: {len(self.train_losses['total'])}\n",
        "      Best Flood IoU: {self.best_flood_iou:.4f}\n",
        "      Best Damage IoU: {self.best_damage_iou:.4f}\n",
        "\n",
        "      Final Training Loss:\n",
        "      - Flood: {final_flood_loss}\n",
        "      - Damage: {final_damage_loss}\n",
        "\n",
        "      Final Validation IoU:\n",
        "      - Flood: {final_flood_iou}\n",
        "      - Damage: {final_damage_iou}\n",
        "      \"\"\"\n",
        "      ax.text(0.1, 0.5, summary_text, fontsize=12,\n",
        "              verticalalignment='center', fontfamily='monospace')\n",
        "\n",
        "      plt.tight_layout()\n",
        "\n",
        "      # Save plot\n",
        "      plot_path = os.path.join(self.config.results_dir, 'training_history.png')\n",
        "      plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
        "      plt.close()\n",
        "\n",
        "      print(f\"Training history plot saved to {plot_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIn9Yb5zoET2"
      },
      "source": [
        "## Main Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "FWiSNbJ9oCeW"
      },
      "outputs": [],
      "source": [
        "def make_safe_dataloader(\n",
        "    dataset,\n",
        "    config,\n",
        "    batch_size,\n",
        "    shuffle\n",
        "):\n",
        "  \"\"\"Create Data Loader that handles 0 num_workers without crashing\"\"\"\n",
        "  num_workers = int(config.num_workers)\n",
        "  kwargs = dict(\n",
        "      batch_size=batch_size,\n",
        "      shuffle=shuffle,\n",
        "      num_workers= num_workers,\n",
        "      pin_memory=config.pin_memory\n",
        "  )\n",
        "  if num_workers > 0:\n",
        "      kwargs.update({\n",
        "      \"prefetch_factor\" : config.prefetch_factor,\n",
        "      \"persistent_workers\" : config.persistent_workers\n",
        "      })\n",
        "  return DataLoader(dataset, **kwargs)\n",
        "\n",
        "@torch.no_grad()\n",
        "def validate_config(config):\n",
        "    \"\"\"Validate configuration before training\"\"\"\n",
        "    print(\"\\nValidating configuration...\")\n",
        "\n",
        "    # Check CUDA\n",
        "    assert torch.cuda.is_available(), \"CUDA not available - A100 required\"\n",
        "\n",
        "    # Check directories\n",
        "    assert config.data_root.exists(), f\"Data root not found: {config.data_root}\"\n",
        "\n",
        "    # Check dataset paths\n",
        "    paths_to_check = [\n",
        "        (\"/content/data/FloodNet\", \"FloodNet\"),\n",
        "        (\"/content/data/RescueNet\", \"RescueNet\"),\n",
        "        (\"/content/data/SpaceNet\", \"SpaceNet8\"),\n",
        "    ]\n",
        "\n",
        "    for path, name in paths_to_check:\n",
        "        if not os.path.exists(path):\n",
        "            print(f\"⚠ Warning: {name} not found at {path}\")\n",
        "\n",
        "    # Check manifest files\n",
        "    if not os.path.exists(sn8_train_manifest):\n",
        "        print(f\"⚠ Warning: SpaceNet8 train manifest not found at {sn8_train_manifest}\")\n",
        "    if not os.path.exists(sn8_val_manifest):\n",
        "        print(f\"⚠ Warning: SpaceNet8 val manifest not found at {sn8_val_manifest}\")\n",
        "\n",
        "    # Check GPU memory\n",
        "    if torch.cuda.is_available():\n",
        "        total_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "        required_mem = 32  # Minimum GB for training\n",
        "        assert total_mem >= required_mem, f\"Need {required_mem}GB GPU, have {total_mem:.1f}GB\"\n",
        "\n",
        "    print(\"✓ Configuration validated\")\n",
        "\n",
        "def compute_or_load_class_weights(dataset, cache_dir, dataset_name):\n",
        "    \"\"\"Compute class weights or load from cache to save 2 hours\"\"\"\n",
        "    cache_file = Path(cache_dir) / f\"{dataset_name}_class_weights.pkl\"\n",
        "\n",
        "    # Try to load from cache\n",
        "    if cache_file.exists():\n",
        "        try:\n",
        "            with open(cache_file, 'rb') as f:\n",
        "                data = pickle.load(f)\n",
        "            print(f\"✅ Loaded cached {dataset_name} weights: {data['weights']}\")\n",
        "            return data['weights']\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️  Cache load failed ({e}), recomputing...\")\n",
        "\n",
        "    # Get num_classes from dataset\n",
        "    # Try to infer from dataset attributes\n",
        "    if hasattr(dataset, 'num_classes'):\n",
        "        num_classes = dataset.num_classes\n",
        "    elif hasattr(dataset, 'datasets'):  # For ConcatDataset\n",
        "        num_classes = dataset.datasets[0].num_classes if hasattr(dataset.datasets[0], 'num_classes') else 2\n",
        "    else:\n",
        "        # Default guess based on name\n",
        "        num_classes = 2 if 'flood' in dataset_name.lower() else 4\n",
        "\n",
        "    # Compute fresh (takes ~1-2 hours)\n",
        "    print(f\"🔄 Computing {dataset_name} class weights (this will take a while)...\")\n",
        "    weights = compute_class_weights(dataset, num_classes)\n",
        "\n",
        "    # Save to cache\n",
        "    try:\n",
        "        cache_file.parent.mkdir(parents=True, exist_ok=True)\n",
        "        with open(cache_file, 'wb') as f:\n",
        "            pickle.dump({\n",
        "                'weights': weights,\n",
        "                'dataset_name': dataset_name,\n",
        "                'num_classes': num_classes,\n",
        "                'num_samples': len(dataset),\n",
        "                'computed_at': str(datetime.now())\n",
        "            }, f)\n",
        "        print(f\"✅ Cached to {cache_file}\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️  Failed to cache: {e}\")\n",
        "\n",
        "    return weights\n",
        "\n",
        "def compute_or_load_class_weights(dataset, cache_dir, dataset_name):\n",
        "    \"\"\"Compute class weights or load from cache to save 2 hours\"\"\"\n",
        "    cache_file = Path(cache_dir) / f\"{dataset_name}_class_weights.pkl\"\n",
        "\n",
        "    # Try to load from cache\n",
        "    if cache_file.exists():\n",
        "        try:\n",
        "            with open(cache_file, 'rb') as f:\n",
        "                data = pickle.load(f)\n",
        "            print(f\"✅ Loaded cached {dataset_name} weights: {data['weights']}\")\n",
        "            return data['weights']\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️  Cache load failed ({e}), recomputing...\")\n",
        "\n",
        "    # Get num_classes from dataset\n",
        "    # Try to infer from dataset attributes\n",
        "    if hasattr(dataset, 'num_classes'):\n",
        "        num_classes = dataset.num_classes\n",
        "    elif hasattr(dataset, 'datasets'):  # For ConcatDataset\n",
        "        num_classes = dataset.datasets[0].num_classes if hasattr(dataset.datasets[0], 'num_classes') else 2\n",
        "    else:\n",
        "        # Default guess based on name\n",
        "        num_classes = 2 if 'flood' in dataset_name.lower() else 4\n",
        "\n",
        "    # Compute fresh (takes ~1-2 hours)\n",
        "    print(f\"🔄 Computing {dataset_name} class weights (this will take a while)...\")\n",
        "    weights = compute_class_weights(dataset, num_classes)\n",
        "\n",
        "    # Save to cache\n",
        "    try:\n",
        "        cache_file.parent.mkdir(parents=True, exist_ok=True)\n",
        "        with open(cache_file, 'wb') as f:\n",
        "            pickle.dump({\n",
        "                'weights': weights,\n",
        "                'dataset_name': dataset_name,\n",
        "                'num_classes': num_classes,\n",
        "                'num_samples': len(dataset),\n",
        "                'computed_at': str(datetime.now())\n",
        "            }, f)\n",
        "        print(f\"✅ Cached to {cache_file}\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️  Failed to cache: {e}\")\n",
        "\n",
        "    return weights\n",
        "\n",
        "\n",
        "def train_complete_model(config):\n",
        "    \"\"\"\n",
        "    Complete training pipeline\n",
        "    \"\"\"\n",
        "    validate_config(config)\n",
        "    # Set device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    if device.type == 'cuda':\n",
        "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "        print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "\n",
        "    # Create datasets\n",
        "    print(\"\\nCreating datasets...\")\n",
        "\n",
        "\n",
        "    # Create augmentation pipelines\n",
        "    train_transform = get_training_augmentation(config)\n",
        "    val_transform = get_validation_augmentation(config)\n",
        "\n",
        "    # Create datasets\n",
        "    flood_train_dataset = FloodNetDataset(\n",
        "        FloodNet_train_img_dir, FloodNet_train_mask_dir, train_transform, binary_flood=True\n",
        "    )\n",
        "\n",
        "    flood_val_dataset = FloodNetDataset(\n",
        "        FloodNet_val_img_dir, FloodNet_val_mask_dir, val_transform, binary_flood=True\n",
        "    )\n",
        "\n",
        "    flood_test_dataset = FloodNetDataset(\n",
        "        FloodNet_test_img_dir, FloodNet_test_mask_dir, val_transform, binary_flood=True\n",
        "    )\n",
        "\n",
        "    damage_train_dataset = RescueNetDataset(\n",
        "        RescueNet_train_img_dir, RescueNet_train_mask_dir, train_transform\n",
        "    )\n",
        "\n",
        "    damage_val_dataset = RescueNetDataset(\n",
        "        RescueNet_val_img_dir, RescueNet_val_mask_dir, val_transform\n",
        "    )\n",
        "\n",
        "    damage_test_dataset = RescueNetDataset(\n",
        "        RescueNet_test_img_dir, RescueNet_test_mask_dir, val_transform\n",
        "    )\n",
        "    # Compute class weights for balanced training\n",
        "    print(\"\\nComputing class weights...\")\n",
        "    flood_class_weights = compute_or_load_class_weights(flood_train_dataset, config.cache_dir, 'floodnet_train')\n",
        "    damage_class_weights = compute_or_load_class_weights(damage_train_dataset, config.cache_dir, 'rescuenet_train')\n",
        "    print(f\"Flood class weights: {flood_class_weights}\")\n",
        "    print(f\"Damage class weights: {damage_class_weights}\")\n",
        "\n",
        "    # Move weights to GPU\n",
        "    if torch.cuda.is_available():\n",
        "        flood_class_weights = flood_class_weights.cuda()\n",
        "        damage_class_weights = damage_class_weights.cuda()\n",
        "\n",
        "    space_train_dataset = SpaceNet8Dataset(\n",
        "        sn8_train_manifest, augment=train_transform,\n",
        "        cache_size=config.spacenet_cache_size, config=config\n",
        "    )\n",
        "\n",
        "    space_val_dataset = SpaceNet8Dataset(\n",
        "        sn8_val_manifest, augment=val_transform,\n",
        "        cache_size=config.spacenet_cache_size, config=config\n",
        "    )\n",
        "\n",
        "    space_train_dataset = SpaceNet8CompatWrapper(space_train_dataset)\n",
        "\n",
        "    space_val_dataset = SpaceNet8CompatWrapper(space_val_dataset)\n",
        "\n",
        "\n",
        "    # Concatenate FloodNet & SpaceNet 8  datasets\n",
        "    flood_train_combined = ConcatDataset([flood_train_dataset, space_train_dataset])\n",
        "    flood_val_combined = ConcatDataset([flood_val_dataset, space_val_dataset])\n",
        "\n",
        "\n",
        "    # Create dataloaders\n",
        "    print(\"Creating dataloaders...\")\n",
        "\n",
        "    flood_train_loader = make_safe_dataloader(\n",
        "        flood_train_combined,\n",
        "        config,\n",
        "        config.batch_size,\n",
        "        True\n",
        "    )\n",
        "\n",
        "    flood_val_loader = make_safe_dataloader(\n",
        "        flood_val_combined,\n",
        "        config,\n",
        "        config.batch_size * 2,\n",
        "        False\n",
        "    )\n",
        "\n",
        "    flood_test_loader = make_safe_dataloader(\n",
        "        flood_test_dataset,\n",
        "        config,\n",
        "        config.batch_size * 2,\n",
        "        False\n",
        "    )\n",
        "\n",
        "    damage_train_loader = make_safe_dataloader(\n",
        "        damage_train_dataset,\n",
        "        config,\n",
        "        config.batch_size,\n",
        "        True\n",
        "    )\n",
        "\n",
        "    damage_val_loader = make_safe_dataloader(\n",
        "        damage_val_dataset,\n",
        "        config,\n",
        "        config.batch_size * 2,\n",
        "        False\n",
        "    )\n",
        "\n",
        "    damage_test_loader = make_safe_dataloader(\n",
        "        damage_test_dataset,\n",
        "        config,\n",
        "        config.batch_size * 2,\n",
        "        False\n",
        "    )\n",
        "\n",
        "    val_loader = {\n",
        "        \"flood\": flood_val_loader,\n",
        "        \"damage\": damage_val_loader\n",
        "    }\n",
        "\n",
        "    test_loader = {\n",
        "        \"flood\": flood_test_loader,\n",
        "        \"damage\": damage_test_loader\n",
        "    }\n",
        "\n",
        "    # Initialize model\n",
        "    print(\"\\nInitializing model...\")\n",
        "    model = EnhancedDisasterModel(\n",
        "        num_classes_flood=config.num_classes_flood,\n",
        "        num_classes_damage=config.num_classes_damage,\n",
        "        backbone=config.backbone,\n",
        "        config=config\n",
        "    )\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Count parameters\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"Total parameters: {total_params:,}\")\n",
        "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "    # Initialize trainer\n",
        "    trainer = OptimizedTrainer(model, config, device)\n",
        "\n",
        "    ckpt_manager = CheckpointManager(model, config, config.checkpoint_dir)\n",
        "\n",
        "    # Optional: Resume from checkpoint\n",
        "    best_combined_score = 0.0\n",
        "    ckpt_path = os.path.join(config.checkpoint_dir, 'best_model.pt')\n",
        "    start_epoch = 0\n",
        "\n",
        "    if os.path.exists(ckpt_path):\n",
        "        if getattr(config, \"auto_resume\", False):\n",
        "            # Auto-resume without prompting\n",
        "            print(f\"Found checkpoint at {ckpt_path}. Auto-resuming...\")\n",
        "            start_epoch = ckpt_manager.load_checkpoint(trainer.model, trainer.optimizer,  trainer.scaler, 'best_model.pt')\n",
        "            if getattr(trainer, \"best_flood_iou\", None) is not None:\n",
        "                best_combined_score = (trainer.best_flood_iou + trainer.best_damage_iou) / 2.0\n",
        "        else:\n",
        "        # Manual prompt\n",
        "            response = input(\"Found existing checkpoint. Resume training? (y/n): \")\n",
        "            if response.lower() == 'y':\n",
        "                start_epoch = ckpt_manager.load_checkpoint(trainer.model, trainer.optimizer, trainer.scaler, 'best_model.pt')\n",
        "                if getattr(trainer, \"best_flood_iou\", None) is not None:\n",
        "                    best_combined_score = (trainer.best_flood_iou + trainer.best_damage_iou) / 2.0\n",
        "                print(f\"Resuming from epoch {start_epoch}\")\n",
        "            else:\n",
        "                print(\"Starting from scratch.\")\n",
        "\n",
        "    # Training loop\n",
        "    print(f\"\\nStarting training from epoch {start_epoch}...\")\n",
        "    print(f\"Total epochs: {config.num_epochs}\")\n",
        "    print(f\"Batch size: {config.batch_size} (effective {config.batch_size * max(1, getattr(config, 'accumulation_steps', 1))})\")\n",
        "\n",
        "\n",
        "    for epoch in range(start_epoch, config.num_epochs):\n",
        "      # Train with bars (Trainer shows per-batch bar)\n",
        "      flood_loss, damage_loss, total_loss = trainer.train_epoch(flood_train_loader, damage_train_loader, epoch)\n",
        "\n",
        "      torch.cuda.empty_cache()\n",
        "\n",
        "      # Validate (you already use tqdm inside validate)\n",
        "      flood_val_loss, flood_iou, flood_class_ious = trainer.validate(flood_val_loader, 'flood',  config.num_classes_flood)\n",
        "      damage_val_loss, damage_iou, damage_class_ious = trainer.validate(damage_val_loader, 'damage', config.num_classes_damage)\n",
        "\n",
        "      torch.cuda.empty_cache()\n",
        "\n",
        "      trainer.val_metrics['flood_iou'].append(flood_iou)\n",
        "      trainer.val_metrics['damage_iou'].append(damage_iou)\n",
        "\n",
        "      combined_score = (flood_iou + damage_iou) / 2.0\n",
        "      is_best = combined_score > best_combined_score\n",
        "      if is_best:\n",
        "        best_combined_score = combined_score\n",
        "        trainer.best_flood_iou  = flood_iou\n",
        "        trainer.best_damage_iou = damage_iou\n",
        "\n",
        "      flood_metrics  = {'iou': flood_iou,  'loss': flood_val_loss,  'class_ious': getattr(flood_class_ious, \"tolist\", lambda: flood_class_ious)()}\n",
        "      damage_metrics = {'iou': damage_iou, 'loss': damage_val_loss, 'class_ious': getattr(damage_class_ious, \"tolist\", lambda: damage_class_ious)()}\n",
        "\n",
        "      ckpt_manager.save_checkpoint(\n",
        "          model=trainer.model,\n",
        "          optimizer=trainer.optimizer,\n",
        "          scaler=trainer.scaler,\n",
        "          epoch=epoch,\n",
        "          flood_metrics=flood_metrics,\n",
        "          damage_metrics=damage_metrics,\n",
        "          is_best=is_best,\n",
        "          train_losses=trainer.train_losses,\n",
        "          val_metrics=trainer.val_metrics,\n",
        "          best_flood_iou=trainer.best_flood_iou,\n",
        "          best_damage_iou=trainer.best_damage_iou\n",
        "      )\n",
        "\n",
        "      ckpt_manager._cleanup_old_checkpoints()\n",
        "\n",
        "      if (epoch + 1) % 5 == 0:\n",
        "        ckpt_manager.plot_training_history()\n",
        "\n",
        "    # Final evaluation on test set\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"FINAL EVALUATION ON TEST SET\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Test flood detection\n",
        "    print(\"\\nFlood Detection Test Results:\")\n",
        "    flood_test_loss, flood_test_iou, flood_test_class_ious = trainer.test(\n",
        "        flood_test_loader, 'flood', config.num_classes_flood\n",
        "    )\n",
        "\n",
        "    # Test damage assessment\n",
        "    print(\"\\nDamage Assessment Test Results:\")\n",
        "    damage_test_loss, damage_test_iou, damage_test_class_ious = trainer.test(\n",
        "        damage_test_loader, 'damage', config.num_classes_damage\n",
        "    )\n",
        "\n",
        "    # Save final results\n",
        "    final_results = {\n",
        "        'flood_test': {\n",
        "            'mean_iou': flood_test_iou,\n",
        "            'class_ious': flood_test_class_ious.tolist(),\n",
        "            'loss': flood_test_loss\n",
        "        },\n",
        "        'damage_test': {\n",
        "            'mean_iou': damage_test_iou,\n",
        "            'class_ious': damage_test_class_ious.tolist(),\n",
        "            'loss': damage_test_loss\n",
        "        },\n",
        "        'training_config': config.__dict__\n",
        "    }\n",
        "\n",
        "    import json\n",
        "    with open(os.path.join(config.results_dir, 'final_results.json'), 'w') as f:\n",
        "        json.dump(final_results, f, indent=2)\n",
        "\n",
        "    print(\"\\nTraining complete!\")\n",
        "    print(f\"Best Flood IoU: {trainer.best_flood_iou:.4f}\")\n",
        "    print(f\"Best Damage IoU: {trainer.best_damage_iou:.4f}\")\n",
        "    print(f\"Results saved to {config.results_dir}\")\n",
        "\n",
        "    return model, trainer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38QRHhtnXXjQ"
      },
      "source": [
        "Usage Monitor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "7dEssuHlXKDq"
      },
      "outputs": [],
      "source": [
        "class GPUMonitor:\n",
        "  \"\"\"Monitor GPU memory usage and prevent OOM\"\"\"\n",
        "\n",
        "  @staticmethod\n",
        "  def get_memory_info():\n",
        "    \"\"\"Get current GPU memory usage\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "      allocated = torch.cuda.memory_allocated()/ 1e9\n",
        "      cached = torch.cuda.memory_reserved() / 1e9\n",
        "      total = torch.cuda.get_device_properties(0).total_memory\n",
        "      return {\n",
        "          'allocated' : allocated,\n",
        "          \"cached\" : cached,\n",
        "          \"total\" : total,\n",
        "          \"free\" : total - allocated,\n",
        "          \"usage_percent\" : (allocated / total ) * 100\n",
        "      }\n",
        "    return None\n",
        "\n",
        "  @staticmethod\n",
        "  def print_memory_stats():\n",
        "    \"\"\"Print current GPU memory usage\"\"\"\n",
        "    info = GPUMonitor.get_memory_info()\n",
        "    if info:\n",
        "      print(f\"GPU Memory Usage: {info['allocated']:.1f}/{info['total']:.1f} GB\"\n",
        "      f\"({info['usage_percent']:.1f}%) - Free: {info['free']:.1f} GB\")\n",
        "\n",
        "  @staticmethod\n",
        "  def clear_cache_if_needed(threshold=.9):\n",
        "    \"\"\"Clear cache if memory usage exceeds threshold\"\"\"\n",
        "    info = GPUMonitor.get_memory_info()\n",
        "    if info and info ['usage_percent'] > threshold * 100:\n",
        "      torch.cuda.empty_cache()\n",
        "      gc.collect()\n",
        "      print(f\"Cleared GPU cache at {info['usage_percent']:.1f}% usage\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZt5LDyhpzEU"
      },
      "source": [
        "## Diagnostic tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "8BnrtAN3jZ4Z",
        "outputId": "6a22e79e-4bd3-4ded-c1b1-c9f955759026"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nimport os\\nimport subprocess\\n\\nprint(\"=\"*70)\\nprint(\"DATA LOCATION CHECK\")\\nprint(\"=\"*70)\\n\\n# Check if data directories exist\\ndata_paths = [\\n    \"/content/data/FloodNet\",\\n    \"/content/data/RescueNet\",\\n    \"/content/data/SpaceNet\"\\n]\\n\\nfor path in data_paths:\\n    if os.path.exists(path):\\n        # Check if it\\'s a symlink (bad - points to Drive)\\n        if os.path.islink(path):\\n            print(f\"⚠️  {path}\")\\n            print(f\"    -> SYMLINK to {os.readlink(path)} (SLOW!)\")\\n        else:\\n            # Check size and file count\\n            result = subprocess.run([\\'du\\', \\'-sh\\', path], capture_output=True, text=True)\\n            size = result.stdout.split()[0] if result.returncode == 0 else \"unknown\"\\n\\n            file_count = subprocess.run([\\'find\\', path, \\'-type\\', \\'f\\', \\'|\\', \\'wc\\', \\'-l\\'],\\n                                       shell=True, capture_output=True, text=True)\\n            files = file_count.stdout.strip() if file_count.returncode == 0 else \"unknown\"\\n\\n            print(f\"✅ {path}\")\\n            print(f\"    Size: {size}, Files: {files}\")\\n            print(f\"    -> LOCAL (FAST)\")\\n    else:\\n        print(f\"❌ {path} - NOT FOUND!\")\\n\\n# Check where /content/data is mounted\\nprint(\"\\n\" + \"=\"*70)\\nprint(\"FILESYSTEM CHECK\")\\nprint(\"=\"*70)\\nresult = subprocess.run([\\'df\\', \\'-h\\', \\'/content\\'], capture_output=True, text=True)\\nprint(result.stdout)\\n\\n# If you see \\'drive\\' or \\'fuse\\' in filesystem, data is on Google Drive (slow!)\\n# If you see \\'overlay\\' or \\'/dev/sda\\', data is on local disk (fast!)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "\"\"\"\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"DATA LOCATION CHECK\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Check if data directories exist\n",
        "data_paths = [\n",
        "    \"/content/data/FloodNet\",\n",
        "    \"/content/data/RescueNet\",\n",
        "    \"/content/data/SpaceNet\"\n",
        "]\n",
        "\n",
        "for path in data_paths:\n",
        "    if os.path.exists(path):\n",
        "        # Check if it's a symlink (bad - points to Drive)\n",
        "        if os.path.islink(path):\n",
        "            print(f\"⚠️  {path}\")\n",
        "            print(f\"    -> SYMLINK to {os.readlink(path)} (SLOW!)\")\n",
        "        else:\n",
        "            # Check size and file count\n",
        "            result = subprocess.run(['du', '-sh', path], capture_output=True, text=True)\n",
        "            size = result.stdout.split()[0] if result.returncode == 0 else \"unknown\"\n",
        "\n",
        "            file_count = subprocess.run(['find', path, '-type', 'f', '|', 'wc', '-l'],\n",
        "                                       shell=True, capture_output=True, text=True)\n",
        "            files = file_count.stdout.strip() if file_count.returncode == 0 else \"unknown\"\n",
        "\n",
        "            print(f\"✅ {path}\")\n",
        "            print(f\"    Size: {size}, Files: {files}\")\n",
        "            print(f\"    -> LOCAL (FAST)\")\n",
        "    else:\n",
        "        print(f\"❌ {path} - NOT FOUND!\")\n",
        "\n",
        "# Check where /content/data is mounted\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"FILESYSTEM CHECK\")\n",
        "print(\"=\"*70)\n",
        "result = subprocess.run(['df', '-h', '/content'], capture_output=True, text=True)\n",
        "print(result.stdout)\n",
        "\n",
        "# If you see 'drive' or 'fuse' in filesystem, data is on Google Drive (slow!)\n",
        "# If you see 'overlay' or '/dev/sda', data is on local disk (fast!)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "2BG8OWEToQwc"
      },
      "outputs": [],
      "source": [
        "#\"\"\"\n",
        "#import os\n",
        "#import subprocess\n",
        "\n",
        "# Quick check\n",
        "#print(\"Does data exist?\")\n",
        "#print(f\"FloodNet: {os.path.exists('/content/data/FloodNet')}\")\n",
        "#print(f\"RescueNet: {os.path.exists('/content/data/RescueNet')}\")\n",
        "#print(f\"SpaceNet: {os.path.exists('/content/data/SpaceNet')}\")\n",
        "\n",
        "# Check if symlinks\n",
        "#print(\"\\nAre they symlinks?\")\n",
        "#print(f\"FloodNet: {os.path.islink('/content/data/FloodNet')}\")\n",
        "\n",
        "# Check filesystem\n",
        "#print(\"\\nFilesystem:\")\n",
        "#result = subprocess.run(['df', '-h', '/content/data'], capture_output=True, text=True)\n",
        "#print(result.stdout)\n",
        "#\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqiyYmzSXYRG"
      },
      "source": [
        "## Initiate Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "uo4oz6kitqtH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "8eefaa546f7a4117b30e5ed675d1e301",
            "293101e161064e72bf97a0bb122a1d6c",
            "772265f8de7444b4b27d95f60d32bc4c",
            "f70b715e760c4de28a9f58dc502ec6ca",
            "03dd781deb05431988da74a55904475d",
            "df7650704b004901bfd5007696861f72",
            "d935f5de32b24b3ab2d71572bf1d6650",
            "73a8e8b9462542538f6915036f2dc93b",
            "77a2ad2953cf49aea8ab5bcbd230865b",
            "8d6202020c4047008167a63d46001f46",
            "93f4abfe8803411f948c73ac07ca1060",
            "c9c4fd355da24855bea1e5bbe879cde2",
            "948e18c108fa4f9d8f093d07f84ca4dc",
            "c8fedbffbafe4816b62ce08454f9d143",
            "b43405c56645462799fa518f65e19bba",
            "426eb4686735429493646e64e969f9e2",
            "daeb4c523fc8413db0052d1c9076c662",
            "0f48ecb0e7fb43febd32777bfa859bfb",
            "a33692f1704d4c2ba21c5f932445a177",
            "28ba87dbbde244e5a75496124e008e79",
            "e6df635abc4c4e55aa52007cdbf950e8",
            "1fb91d991ef74154acfd89dc40017589",
            "deb5dd9bdfbc4ebcb3a5818b229e041e",
            "818f864f4eca428783ff03dc0d01ad8f",
            "c1c7d886e4bf46ffab1d54358751b211",
            "1183f2853e4a439993f3aa685da995ed",
            "71d431e507464f2e8290930df5624cb2",
            "bf4b20a6bc67457eb93c226c6d98c594",
            "f6e1d851aec04676bffb36aa037bdaeb",
            "1a421065dcc44b2ba29c68189bbdaf15",
            "173534e0d0fa4024820fb53f5c81c7aa",
            "c3ebe8876e184f619736fbaad7c58e01",
            "07dee3b50f584902b805428ec00c0649"
          ]
        },
        "outputId": "c622d0b2-d8b5-4604-ea40-fccf3afc3fce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ A100 detected: NVIDIA A100-SXM4-80GB\n",
            "  Memory: 85.2 GB\n",
            "Directories initialized in /content\n",
            "Device: cuda\n",
            "\n",
            "Validating configuration...\n",
            "✓ Configuration validated\n",
            "Using device: cuda\n",
            "GPU: NVIDIA A100-SXM4-80GB\n",
            "Memory: 85.2 GB\n",
            "\n",
            "Creating datasets...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/albumentations/core/validation.py:114: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
            "  original_init(self, **validated_kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FloodNet dataset: 1445 images, 1445 with masks\n",
            "FloodNet dataset: 450 images, 450 with masks\n",
            "FloodNet dataset: 448 images, 448 with masks\n",
            "\n",
            "Computing class weights...\n",
            "🔄 Computing floodnet_train class weights (this will take a while)...\n",
            "Fast class weight computation (bypassing augmentation)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Analyzing masks:   0%|          | 0/1445 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8eefaa546f7a4117b30e5ed675d1e301"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Cached to /content/cache/floodnet_train_class_weights.pkl\n",
            "🔄 Computing rescuenet_train class weights (this will take a while)...\n",
            "Fast class weight computation (bypassing augmentation)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Analyzing masks:   0%|          | 0/3595 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c9c4fd355da24855bea1e5bbe879cde2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Cached to /content/cache/rescuenet_train_class_weights.pkl\n",
            "Flood class weights: tensor([nan, nan])\n",
            "Damage class weights: tensor([nan, nan, nan, nan])\n",
            "Prioritized 158 samples by disaster content likelihood\n",
            "Preloading top 10 disaster samples to A100 GPU memory...\n",
            "  Adjusted preload count to 10 based on available memory\n",
            "Successfully preloaded 10 disaster-rich samples to GPU\n",
            "Disaster-focused SpaceNet8: 158 samples\n",
            "Cache size: 50, GPU preloaded: 10\n",
            "Prioritized 44 samples by disaster content likelihood\n",
            "Preloading top 10 disaster samples to A100 GPU memory...\n",
            "  Adjusted preload count to 10 based on available memory\n",
            "Successfully preloaded 10 disaster-rich samples to GPU\n",
            "Disaster-focused SpaceNet8: 44 samples\n",
            "Cache size: 50, GPU preloaded: 10\n",
            "Creating dataloaders...\n",
            "\n",
            "Initializing model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DeepLabV3_ResNet101_Weights.COCO_WITH_VOC_LABELS_V1`. You can also use `weights=DeepLabV3_ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/deeplabv3_resnet101_coco-586e9e4e.pth\" to /root/.cache/torch/hub/checkpoints/deeplabv3_resnet101_coco-586e9e4e.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 233M/233M [00:01<00:00, 198MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using ResNet101 backbone\n",
            "Total parameters: 63,422,632\n",
            "Trainable parameters: 63,422,632\n",
            "A100 optimizations enabled\n",
            "Disaster mapping trainer initialized for A100\n",
            "\n",
            "Starting training from epoch 0...\n",
            "Total epochs: 50\n",
            "Batch size: 32 (effective 128)\n",
            "Disaster mapping training epoch 0: 113 iterations\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 1:   0%|          | 0/113 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "deb5dd9bdfbc4ebcb3a5818b229e041e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2769196445.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_complete_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1052874934.py\u001b[0m in \u001b[0;36mtrain_complete_model\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    331\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m       \u001b[0;31m# Train with bars (Trainer shows per-batch bar)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m       \u001b[0mflood_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdamage_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflood_train_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdamage_train_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m       \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2397027747.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(self, flood_loader, damage_loader, epoch)\u001b[0m\n\u001b[1;32m    114\u001b[0m           \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m               \u001b[0mdata_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m               \u001b[0mflood_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflood_cycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m               \u001b[0mdamage_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdamage_cycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m               \u001b[0mtimes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data_load'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdata_start\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    732\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1491\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1492\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1494\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1442\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1443\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1444\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1445\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1446\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1283\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1285\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1286\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    357\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "if __name__ == '__main__':\n",
        "  config = Config()\n",
        "  model, trainer = train_complete_model(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZJYT4GVWsFt"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi --query-gpu=utilization.gpu,utilization.memory,memory.used,memory.total --format=csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NP1v1bOYXNVK"
      },
      "source": [
        "##Model Demonstration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9W7AkS77xDtf"
      },
      "outputs": [],
      "source": [
        "# def test_on_new_image_with_tiff(model_path, image_path):\n",
        "#     import torch\n",
        "#     import cv2\n",
        "#     import numpy as np\n",
        "#     import matplotlib.pyplot as plt\n",
        "#     from PIL import Image\n",
        "#     import os\n",
        "\n",
        "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "#     try:\n",
        "#         # Load model\n",
        "#         checkpoint = torch.load(model_path, map_location=device, weights_only=False)\n",
        "\n",
        "#         model = EnhancedDisasterModel(\n",
        "#             num_classes_flood=2,\n",
        "#             num_classes_damage=4,\n",
        "#             backbone='resnet101'\n",
        "#         )\n",
        "#         model.load_state_dict(checkpoint['model_state_dict'])\n",
        "#         model.to(device)\n",
        "#         model.eval()\n",
        "\n",
        "#         # Check file extension and load accordingly\n",
        "#         file_ext = os.path.splitext(image_path)[1].lower()\n",
        "#         print(f\"  → Processing {file_ext} file: {os.path.basename(image_path)}\")\n",
        "\n",
        "#         if file_ext in ['.tif', '.tiff']:\n",
        "#             # Use PIL for TIFF files (better TIFF support)\n",
        "#             print(\"  → Using PIL for TIFF loading...\")\n",
        "#             try:\n",
        "#                 image_pil = Image.open(image_path)\n",
        "\n",
        "#                 # Handle different TIFF modes\n",
        "#                 if image_pil.mode == 'RGB':\n",
        "#                     image = np.array(image_pil)\n",
        "#                 elif image_pil.mode == 'RGBA':\n",
        "#                     # Convert RGBA to RGB\n",
        "#                     image = np.array(image_pil.convert('RGB'))\n",
        "#                 elif image_pil.mode in ['L', 'P']:\n",
        "#                     # Convert grayscale or palette to RGB\n",
        "#                     image = np.array(image_pil.convert('RGB'))\n",
        "#                 elif image_pil.mode == 'I' or image_pil.mode == 'F':\n",
        "#                     # Handle 32-bit integer or float images\n",
        "#                     arr = np.array(image_pil)\n",
        "#                     # Normalize to 0-255 range\n",
        "#                     arr = ((arr - arr.min()) / (arr.max() - arr.min()) * 255).astype(np.uint8)\n",
        "#                     image = np.stack([arr, arr, arr], axis=-1)  # Convert to RGB\n",
        "#                 else:\n",
        "#                     print(f\"  → Converting from mode {image_pil.mode} to RGB\")\n",
        "#                     image = np.array(image_pil.convert('RGB'))\n",
        "\n",
        "#                 print(f\"  → TIFF image loaded: {image.shape}, dtype: {image.dtype}\")\n",
        "\n",
        "#             except Exception as e:\n",
        "#                 print(f\"  ✗ PIL failed, trying OpenCV for TIFF: {e}\")\n",
        "#                 # Fallback to OpenCV\n",
        "#                 image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
        "#                 if image is None:\n",
        "#                     raise ValueError(f\"Could not load TIFF image from {image_path}\")\n",
        "#                 image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "#         else:\n",
        "#             # Use OpenCV for standard formats (PNG, JPG, JPEG)\n",
        "#             print(\"  → Using OpenCV for standard image loading...\")\n",
        "#             image = cv2.imread(image_path)\n",
        "#             if image is None:\n",
        "#                 raise ValueError(f\"Could not load image from {image_path}\")\n",
        "#             image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "#         print(f\"  → Final image shape: {image.shape}, dtype: {image.dtype}\")\n",
        "\n",
        "#         # Ensure image is in correct format (0-255, uint8)\n",
        "#         if image.dtype != np.uint8:\n",
        "#             if image.max() <= 1.0:\n",
        "#                 # Image is in 0-1 range, convert to 0-255\n",
        "#                 image = (image * 255).astype(np.uint8)\n",
        "#             else:\n",
        "#                 # Image might be in different range, normalize\n",
        "#                 image = ((image - image.min()) / (image.max() - image.min()) * 255).astype(np.uint8)\n",
        "\n",
        "#         # Apply transforms\n",
        "#         transform = get_validation_augmentation()\n",
        "#         input_tensor = transform(image=image)['image']\n",
        "#         input_tensor = input_tensor.unsqueeze(0).to(device)\n",
        "\n",
        "#         # Make predictions\n",
        "#         with torch.no_grad():\n",
        "#             flood_out, damage_out = model(input_tensor)\n",
        "\n",
        "#         flood_pred = torch.argmax(flood_out, dim=1)[0].cpu().numpy()\n",
        "#         damage_pred = torch.argmax(damage_out, dim=1)[0].cpu().numpy()\n",
        "\n",
        "#         # Visualize results\n",
        "#         fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "#         axes[0].imshow(image)\n",
        "#         axes[0].set_title(f'Original Image ({file_ext})')\n",
        "#         axes[0].axis('off')\n",
        "\n",
        "#         axes[1].imshow(flood_pred, cmap='Blues')\n",
        "#         axes[1].set_title('Flood Prediction')\n",
        "#         axes[1].axis('off')\n",
        "\n",
        "#         axes[2].imshow(damage_pred, cmap='Reds')\n",
        "#         axes[2].set_title('Damage Prediction')\n",
        "#         axes[2].axis('off')\n",
        "\n",
        "#         plt.tight_layout()\n",
        "#         plt.show()\n",
        "\n",
        "#         return flood_pred, damage_pred\n",
        "\n",
        "#     except Exception as e:\n",
        "#         print(f\"  ✗ ERROR processing {image_path}: {str(e)}\")\n",
        "#         return None, None\n",
        "\n",
        "\n",
        "# import os\n",
        "# import random\n",
        "\n",
        "# def test_on_images_with_tiff(model_path, folder_path, num_img=10):\n",
        "#     \"\"\"\n",
        "#     Test the disaster model on multiple images including TIFF files.\n",
        "\n",
        "#     Supported formats: PNG, JPG, JPEG, TIF, TIFF\n",
        "#     \"\"\"\n",
        "#     try:\n",
        "#         # Updated file extension list to include TIFF\n",
        "#         image_extensions = (\".png\", \".jpg\", \".jpeg\", \".tif\", \".tiff\")\n",
        "#         image_files = [f for f in os.listdir(folder_path)\n",
        "#                       if f.lower().endswith(image_extensions)]\n",
        "\n",
        "#         if not image_files:\n",
        "#             print(f\"No supported image files found in {folder_path}\")\n",
        "#             print(f\"Looking for: {image_extensions}\")\n",
        "#             all_files = os.listdir(folder_path)[:10]  # Show first 10 files\n",
        "#             print(f\"Files in folder: {all_files}\")\n",
        "#             return []\n",
        "\n",
        "#         print(f\"Found {len(image_files)} supported images\")\n",
        "\n",
        "#         # Group by file type for info\n",
        "#         file_types = {}\n",
        "#         for f in image_files:\n",
        "#             ext = os.path.splitext(f)[1].lower()\n",
        "#             file_types[ext] = file_types.get(ext, 0) + 1\n",
        "\n",
        "#         print(f\"File type breakdown: {file_types}\")\n",
        "\n",
        "#         # Sample random images\n",
        "#         sample_files = random.sample(image_files, min(num_img, len(image_files)))\n",
        "#         print(f\"Processing {len(sample_files)} images...\")\n",
        "\n",
        "#         results = []\n",
        "#         successful_predictions = 0\n",
        "\n",
        "#         for i, file_name in enumerate(sample_files, 1):\n",
        "#             file_path = os.path.join(folder_path, file_name)\n",
        "#             print(f\"\\n[{i}/{len(sample_files)}] Processing: {file_name}\")\n",
        "\n",
        "#             flood_pred, damage_pred = test_on_new_image_with_tiff(model_path, file_path)\n",
        "\n",
        "#             if flood_pred is not None and damage_pred is not None:\n",
        "#                 results.append({\n",
        "#                     \"file\": file_name,\n",
        "#                     \"flood_mask\": flood_pred,\n",
        "#                     \"damage_mask\": damage_pred\n",
        "#                 })\n",
        "#                 successful_predictions += 1\n",
        "#                 print(f\"  ✓ Success!\")\n",
        "#             else:\n",
        "#                 print(f\"  ✗ Failed\")\n",
        "\n",
        "#         print(f\"\\nCompleted: {successful_predictions}/{len(sample_files)} images processed successfully\")\n",
        "#         return results\n",
        "\n",
        "#     except Exception as e:\n",
        "#         print(f\"Error in test_on_images_with_tiff: {str(e)}\")\n",
        "#         return []\n",
        "\n",
        "\n",
        "# # Quick check function to see what file types are in your folder\n",
        "# def check_file_types(folder_path):\n",
        "#     \"\"\"Check what file types are present in the folder\"\"\"\n",
        "#     print(f\"=== File Analysis for: {folder_path} ===\")\n",
        "\n",
        "#     if not os.path.exists(folder_path):\n",
        "#         print(f\"Folder does not exist!\")\n",
        "#         return\n",
        "\n",
        "#     all_files = os.listdir(folder_path)\n",
        "#     print(f\"Total files: {len(all_files)}\")\n",
        "\n",
        "#     # Group by extension\n",
        "#     extensions = {}\n",
        "#     for f in all_files:\n",
        "#         ext = os.path.splitext(f)[1].lower()\n",
        "#         if not ext:\n",
        "#             ext = \"(no extension)\"\n",
        "#         extensions[ext] = extensions.get(ext, 0) + 1\n",
        "\n",
        "#     print(f\"\\nFile extensions found:\")\n",
        "#     for ext, count in sorted(extensions.items()):\n",
        "#         print(f\"  {ext}: {count} files\")\n",
        "\n",
        "#     # Check for TIFF specifically\n",
        "#     tiff_files = [f for f in all_files if f.lower().endswith(('.tif', '.tiff'))]\n",
        "#     if tiff_files:\n",
        "#         print(f\"\\nTIFF files found: {len(tiff_files)}\")\n",
        "#         print(f\"First few TIFF files: {tiff_files[:5]}\")\n",
        "#     else:\n",
        "#         print(f\"\\nNo TIFF files found.\")\n",
        "\n",
        "\n",
        "# # Usage example\n",
        "# if __name__ == \"__main__\":\n",
        "#     model_path = \"/content/checkpoints/best_model.pth\"\n",
        "#     folder_path = \"/content/drive/MyDrive/G.E.M.S./FloodNet/FloodNet-Supervised_v1.0/test/test-label-img/\"\n",
        "\n",
        "#     # First check what file types you have\n",
        "#     check_file_types(folder_path)\n",
        "\n",
        "#     # Then run with TIFF support\n",
        "#     if os.path.exists(model_path) and os.path.exists(folder_path):\n",
        "#         results = test_on_images_with_tiff(model_path, folder_path, num_img=10)\n",
        "#         print(f\"Processing complete. Got results for {len(results)} images.\")\n",
        "#     else:\n",
        "#         print(\"Missing model or folder path\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SgqmnrpqkY_w"
      },
      "outputs": [],
      "source": [
        "#!pip install nbconvert\n",
        "#!jupyter nbconvert --to html Hermes.0.3.6.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZslDks6aWCEq"
      },
      "outputs": [],
      "source": [
        "# print(f\"Flood pixels: {(flood_mask == 1).sum()}\")\n",
        "# print(f\"Damage distribution: {np.bincount(damage_mask.flatten())}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8eefaa546f7a4117b30e5ed675d1e301": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_293101e161064e72bf97a0bb122a1d6c",
              "IPY_MODEL_772265f8de7444b4b27d95f60d32bc4c",
              "IPY_MODEL_f70b715e760c4de28a9f58dc502ec6ca"
            ],
            "layout": "IPY_MODEL_03dd781deb05431988da74a55904475d"
          }
        },
        "293101e161064e72bf97a0bb122a1d6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df7650704b004901bfd5007696861f72",
            "placeholder": "​",
            "style": "IPY_MODEL_d935f5de32b24b3ab2d71572bf1d6650",
            "value": "Analyzing masks: 100%"
          }
        },
        "772265f8de7444b4b27d95f60d32bc4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_73a8e8b9462542538f6915036f2dc93b",
            "max": 1445,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_77a2ad2953cf49aea8ab5bcbd230865b",
            "value": 1445
          }
        },
        "f70b715e760c4de28a9f58dc502ec6ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d6202020c4047008167a63d46001f46",
            "placeholder": "​",
            "style": "IPY_MODEL_93f4abfe8803411f948c73ac07ca1060",
            "value": " 1445/1445 [00:00&lt;00:00, 154753.58it/s]"
          }
        },
        "03dd781deb05431988da74a55904475d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df7650704b004901bfd5007696861f72": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d935f5de32b24b3ab2d71572bf1d6650": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "73a8e8b9462542538f6915036f2dc93b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77a2ad2953cf49aea8ab5bcbd230865b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8d6202020c4047008167a63d46001f46": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93f4abfe8803411f948c73ac07ca1060": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c9c4fd355da24855bea1e5bbe879cde2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_948e18c108fa4f9d8f093d07f84ca4dc",
              "IPY_MODEL_c8fedbffbafe4816b62ce08454f9d143",
              "IPY_MODEL_b43405c56645462799fa518f65e19bba"
            ],
            "layout": "IPY_MODEL_426eb4686735429493646e64e969f9e2"
          }
        },
        "948e18c108fa4f9d8f093d07f84ca4dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_daeb4c523fc8413db0052d1c9076c662",
            "placeholder": "​",
            "style": "IPY_MODEL_0f48ecb0e7fb43febd32777bfa859bfb",
            "value": "Analyzing masks: 100%"
          }
        },
        "c8fedbffbafe4816b62ce08454f9d143": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a33692f1704d4c2ba21c5f932445a177",
            "max": 3595,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_28ba87dbbde244e5a75496124e008e79",
            "value": 3595
          }
        },
        "b43405c56645462799fa518f65e19bba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e6df635abc4c4e55aa52007cdbf950e8",
            "placeholder": "​",
            "style": "IPY_MODEL_1fb91d991ef74154acfd89dc40017589",
            "value": " 3595/3595 [00:00&lt;00:00, 301076.69it/s]"
          }
        },
        "426eb4686735429493646e64e969f9e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "daeb4c523fc8413db0052d1c9076c662": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f48ecb0e7fb43febd32777bfa859bfb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a33692f1704d4c2ba21c5f932445a177": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "28ba87dbbde244e5a75496124e008e79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e6df635abc4c4e55aa52007cdbf950e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1fb91d991ef74154acfd89dc40017589": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "deb5dd9bdfbc4ebcb3a5818b229e041e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_818f864f4eca428783ff03dc0d01ad8f",
              "IPY_MODEL_c1c7d886e4bf46ffab1d54358751b211",
              "IPY_MODEL_1183f2853e4a439993f3aa685da995ed"
            ],
            "layout": "IPY_MODEL_71d431e507464f2e8290930df5624cb2"
          }
        },
        "818f864f4eca428783ff03dc0d01ad8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf4b20a6bc67457eb93c226c6d98c594",
            "placeholder": "​",
            "style": "IPY_MODEL_f6e1d851aec04676bffb36aa037bdaeb",
            "value": "Epoch 1:   0%"
          }
        },
        "c1c7d886e4bf46ffab1d54358751b211": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a421065dcc44b2ba29c68189bbdaf15",
            "max": 113,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_173534e0d0fa4024820fb53f5c81c7aa",
            "value": 0
          }
        },
        "1183f2853e4a439993f3aa685da995ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c3ebe8876e184f619736fbaad7c58e01",
            "placeholder": "​",
            "style": "IPY_MODEL_07dee3b50f584902b805428ec00c0649",
            "value": " 0/113 [20:36&lt;?, ?it/s]"
          }
        },
        "71d431e507464f2e8290930df5624cb2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf4b20a6bc67457eb93c226c6d98c594": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6e1d851aec04676bffb36aa037bdaeb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1a421065dcc44b2ba29c68189bbdaf15": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "173534e0d0fa4024820fb53f5c81c7aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c3ebe8876e184f619736fbaad7c58e01": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "07dee3b50f584902b805428ec00c0649": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}