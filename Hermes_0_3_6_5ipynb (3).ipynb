{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WR_rxRE2VCL3"
      },
      "source": [
        "# HERMES 0.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Daw6wtpWUoF"
      },
      "outputs": [],
      "source": [
        "#import signal\n",
        "#os.kill(os.getpid(), signal.SIGKILL)\n",
        "#!rm -rf /content/*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pj_6OIcsWjRn"
      },
      "outputs": [],
      "source": [
        "!pip install albumentations tqdm segmentation-models-pytorch torchvision\n",
        "!pip install --upgrade \"docutils>=0.20,<0.22\"\n",
        "!pip install awscli -q\n",
        "!pip install rasterio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBVFudt5U7Y5"
      },
      "source": [
        "## IMPORTS AND DEPENDENCIES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J00SrcNPVyO6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
        "from torch.amp import GradScaler, autocast\n",
        "from torch.nn.parallel import DataParallel\n",
        "\n",
        "#Computer Vision and image processing\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "#Torchvision for pretrained models and transforms\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.models.segmentation import deeplabv3_resnet50, deeplabv3_resnet101\n",
        "\n",
        "#Data augmentation library\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "#Utilities\n",
        "import os\n",
        "import glob\n",
        "import time\n",
        "import zipfile\n",
        "import shutil\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import json\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "import rasterio as rio\n",
        "from tqdm import tqdm\n",
        "from itertools import cycle\n",
        "from functools import lru_cache\n",
        "from collections import defaultdict\n",
        "from datetime import timedelta\n",
        "import psutil\n",
        "\n",
        "#Optional: Weights & Biases for experiment tracking\n",
        "#try:\n",
        "  #import wandb\n",
        "  #WANDB_AVAILABLE = True\n",
        "#except ImportError:\n",
        "  #WANDB_AVAILABLE = False\n",
        "  #print(\"Warning: Weights & Biases not installed. Please install with `pip install wandb`\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iT9TVIsFU3DE"
      },
      "source": [
        "## Config\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBet-zahUlez"
      },
      "outputs": [],
      "source": [
        "#Set random seeds for reproducbility\n",
        "def set_seed(seed=42):\n",
        "  \"\"\"Set random seeds for reproducibility\"\"\"\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed(seed)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "#Configuration and Parameters\n",
        "\n",
        "class Config:\n",
        "  #model architecture\n",
        "  backbone = 'resnet101'\n",
        "  num_classes_flood = 2\n",
        "  num_classes_damage = 4\n",
        "  #training parameters\n",
        "  batch_size= 32\n",
        "  accumulation_steps = 6\n",
        "  num_epochs = 20\n",
        "  learning_rate = 8e-5\n",
        "  weight_decay = 1e-4\n",
        "  #data loading\n",
        "  num_workers = 12\n",
        "  pin_memory = True\n",
        "  prefetch_factor = 6\n",
        "  persistent_workers = True\n",
        "  spacenet_cache_size=500\n",
        "  # image dimensions\n",
        "  img_height = 512\n",
        "  img_width = 512\n",
        "  # loss weighs\n",
        "  ce_weight = 0.3\n",
        "  dice_weight = 0.4\n",
        "  focal_weight = 0.3\n",
        "  # task weights for mulit-tasking learning\n",
        "  flood_task_weight = 0.6\n",
        "  damage_task_weight = 0.4\n",
        "\n",
        "  # Paths (update these based on your setup will need to edit)\n",
        "  Data_Root = '/content'\n",
        "  Checkpoint_Dir = '/content/checkpoints'\n",
        "  Results_Dir = '/content/results'\n",
        "\n",
        "  # Create directories\n",
        "  os.makedirs(Checkpoint_Dir, exist_ok=True)\n",
        "  os.makedirs(Results_Dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IVeWK3MCoo1X"
      },
      "outputs": [],
      "source": [
        "#Pulling FloodNet dataset from GitHub DropBox due to file size at ~12GB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-aG659vxGq-t"
      },
      "outputs": [],
      "source": [
        "#!wget \"https://www.dropbox.com/scl/fo/k33qdif15ns2qv2jdxvhx/ANGaa8iPRhvlrvcKXjnmNRc?rlkey=ao2493wzl1cltonowjdbrnp7f&e=3&st=6lg4ncwc&dl=1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWkKOToKG0fn"
      },
      "outputs": [],
      "source": [
        "# import zipfile\n",
        "\n",
        "# zip_path = '/content/ANGaa8iPRhvlrvcKXjnmNRc?rlkey=ao2493wzl1cltonowjdbrnp7f&e=3&st=6lg4ncwc&dl=1'\n",
        "# extract_path = '/content/FloodNet'\n",
        "\n",
        "# with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "#   zip_ref.extractall(extract_path)\n",
        "\n",
        "#   print(\"Extraction complete.\")\n",
        "#   print(os.listdir('/content/FloodNet'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G09dsszhHBUZ"
      },
      "outputs": [],
      "source": [
        "#Defining dataset pathways\n",
        "FloodNet_train_img_dir = '/content/drive/MyDrive/G.E.M.S./FloodNet/FloodNet-Supervised_v1.0/train/train-org-img'\n",
        "FloodNet_train_mask_dir = '/content/drive/MyDrive/G.E.M.S./FloodNet/FloodNet-Supervised_v1.0/train/train-label-img'\n",
        "\n",
        "\n",
        "FloodNet_val_img_dir ='/content/drive/MyDrive/G.E.M.S./FloodNet/FloodNet-Supervised_v1.0/val/val-org-img'\n",
        "FloodNet_val_mask_dir = '/content/drive/MyDrive/G.E.M.S./FloodNet/FloodNet-Supervised_v1.0/val/val-label-img'\n",
        "\n",
        "FloodNet_test_img_dir = '/content/drive/MyDrive/G.E.M.S./FloodNet/FloodNet-Supervised_v1.0/test/test-org-img'\n",
        "FloodNet_test_mask_dir = '/content/drive/MyDrive/G.E.M.S./FloodNet/FloodNet-Supervised_v1.0/test/test-label-img'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55E7uZP5Ikmy"
      },
      "outputs": [],
      "source": [
        "# os.remove(\"/content/ANGaa8iPRhvlrvcKXjnmNRc?rlkey=ao2493wzl1cltonowjdbrnp7f&e=3&st=6lg4ncwc&dl=1\")\n",
        "# print(\"Zip File Deleted\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q5htIfoogw0N"
      },
      "outputs": [],
      "source": [
        "#with zipfile.ZipFile(\"/content/drive/MyDrive/G.E.M.S./.zip\", \"r\") as zip_ref:\n",
        "  #zip_ref.extractall(\"/content/SN8\")\n",
        "\n",
        "#print(\"Extraction complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yB7J5MuAhM0W"
      },
      "outputs": [],
      "source": [
        "SpaceNet8_train_img_dir = '/drive/MyDrive/G.E.M.S./sn8/images/train'\n",
        "SpaceNet8_train_mask_dir = '/drive/MyDrive/G.E.M.S./sn8/masks/train'\n",
        "\n",
        "SpaceNet8_val_img_dir = '/drive/MyDrive/G.E.M.S./sn8/images/val'\n",
        "SpaceNet8_val_mask_dir = '/drive/MyDrive/G.E.M.S./sn8/masks/val'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLW2rNaPIpWN"
      },
      "outputs": [],
      "source": [
        "#!pip install gdown\n",
        "# import gdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UX04bC9zIszd"
      },
      "outputs": [],
      "source": [
        "#!gdown --fuzzy \"https://drive.google.com/file/d/1iRkEX9LQ8Hi-38QMyaReFJ8wXDDyYJAg/view?usp=sharing\" -O RescueNet.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "atY77jh0DhDA"
      },
      "outputs": [],
      "source": [
        "# import zipfile\n",
        "\n",
        "# with zipfile.ZipFile(\"/content/drive/MyDrive/G.E.M.S./RescueNet.zip\", \"r\") as zip_ref:\n",
        "#      zip_ref.extractall(\"/content/RescueNet\")\n",
        "# \\\n",
        "# print(\"Extraction complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tMT0inR5dj39"
      },
      "outputs": [],
      "source": [
        "print(os.listdir('/content/drive/MyDrive/G.E.M.S.'))\n",
        "#print(os.listdir('/content/RescueNet'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yTDX_9BiD8hw"
      },
      "outputs": [],
      "source": [
        "RescueNet_train_img_dir = '/content/drive/MyDrive/G.E.M.S./RescueNet/train/train-org-img'\n",
        "RescueNet_train_mask_dir = '/content/drive/MyDrive/G.E.M.S./RescueNet/train/train-label-img'\n",
        "\n",
        "RescueNet_val_img_dir = '/content/drive/MyDrive/G.E.M.S./RescueNet/val/val-org-img'\n",
        "RescueNet_val_mask_dir = '/content/drive/MyDrive/G.E.M.S./RescueNet/val/val-label-img'\n",
        "\n",
        "RescueNet_test_img_dir = '/content/drive/MyDrive/G.E.M.S./RescueNet/test/test-org-img'\n",
        "RescueNet_test_mask_dir = '/content/drive/MyDrive/G.E.M.S./RescueNet/test/test-label-img'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MXb9EZZCEbE7"
      },
      "outputs": [],
      "source": [
        "#os.remove(\"/content/RescueNet.zip\")\n",
        "#print(\"Files Deleted\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNOwuNO5YDt3"
      },
      "outputs": [],
      "source": [
        "sn8_path = \"/content/drive/MyDrive/G.E.M.S./sn8\"\n",
        "sn8_train_manifest = \"/content/drive/MyDrive/G.E.M.S./sn8/manifests/train_manifest.csv\"\n",
        "sn8_val_manifest = \"/content/drive/MyDrive/G.E.M.S./sn8/manifests/val_manifest.csv\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmUApdU4TuzB"
      },
      "source": [
        "## Loading Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LvMldvFTE2SU"
      },
      "outputs": [],
      "source": [
        "class FloodNetDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset class for FloodNet flood detection data\n",
        "\n",
        "    Expected file structure:\n",
        "    - Images: original disaster images\n",
        "    - Masks: binary masks (0: non-flooded, 1: flooded)\n",
        "    \"\"\"\n",
        "    FLOODNET_COLORS = {\n",
        "        (0,0,0):0, #Unlabeled\n",
        "        (128,0,0):1, #Building-flooded\n",
        "        (0,128,0):2, # Buildings-non-flooded\n",
        "        (128,128,0):3, #Road-flooded\n",
        "        (0,0,128):4, #Road-non-flooded\n",
        "        (128,0,128):5, #Water\n",
        "        (0,128,128):6, #Tree\n",
        "        (128,128,128):7,#Vehicle\n",
        "        (64,0,0):8, #Pool\n",
        "        (192,0,0):9, #Grass\n",
        "\n",
        "    }\n",
        "\n",
        "\n",
        "    def __init__(self, img_dir, mask_dir, transform=None, binary_flood=True):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img_dir: Directory containing input images\n",
        "            mask_dir: Directory containing segmentation masks\n",
        "            transform: Albumentations transform pipeline\n",
        "        \"\"\"\n",
        "        self.img_dir = img_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.img_names = sorted(os.listdir(img_dir))\n",
        "        self.transform = transform\n",
        "        self.binary_flood= binary_flood\n",
        "\n",
        "        # Filter images that have corresponding masks\n",
        "        all_imgs = sorted(os.listdir(img_dir))\n",
        "        self.img_names = []\n",
        "\n",
        "        for img_name in all_imgs:\n",
        "          base_name = img_name.rsplit(\".\", 1)[0]\n",
        "          pattern = os.path.join(mask_dir, base_name + \"_*.png\")\n",
        "          if glob.glob(pattern):\n",
        "            self.img_names.append(img_name)\n",
        "          else:\n",
        "            print(f\"Warning: No mask found for {img_name}, excluding from dataset\")\n",
        "\n",
        "        print(f\"FloodNet dataset: {len(all_imgs)} images, {len(self.img_names)} with masks\")\n",
        "\n",
        "\n",
        "    def rgb_to_class(self, mask_rgb):\n",
        "        \"\"\"Convert RGB mask to class indices\"\"\"\n",
        "        h, w = mask_rgb.shape[:2]\n",
        "        mask_class = np.zeros((h,w), dtype=np.uint8)\n",
        "\n",
        "        for rgb, class_id in self.FLOODNET_COLORS.items():\n",
        "            #Find pixels matching this RGB value\n",
        "            matches = np.all(mask_rgb == rgb, axis=-1)\n",
        "            mask_class[matches] = class_id\n",
        "\n",
        "        return mask_class\n",
        "\n",
        "    def create_flood_mask(self, class_mask):\n",
        "      \"\"\"Create binary flood/non-_flood mask from classes\"\"\"\n",
        "      flood_mask = np.zeros_like(class_mask, dtype=np.uint8)\n",
        "      flooded_classes = [1,3,5] # Building-flooded\n",
        "\n",
        "\n",
        "      for class_id in flooded_classes:\n",
        "        flood_mask[class_mask == class_id] = 1\n",
        "\n",
        "      return flood_mask\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_names)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Load image and mask pair with error handling\"\"\"\n",
        "        img_name = self.img_names[idx]\n",
        "\n",
        "        # Extract base filename without extension\n",
        "        base_name = img_name.rsplit(\".\", 1)[0]\n",
        "\n",
        "        # Find matching mask (FloodNet naming pattern)\n",
        "        pattern = os.path.join(self.mask_dir, base_name + \"_*.png\")\n",
        "        matching_masks = glob.glob(pattern)\n",
        "\n",
        "        if len(matching_masks) == 0:\n",
        "            raise FileNotFoundError(f\"No matching mask for: {img_name}\")\n",
        "\n",
        "        mask_path = matching_masks[0]\n",
        "        img_path = os.path.join(self.img_dir, img_name)\n",
        "\n",
        "        # Load image (OpenCV loads as BGR, convert to RGB)\n",
        "        image = cv2.imread(img_path)\n",
        "        if image is None:\n",
        "            raise FileNotFoundError(f\"Failed to load image: {img_path}\")\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Load mask as grayscale\n",
        "        mask_rgb = cv2.imread(mask_path)\n",
        "        if mask_rgb is None:\n",
        "            raise FileNotFoundError(f\"Failed to load mask: {mask_path}\")\n",
        "        mask_rgb = cv2.cvtColor(mask_rgb, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Convert RGB mask to class indices\n",
        "        class_mask = self.rgb_to_class(mask_rgb)\n",
        "\n",
        "        # Create appropriate output mask\n",
        "        if self.binary_flood:\n",
        "          mask = self.create_flood_mask(class_mask)\n",
        "        else:\n",
        "          mask = class_mask\n",
        "\n",
        "\n",
        "        # Apply augmentations\n",
        "        if self.transform:\n",
        "            augmented = self.transform(image=image, mask=mask)\n",
        "            image = augmented[\"image\"]\n",
        "            mask = augmented[\"mask\"]\n",
        "\n",
        "        return image, mask.long()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwZHhRF4PKVk"
      },
      "outputs": [],
      "source": [
        "class RescueNetDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset class for RescueNet damage assessment data\n",
        "\n",
        "    Expected file structure:\n",
        "    - Images: original disaster images\n",
        "    - Masks: multi-class masks (0-3 for damage levels)\n",
        "    \"\"\"\n",
        "    def __init__(self, img_dir, mask_dir, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img_dir: Directory containing input images\n",
        "            mask_dir: Directory containing segmentation masks\n",
        "            transform: Albumentations transform pipeline\n",
        "        \"\"\"\n",
        "        self.img_dir = img_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.img_names = sorted(os.listdir(img_dir))\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_names)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Load image and mask pair with error handling\"\"\"\n",
        "        img_name = self.img_names[idx]\n",
        "\n",
        "        # Extract base filename without extension\n",
        "        base_name = img_name.rsplit(\".\", 1)[0]\n",
        "\n",
        "        # Find matching mask (RescueNet naming pattern)\n",
        "        pattern = os.path.join(self.mask_dir, base_name + \"_lab*.png\")\n",
        "        matching_masks = glob.glob(pattern)\n",
        "\n",
        "        if len(matching_masks) == 0:\n",
        "            raise FileNotFoundError(f\"No matching mask for: {img_name}\")\n",
        "\n",
        "        mask_path = matching_masks[0]\n",
        "        img_path = os.path.join(self.img_dir, img_name)\n",
        "\n",
        "        # Load image\n",
        "        image = cv2.imread(img_path)\n",
        "        if image is None:\n",
        "            raise FileNotFoundError(f\"Failed to load image: {img_path}\")\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Load mask\n",
        "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "        if mask is None:\n",
        "            raise FileNotFoundError(f\"Failed to load mask: {mask_path}\")\n",
        "\n",
        "        # Ensure mask values are in valid range (0-3)\n",
        "        mask = np.clip(mask, 0, 3).astype(np.uint8)\n",
        "\n",
        "        # Apply augmentations\n",
        "        if self.transform:\n",
        "            augmented = self.transform(image=image, mask=mask)\n",
        "            image = augmented[\"image\"]\n",
        "            mask = augmented[\"mask\"]\n",
        "\n",
        "        return image, mask.long()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tBV4kCq4lJtL"
      },
      "outputs": [],
      "source": [
        "class SpaceNet8Dataset(Dataset):\n",
        "    def __init__(self, manifest_csv, augment=None,\n",
        "                 cache_size=500, disaster_focus=True, preload_critical_samples=True):\n",
        "        self.df = pd.read_csv(manifest_csv)\n",
        "        self.augment = augment\n",
        "        self.disaster_focus = disaster_focus\n",
        "        self.preload_critical_samples = preload_critical_samples\n",
        "\n",
        "        # Quick LRU cache for frequently accessed files\n",
        "        from functools import lru_cache\n",
        "        self._load_file = lru_cache(maxsize=cache_size)(self._load_file_uncached)\n",
        "\n",
        "        if disaster_focus:\n",
        "            self._prioritize_disaster_samples()  # Added underscore\n",
        "\n",
        "        self.gpu_preloaded = {}\n",
        "        if preload_critical_samples and torch.cuda.is_available():\n",
        "            self._preload_disaster_samples()  # Added underscore\n",
        "\n",
        "        print(f\"Disaster-focused SpaceNet8: {len(self.df)} samples\")\n",
        "        print(f\"Cache size: {cache_size}, GPU preloaded: {len(self.gpu_preloaded)}\")\n",
        "\n",
        "    def _prioritize_disaster_samples(self):\n",
        "        \"\"\"Prioritize samples likely to contain flood/damage for training efficiency\"\"\"\n",
        "        file_sizes = []\n",
        "        for _, row in self.df.iterrows():\n",
        "            try:\n",
        "                mask_size = os.path.getsize(row[\"mask_path\"]) if os.path.exists(row[\"mask_path\"]) else 0\n",
        "                file_sizes.append(mask_size)\n",
        "            except:\n",
        "                file_sizes.append(0)\n",
        "\n",
        "        self.df[\"mask_size\"] = file_sizes\n",
        "        self.df = self.df.sort_values(by='mask_size', ascending=False).reset_index(drop=True)\n",
        "        print(f\"Prioritized {len(self.df)} samples by disaster content likelihood\")\n",
        "\n",
        "    def _preload_disaster_samples(self, preload_count=100):\n",
        "        \"\"\"Preload disaster-rich samples to GPU for faster training\"\"\"\n",
        "        print(f\"Preloading top {preload_count} disaster samples to A100 GPU memory...\")\n",
        "\n",
        "        for idx in range(min(preload_count, len(self.df))):\n",
        "            row = self.df.iloc[idx]\n",
        "            try:\n",
        "                img, mask = self._load_file_uncached(row[\"post_path\"], row[\"mask_path\"])\n",
        "\n",
        "                img_tensor = torch.from_numpy(np.moveaxis(img, 0, -1)).float() / 255.0\n",
        "                mask_tensor = torch.from_numpy(mask).long()\n",
        "\n",
        "                self.gpu_preloaded[idx] = (\n",
        "                    img_tensor.cuda(non_blocking=True),\n",
        "                    mask_tensor.cuda(non_blocking=True)\n",
        "                )\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to preload sample {idx}: {e}\")\n",
        "\n",
        "        print(f\"Preloaded {len(self.gpu_preloaded)} disaster-rich samples to GPU\")\n",
        "\n",
        "    def _load_file_uncached(self, post_path, mask_path):\n",
        "        \"\"\"Load and cache raw file for optimization\"\"\"\n",
        "        try:\n",
        "            with rio.Env(\n",
        "                GDAL_DISABLE_READDIR_ON_OPEN=\"EMPTY_DIR\",\n",
        "                GDAL_CACHEMAX=2048,\n",
        "                CPL_VSIL_CURL_ALLOWED_EXTENSIONS='.tif,.tiff',  # Fixed comma\n",
        "                GDAL_NUM_THREADS='ALL_CPUS'\n",
        "            ):\n",
        "                with rio.open(post_path) as src:\n",
        "                    # (C,H,W) order - FIXED INDENTATION\n",
        "                    img = src.read([1, 2, 3])\n",
        "                with rio.open(mask_path) as src:\n",
        "                    mask = src.read(1)\n",
        "            return img.astype(np.uint8), mask.astype(np.int64)  # Fixed dtype\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to load file {os.path.basename(post_path)}: {e}\")\n",
        "            return (np.zeros((3, 512, 512), dtype=np.uint8),\n",
        "                    np.zeros((512, 512), dtype=np.int64))  # Fixed dtype\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Check GPU preloaded samples first (fastest)\n",
        "        if idx in self.gpu_preloaded:\n",
        "            img_tensor, mask_tensor = self.gpu_preloaded[idx]\n",
        "\n",
        "            if self.augment:\n",
        "                img_np = img_tensor.cpu().numpy().transpose(1, 2, 0)  # Fixed comma\n",
        "                mask_np = mask_tensor.cpu().numpy()\n",
        "                augmented = self.augment(image=img_np, mask=mask_np)\n",
        "                return augmented[\"image\"], augmented[\"mask\"]\n",
        "\n",
        "            return img_tensor, mask_tensor\n",
        "\n",
        "        # If not in cache, load from disk\n",
        "        row = self.df.iloc[idx]\n",
        "        img, mask = self._load_file(row[\"post_path\"], row[\"mask_path\"])\n",
        "\n",
        "        # Convert to HWC for Albumentations\n",
        "        img = np.moveaxis(img, 0, -1)\n",
        "        binary_flood_mask = (mask > 0).astype(np.int64)\n",
        "\n",
        "        # Apply augmentation if given\n",
        "        if self.augment:\n",
        "            augmented = self.augment(image=img, mask=binary_flood_mask)  # Fixed mask variable\n",
        "            return augmented[\"image\"], augmented[\"mask\"]\n",
        "\n",
        "        # Back to CHW + normalize\n",
        "        else:\n",
        "            img_tensor = torch.from_numpy(np.moveaxis(img, -1, 0)).float() / 255.0\n",
        "            mask_tensor = torch.from_numpy(binary_flood_mask).long()\n",
        "            return img_tensor, mask_tensor\n",
        "\n",
        "\n",
        "class SpaceNet8CompatWrapper(Dataset):\n",
        "    \"\"\"Wrapper to make SpaceNet8 compatible with FloodNet\"\"\"\n",
        "    def __init__(self, spacenet_dataset):\n",
        "        self.spacenet_dataset = spacenet_dataset\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.spacenet_dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_tensor, mask_tensor = self.spacenet_dataset[idx]\n",
        "        # Ensure mask is binary for flood detection (0=no flood, 1=flood)\n",
        "        # SpaceNet8 uses: 0=no building, 1=no damage, 2=minor, 3=major, 4=destroyed\n",
        "        # Convert to binary: 0=no flood, 1=flood (any building damage indicates potential flooding)\n",
        "        binary_mask = (mask_tensor > 0).long()\n",
        "\n",
        "        return img_tensor, binary_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKWSLSXXSpYf"
      },
      "source": [
        "## Data Augmentation Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Us6gA-NJSt-x"
      },
      "outputs": [],
      "source": [
        "def get_training_augmentation():\n",
        "    \"\"\"\n",
        "    Training augmentation pipeline with various geometric and color transforms\n",
        "    \"\"\"\n",
        "    return A.Compose([\n",
        "        # Resize to target dimensions\n",
        "        A.Resize(Config.img_height, Config.img_width),\n",
        "\n",
        "        # Geometric transforms\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.VerticalFlip(p=0.1),  # Less common but useful for aerial images\n",
        "        A.RandomRotate90(p=0.5),\n",
        "\n",
        "        # Slight rotations and shifts\n",
        "        A.ShiftScaleRotate(\n",
        "            shift_limit=0.1,\n",
        "            scale_limit=0.1,\n",
        "            rotate_limit=15,\n",
        "            border_mode=cv2.BORDER_REFLECT,\n",
        "            p=0.5\n",
        "        ),\n",
        "\n",
        "        # Color augmentations\n",
        "        A.RandomBrightnessContrast(\n",
        "            brightness_limit=0.2,\n",
        "            contrast_limit=0.2,\n",
        "            p=0.5\n",
        "        ),\n",
        "        A.ColorJitter(\n",
        "            brightness=0.1,\n",
        "            contrast=0.1,\n",
        "            saturation=0.1,\n",
        "            hue=0.05,\n",
        "            p=0.3\n",
        "        ),\n",
        "\n",
        "        # Weather effects (useful for disaster scenarios)\n",
        "        A.RandomRain(p=0.1),\n",
        "        A.RandomFog(p=0.1),\n",
        "\n",
        "        # Noise and blur\n",
        "        A.GaussNoise(noise_scale_factor=0.1, p=0.2),\n",
        "        A.GaussianBlur(blur_limit=(3, 7), p=0.2),\n",
        "\n",
        "        # Normalize with ImageNet statistics\n",
        "        A.Normalize(\n",
        "            mean=(0.485, 0.456, 0.406),\n",
        "            std=(0.229, 0.224, 0.225)\n",
        "        ),\n",
        "\n",
        "        # Convert to PyTorch tensor\n",
        "        ToTensorV2()\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AetmH_CoTA10"
      },
      "outputs": [],
      "source": [
        "def get_validation_augmentation():\n",
        "    \"\"\"\n",
        "    Validation augmentation pipeline (only essential transforms)\n",
        "    \"\"\"\n",
        "    return A.Compose([\n",
        "        A.Resize(Config.img_height, Config.img_width),\n",
        "        A.Normalize(\n",
        "            mean=(0.485, 0.456, 0.406),\n",
        "            std=(0.229, 0.224, 0.225)\n",
        "        ),\n",
        "        ToTensorV2()\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wTOtyNZPTC1w"
      },
      "outputs": [],
      "source": [
        "def get_test_augmentation():\n",
        "    \"\"\"\n",
        "    Test augmentation pipeline (same as validation)\n",
        "    \"\"\"\n",
        "    return get_validation_augmentation()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Me6035kVTH9I"
      },
      "source": [
        "## Attention models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LS4o6fvJTHjX"
      },
      "outputs": [],
      "source": [
        "class ChannelAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Channel attention module to focus on important feature channels\n",
        "    Squeeze-and-Excitation style attention mechanism\n",
        "    \"\"\"\n",
        "    def __init__(self, channels, reduction=16):\n",
        "        super().__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
        "\n",
        "        # Shared MLP\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(channels, channels // reduction, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(channels // reduction, channels, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _, _ = x.size()\n",
        "\n",
        "        # Average pooling path\n",
        "        avg_out = self.fc(self.avg_pool(x).view(b, c))\n",
        "\n",
        "        # Max pooling path\n",
        "        max_out = self.fc(self.max_pool(x).view(b, c))\n",
        "\n",
        "        # Combine and apply attention\n",
        "        attention = avg_out + max_out\n",
        "        return x * attention.view(b, c, 1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "83_aRdS6TRdC"
      },
      "outputs": [],
      "source": [
        "class SpatialAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Spatial attention module to focus on important spatial regions\n",
        "    \"\"\"\n",
        "    def __init__(self, kernel_size=7):\n",
        "        super().__init__()\n",
        "        padding = kernel_size // 2\n",
        "        self.conv = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Channel-wise average and max\n",
        "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
        "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
        "\n",
        "        # Concatenate and convolve\n",
        "        x_cat = torch.cat([avg_out, max_out], dim=1)\n",
        "        attention = self.sigmoid(self.conv(x_cat))\n",
        "\n",
        "        return x * attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDnVRkb0TYu4"
      },
      "source": [
        "# Multi-Task Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "K-JJXAoyTYQC"
      },
      "outputs": [],
      "source": [
        "class EnhancedDisasterModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Enhanced multi-task segmentation model for disaster assessment\n",
        "\n",
        "    Features:\n",
        "    - Shared backbone for feature extraction\n",
        "    - Task-specific heads with attention mechanisms\n",
        "    - Feature fusion for cross-task learning\n",
        "    - Deep supervision options\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes_flood=2, num_classes_damage=4, backbone='resnet101'):\n",
        "        super().__init__()\n",
        "\n",
        "        # Initialize backbone based on configuration\n",
        "        if backbone == 'resnet101':\n",
        "            base_model = deeplabv3_resnet101(pretrained=True)\n",
        "            print(\"Using ResNet101 backbone\")\n",
        "        else:\n",
        "            base_model = deeplabv3_resnet50(pretrained=True)\n",
        "            print(\"Using ResNet50 backbone\")\n",
        "\n",
        "        # Extract backbone and ASPP module\n",
        "        self.backbone = base_model.backbone\n",
        "        self.aspp = base_model.classifier[0]\n",
        "\n",
        "        # Attention mechanisms for feature refinement\n",
        "        self.channel_attention = ChannelAttention(256)\n",
        "        self.spatial_attention = SpatialAttention()\n",
        "\n",
        "        # Task-specific feature extraction branches\n",
        "        # Flood detection branch\n",
        "        self.flood_branch = nn.Sequential(\n",
        "            nn.Conv2d(256, 512, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout2d(0.3),\n",
        "\n",
        "            nn.Conv2d(512, 256, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout2d(0.2),\n",
        "\n",
        "            nn.Conv2d(256, 128, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        # Damage assessment branch\n",
        "        self.damage_branch = nn.Sequential(\n",
        "            nn.Conv2d(256, 512, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout2d(0.3),\n",
        "\n",
        "            nn.Conv2d(512, 256, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout2d(0.2),\n",
        "\n",
        "            nn.Conv2d(256, 128, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        # Classification heads\n",
        "        self.flood_classifier = nn.Conv2d(128, num_classes_flood, 1)\n",
        "        self.damage_classifier = nn.Conv2d(128, num_classes_damage, 1)\n",
        "\n",
        "        # Optional: Cross-task feature fusion\n",
        "        self.enable_fusion = True\n",
        "        if self.enable_fusion:\n",
        "            self.fusion_conv = nn.Sequential(\n",
        "                nn.Conv2d(256, 128, 1, bias=False),\n",
        "                nn.BatchNorm2d(128),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Conv2d(128, 256, 1, bias=False),\n",
        "                nn.BatchNorm2d(256),\n",
        "                nn.Sigmoid()\n",
        "            )\n",
        "\n",
        "    def forward(self, x, return_features=False):\n",
        "        \"\"\"\n",
        "        Forward pass through the network\n",
        "\n",
        "        Args:\n",
        "            x: Input tensor [B, 3, H, W]\n",
        "            return_features: Whether to return intermediate features\n",
        "\n",
        "        Returns:\n",
        "            flood_out: Flood segmentation output [B, 2, H, W]\n",
        "            damage_out: Damage segmentation output [B, 4, H, W]\n",
        "            features (optional): Intermediate features for visualization\n",
        "        \"\"\"\n",
        "        # Store input shape for upsampling\n",
        "        input_shape = x.shape[-2:]\n",
        "\n",
        "        # Extract multi-level features from backbone\n",
        "        features = self.backbone(x)\n",
        "\n",
        "        # Get high-level features\n",
        "        x = features['out']\n",
        "\n",
        "        # Apply ASPP for multi-scale context\n",
        "        x = self.aspp(x)\n",
        "\n",
        "        # Apply attention mechanisms\n",
        "        x = self.channel_attention(x)\n",
        "        x = self.spatial_attention(x)\n",
        "\n",
        "        # Task-specific processing\n",
        "        flood_features = self.flood_branch(x)\n",
        "        damage_features = self.damage_branch(x)\n",
        "\n",
        "        # Optional cross-task feature fusion\n",
        "        if self.enable_fusion:\n",
        "            # Concatenate task features\n",
        "            combined = torch.cat([flood_features, damage_features], dim=1)\n",
        "\n",
        "            # Generate fusion weights\n",
        "            fusion_weights = self.fusion_conv(combined)\n",
        "\n",
        "            # Apply fusion\n",
        "            flood_features = flood_features + fusion_weights[:, :128] * damage_features\n",
        "            damage_features = damage_features + fusion_weights[:, 128:] * flood_features\n",
        "\n",
        "        # Generate predictions\n",
        "        flood_out = self.flood_classifier(flood_features)\n",
        "        damage_out = self.damage_classifier(damage_features)\n",
        "\n",
        "        # Upsample to original resolution\n",
        "        flood_out = F.interpolate(\n",
        "            flood_out, size=input_shape,\n",
        "            mode='bilinear', align_corners=False\n",
        "        )\n",
        "        damage_out = F.interpolate(\n",
        "            damage_out, size=input_shape,\n",
        "            mode='bilinear', align_corners=False\n",
        "        )\n",
        "\n",
        "        if return_features:\n",
        "            return flood_out, damage_out, flood_features, damage_features\n",
        "\n",
        "        return flood_out, damage_out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x22JpKxYVbSh"
      },
      "source": [
        "## Custom Loss Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MeVFNwxBVfM0"
      },
      "outputs": [],
      "source": [
        "class DisasterFocusedLoss(nn.Module):\n",
        "    \"\"\"Loss function optimized for disaster mapping class imbalances\"\"\"\n",
        "\n",
        "    def __init__(self, ce_weight=0.3, dice_weight=0.4, focal_weight=0.3,\n",
        "                 class_weights=None, focal_alpha=0.25, focal_gamma=2.0):\n",
        "        super().__init__()\n",
        "        self.ce_weight = ce_weight\n",
        "        self.dice_weight = dice_weight\n",
        "        self.focal_weight = focal_weight\n",
        "        self.focal_alpha = focal_alpha\n",
        "        self.focal_gamma = focal_gamma\n",
        "\n",
        "        self.ce_loss = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        # Cross-entropy with disaster class weighting\n",
        "        ce = self.ce_loss(pred, target)\n",
        "\n",
        "        # Dice loss - critical for disaster boundary accuracy\n",
        "        dice = self._dice_loss_disaster(pred, target)\n",
        "\n",
        "        # Focal loss - handle severe class imbalance in disaster data\n",
        "        focal = self._focal_loss_disaster(pred, target)\n",
        "\n",
        "        total_loss = (\n",
        "            self.ce_weight * ce +\n",
        "            self.dice_weight * dice +\n",
        "            self.focal_weight * focal\n",
        "        )\n",
        "\n",
        "        return total_loss, {\n",
        "            'ce': ce.item(),\n",
        "            'dice': dice.item(),\n",
        "            'focal': focal.item()\n",
        "        }\n",
        "\n",
        "    def _dice_loss_disaster(self, pred, target):\n",
        "        \"\"\"Dice loss optimized for disaster mapping accuracy\"\"\"\n",
        "        pred_soft = F.softmax(pred, dim=1)\n",
        "        target_one_hot = F.one_hot(target, pred.shape[1]).permute(0, 3, 1, 2).float()\n",
        "\n",
        "        # Flatten for efficient computation\n",
        "        pred_flat = pred_soft.reshape(pred_soft.shape[0], pred_soft.shape[1], -1)\n",
        "        target_flat = target_one_hot.reshape(target_one_hot.shape[0], target_one_hot.shape[1], -1)\n",
        "\n",
        "        intersection = (pred_flat * target_flat).sum(dim=2)\n",
        "        union = pred_flat.sum(dim=2) + target_flat.sum(dim=2)\n",
        "\n",
        "        # Smooth dice with small epsilon for disaster data stability\n",
        "        dice = (2 * intersection + 1e-6) / (union + 1e-6)\n",
        "\n",
        "        return 1 - dice.mean()\n",
        "\n",
        "    def _focal_loss_disaster(self, pred, target):\n",
        "        \"\"\"Focal loss with disaster-specific parameters\"\"\"\n",
        "        ce_loss = F.cross_entropy(pred, target, reduction='none')\n",
        "        pt = torch.exp(-ce_loss)\n",
        "\n",
        "        # Disaster-tuned focal loss\n",
        "        focal_loss = self.focal_alpha * (1 - pt) ** self.focal_gamma * ce_loss\n",
        "\n",
        "        return focal_loss.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5TL0-McWGOp"
      },
      "source": [
        "## Evaluation Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5hgto8UdWLlE"
      },
      "outputs": [],
      "source": [
        "class MetricCalculator:\n",
        "    \"\"\"\n",
        "    Calculate various segmentation metrics\n",
        "    \"\"\"\n",
        "    @staticmethod\n",
        "    def calculate_iou(pred, target, num_classes):\n",
        "        \"\"\"\n",
        "        Calculate Intersection over Union per class\n",
        "        \"\"\"\n",
        "        ious = []\n",
        "        pred = pred.view(-1)\n",
        "        target = target.view(-1)\n",
        "\n",
        "        for cls in range(num_classes):\n",
        "            pred_inds = pred == cls\n",
        "            target_inds = target == cls\n",
        "\n",
        "            intersection = (pred_inds & target_inds).sum().item()\n",
        "            union = (pred_inds | target_inds).sum().item()\n",
        "\n",
        "            if union == 0:\n",
        "                ious.append(float('nan'))\n",
        "            else:\n",
        "                ious.append(intersection / union)\n",
        "\n",
        "        return ious\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_dice(pred, target, num_classes):\n",
        "        \"\"\"\n",
        "        Calculate Dice coefficient per class\n",
        "        \"\"\"\n",
        "        dices = []\n",
        "        pred = pred.view(-1)\n",
        "        target = target.view(-1)\n",
        "\n",
        "        for cls in range(num_classes):\n",
        "            pred_inds = pred == cls\n",
        "            target_inds = target == cls\n",
        "\n",
        "            intersection = (pred_inds & target_inds).sum().item()\n",
        "            pred_sum = pred_inds.sum().item()\n",
        "            target_sum = target_inds.sum().item()\n",
        "\n",
        "            if pred_sum + target_sum == 0:\n",
        "                dices.append(float('nan'))\n",
        "            else:\n",
        "                dices.append(2 * intersection / (pred_sum + target_sum))\n",
        "\n",
        "        return dices\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_pixel_accuracy(pred, target):\n",
        "        \"\"\"\n",
        "        Calculate overall pixel accuracy\n",
        "        \"\"\"\n",
        "        correct = (pred == target).sum().item()\n",
        "        total = target.numel()\n",
        "        return correct / total\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7m1MqUlg9nf"
      },
      "source": [
        "## Pretraining Flood model with SpaceNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "YrFLRTslg45h"
      },
      "outputs": [],
      "source": [
        "# Create a separate pretraining function\n",
        "def pretrain_flood_on_spacenet(model, spacenet_loader, num_epochs=10):\n",
        "    print(\"Pretraining flood detection on SpaceNet8...\")\n",
        "\n",
        "    # Only optimize flood-related parameters\n",
        "    pretrain_optimizer = torch.optim.Adam([\n",
        "        {'params': model.backbone.parameters(), 'lr': 1e-5},\n",
        "        {'params': model.flood_branch.parameters(), 'lr': 5e-5},\n",
        "        {'params': model.flood_classifier.parameters(), 'lr': 5e-5}\n",
        "    ])\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "      for images, masks in tqdm(spacenet_loader):\n",
        "        flood_out, _ = model(images)\n",
        "        loss = F.cross_entropy(flood_out, masks)\n",
        "\n",
        "        pretrain_optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        pretrain_optimizer.step()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNFpPUyRWSbn"
      },
      "source": [
        "## Optimized Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "i98iJ4O1WRe0"
      },
      "outputs": [],
      "source": [
        "class OptimizedTrainer:\n",
        "    def __init__(self, model, config, device='cuda'):\n",
        "        self.model = model\n",
        "        self.config = config\n",
        "        self.device = device\n",
        "\n",
        "        # A100 optimizations\n",
        "        self._enable_optimizations()\n",
        "\n",
        "        # Mixed precision training\n",
        "        self.scaler = GradScaler(init_scale=2**16, growth_interval=100)\n",
        "\n",
        "        # Optimizer with task-specific learning rates\n",
        "        self.optimizer = self._create_disaster_optimizer()\n",
        "\n",
        "        # Loss functions\n",
        "        self.flood_loss_fn = self._create_disaster_loss('flood')\n",
        "        self.damage_loss_fn = self._create_disaster_loss('damage')\n",
        "\n",
        "        # Metrics tracking\n",
        "        self.metric_calculator = MetricCalculator()\n",
        "        self.disaster_metrics = defaultdict(list)\n",
        "        self.best_disaster_iou = {'flood': 0.0, 'damage': 0.0}\n",
        "\n",
        "        # Training history tracking\n",
        "        self.train_losses = {'flood': [], 'damage': [], 'total': []}\n",
        "        self.val_metrics = {'flood_iou': [], 'damage_iou': []}\n",
        "        self.best_flood_iou = 0.0\n",
        "        self.best_damage_iou = 0.0\n",
        "        self.best_combined_score = 0.0\n",
        "\n",
        "        print(\"Disaster mapping trainer initialized for A100\")\n",
        "\n",
        "    def _enable_optimizations(self):\n",
        "        \"\"\"Enable A100-specific optimizations\"\"\"\n",
        "        torch.backends.cudnn.allow_tf32 = True\n",
        "        torch.backends.cuda.matmul.allow_tf32 = True\n",
        "        torch.backends.cudnn.benchmark = True\n",
        "        torch.cuda.set_per_process_memory_fraction(0.9)\n",
        "        print(\"A100 optimizations enabled\")\n",
        "\n",
        "    def _create_disaster_optimizer(self):\n",
        "        \"\"\"Create optimizer with disaster-focused learning rates\"\"\"\n",
        "        param_groups = [\n",
        "            {\n",
        "                'params': [p for n, p in self.model.backbone.named_parameters()],\n",
        "                'lr': self.config.learning_rate * 0.1,\n",
        "                'weight_decay': self.config.weight_decay\n",
        "            },\n",
        "            {\n",
        "                'params': [p for n, p in self.model.aspp.named_parameters()],\n",
        "                'lr': self.config.learning_rate * 0.3,\n",
        "                'weight_decay': self.config.weight_decay * 0.5\n",
        "            },\n",
        "            {\n",
        "                'params': list(self.model.flood_branch.parameters()) +\n",
        "                          list(self.model.flood_classifier.parameters()),\n",
        "                'lr': self.config.learning_rate * 1.2,\n",
        "                'weight_decay': self.config.weight_decay * 0.8\n",
        "            },\n",
        "            {\n",
        "                'params': list(self.model.damage_branch.parameters()) +\n",
        "                          list(self.model.damage_classifier.parameters()),\n",
        "                'lr': self.config.learning_rate * 1.5,\n",
        "                'weight_decay': self.config.weight_decay * 0.6\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        return torch.optim.AdamW(\n",
        "            param_groups,\n",
        "            eps=1e-8,\n",
        "            betas=(0.9, 0.999),\n",
        "            amsgrad=True\n",
        "        )\n",
        "\n",
        "    def _create_disaster_loss(self, task):\n",
        "        \"\"\"Create disaster-focused loss function\"\"\"\n",
        "        if task == 'flood':\n",
        "            class_weights = torch.tensor([0.2, 3.5]).cuda()\n",
        "            alpha = 0.8\n",
        "        else:\n",
        "            class_weights = torch.tensor([0.3, 1.5, 2.2, 2.8]).cuda()\n",
        "            alpha = 0.6\n",
        "\n",
        "        return DisasterFocusedLoss(\n",
        "            ce_weight=0.3,\n",
        "            dice_weight=0.4,\n",
        "            focal_weight=0.3,\n",
        "            class_weights=class_weights,\n",
        "            focal_alpha=alpha\n",
        "        )\n",
        "\n",
        "    def train_epoch(self, flood_loader, damage_loader, epoch):\n",
        "        \"\"\"A100-optimized training epoch for disaster mapping\"\"\"\n",
        "        self.model.train()\n",
        "\n",
        "        # Use cycle iterators for continuous iteration\n",
        "        flood_cycle = cycle(flood_loader)\n",
        "        damage_cycle = cycle(damage_loader)\n",
        "\n",
        "        # Determine number of iterations\n",
        "        num_iterations = max(len(flood_loader), len(damage_loader))\n",
        "        epoch_metrics = defaultdict(float)\n",
        "\n",
        "        print(f\"Disaster mapping training epoch {epoch}: {num_iterations} iterations\")\n",
        "\n",
        "        # Initialize gradient accumulation\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        for i in range(num_iterations):\n",
        "            # Process both tasks simultaneously\n",
        "            metrics = self._simultaneous_disaster_step(\n",
        "                next(flood_cycle), next(damage_cycle), i\n",
        "            )\n",
        "\n",
        "            # Accumulate metrics\n",
        "            for key, value in metrics.items():\n",
        "                epoch_metrics[key] += value\n",
        "\n",
        "            # Gradient update\n",
        "            if (i + 1) % self.config.accumulation_steps == 0:\n",
        "                self._disaster_gradient_update()\n",
        "\n",
        "            # Progress reporting\n",
        "            if i % 100 == 0 and i > 0:\n",
        "                flood_loss = epoch_metrics['flood_loss'] / (i + 1)\n",
        "                damage_loss = epoch_metrics['damage_loss'] / (i + 1)\n",
        "                memory_gb = torch.cuda.max_memory_allocated() / 1e9\n",
        "\n",
        "                print(f\"Step {i}: Flood Loss: {flood_loss:.4f}, \"\n",
        "                      f\"Damage Loss: {damage_loss:.4f}, GPU: {memory_gb:.1f}GB\")\n",
        "\n",
        "        # Final gradient update if needed\n",
        "        if num_iterations % self.config.accumulation_steps != 0:\n",
        "            self._disaster_gradient_update()\n",
        "\n",
        "        # Calculate epoch averages\n",
        "        epoch_flood_loss = epoch_metrics['flood_loss'] / num_iterations\n",
        "        epoch_damage_loss = epoch_metrics['damage_loss'] / num_iterations\n",
        "        epoch_total_loss = epoch_flood_loss + epoch_damage_loss\n",
        "\n",
        "        # Store epoch statistics\n",
        "        self.train_losses['flood'].append(epoch_flood_loss)\n",
        "        self.train_losses['damage'].append(epoch_damage_loss)\n",
        "        self.train_losses['total'].append(epoch_total_loss)\n",
        "\n",
        "        return epoch_flood_loss, epoch_damage_loss\n",
        "\n",
        "    def _simultaneous_disaster_step(self, flood_batch, damage_batch, step_idx):\n",
        "        \"\"\"Process both disaster tasks simultaneously on A100\"\"\"\n",
        "        flood_imgs, flood_masks = flood_batch\n",
        "        damage_imgs, damage_masks = damage_batch\n",
        "\n",
        "        # Move to GPU with non-blocking transfer\n",
        "        flood_imgs = flood_imgs.to(self.device, non_blocking=True)\n",
        "        flood_masks = flood_masks.to(self.device, non_blocking=True)\n",
        "        damage_imgs = damage_imgs.to(self.device, non_blocking=True)\n",
        "        damage_masks = damage_masks.to(self.device, non_blocking=True)\n",
        "\n",
        "        metrics = {}\n",
        "\n",
        "        with autocast(device_type=self.device.type):\n",
        "            # Process flood task\n",
        "            flood_out_1, _ = self.model(flood_imgs)\n",
        "            flood_loss, flood_components = self.flood_loss_fn(flood_out_1, flood_masks)\n",
        "\n",
        "            # Process damage task\n",
        "            _, damage_out_2 = self.model(damage_imgs)\n",
        "            damage_loss, damage_components = self.damage_loss_fn(damage_out_2, damage_masks)\n",
        "\n",
        "            # Combined loss with task weighting\n",
        "            total_loss = (\n",
        "                self.config.flood_task_weight * flood_loss +\n",
        "                self.config.damage_task_weight * damage_loss\n",
        "            ) / self.config.accumulation_steps\n",
        "\n",
        "        # Backward pass\n",
        "        self.scaler.scale(total_loss).backward()\n",
        "\n",
        "        # Collect metrics\n",
        "        metrics.update({\n",
        "            'flood_loss': flood_loss.item(),\n",
        "            'damage_loss': damage_loss.item(),\n",
        "            'total_loss': total_loss.item() * self.config.accumulation_steps,\n",
        "            'flood_dice': flood_components.get('dice', 0),\n",
        "            'damage_dice': damage_components.get('dice', 0),\n",
        "        })\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def _disaster_gradient_update(self):\n",
        "        \"\"\"Gradient update optimized for disaster mapping\"\"\"\n",
        "        # Unscale gradients\n",
        "        self.scaler.unscale_(self.optimizer)\n",
        "\n",
        "        # Gradient clipping for stability\n",
        "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.5)\n",
        "\n",
        "        # Step optimizer\n",
        "        self.scaler.step(self.optimizer)\n",
        "        self.scaler.update()\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "    def validate(self, loader, task, num_classes):\n",
        "        \"\"\"Validate model on specific disaster task\"\"\"\n",
        "        self.model.eval()\n",
        "\n",
        "        total_loss = 0\n",
        "        all_ious = []\n",
        "        all_dice = []\n",
        "        all_accuracy = []\n",
        "\n",
        "        # Disable gradient computation for efficiency\n",
        "        with torch.no_grad():\n",
        "            for images, masks in tqdm(loader, desc=f'Validating {task}'):\n",
        "                images = images.to(self.device, non_blocking=True)\n",
        "                masks = masks.to(self.device, non_blocking=True)\n",
        "\n",
        "                # Forward pass with mixed precision\n",
        "                with autocast(device_type=self.device.type):\n",
        "                    flood_out, damage_out = self.model(images)\n",
        "\n",
        "                    # Select appropriate output\n",
        "                    if task == 'flood':\n",
        "                        output = flood_out\n",
        "                        loss, _ = self.flood_loss_fn(output, masks)\n",
        "                    else:\n",
        "                        output = damage_out\n",
        "                        loss, _ = self.damage_loss_fn(output, masks)\n",
        "\n",
        "                total_loss += loss.item()\n",
        "\n",
        "                # Get predictions\n",
        "                preds = torch.argmax(output, dim=1)\n",
        "\n",
        "                # Calculate metrics for each sample in batch\n",
        "                for i in range(preds.shape[0]):\n",
        "                    # IoU per class\n",
        "                    ious = self.metric_calculator.calculate_iou(\n",
        "                        preds[i], masks[i], num_classes\n",
        "                    )\n",
        "                    all_ious.append(ious)\n",
        "\n",
        "                    # Dice coefficient\n",
        "                    dice = self.metric_calculator.calculate_dice(\n",
        "                        preds[i], masks[i], num_classes\n",
        "                    )\n",
        "                    all_dice.append(dice)\n",
        "\n",
        "                    # Pixel accuracy\n",
        "                    accuracy = self.metric_calculator.calculate_pixel_accuracy(\n",
        "                        preds[i], masks[i]\n",
        "                    )\n",
        "                    all_accuracy.append(accuracy)\n",
        "\n",
        "        # Calculate average metrics\n",
        "        avg_loss = total_loss / len(loader)\n",
        "\n",
        "        # Calculate per-class and mean IoU\n",
        "        all_ious = np.array(all_ious)\n",
        "        class_ious = np.nanmean(all_ious, axis=0)\n",
        "        mean_iou = np.nanmean(class_ious)\n",
        "\n",
        "        # Calculate mean dice and accuracy\n",
        "        mean_dice = np.nanmean(all_dice)\n",
        "        mean_accuracy = np.mean(all_accuracy)\n",
        "\n",
        "        # Print validation results\n",
        "        print(f\"\\n{task.upper()} Validation Results:\")\n",
        "        print(f\"Average Loss: {avg_loss:.4f}\")\n",
        "        print(f\"Mean IoU: {mean_iou:.4f}\")\n",
        "        print(f\"Mean Dice: {mean_dice:.4f}\")\n",
        "        print(f\"Pixel Accuracy: {mean_accuracy:.4f}\")\n",
        "\n",
        "        # Per-class IoU\n",
        "        for i, iou in enumerate(class_ious):\n",
        "            class_name = self._get_class_name(task, i)\n",
        "            print(f\"  {class_name} IoU: {iou:.4f}\")\n",
        "\n",
        "        return avg_loss, mean_iou, class_ious\n",
        "\n",
        "    def _get_class_name(self, task, class_idx):\n",
        "        \"\"\"Get human-readable class names for disaster mapping\"\"\"\n",
        "        if task == 'flood':\n",
        "            return ['Non-Flooded', 'Flooded'][class_idx]\n",
        "        else:\n",
        "            return ['No Damage', 'Minor', 'Major', 'Destroyed'][class_idx]\n",
        "\n",
        "    def test(self, loader, task, num_classes):\n",
        "        \"\"\"Test model - same as validate for disaster mapping\"\"\"\n",
        "        return self.validate(loader, task, num_classes)\n",
        "\n",
        "    def save_checkpoint(self, epoch, flood_metrics, damage_metrics, is_best=False):\n",
        "        \"\"\"Save training checkpoint\"\"\"\n",
        "        checkpoint = {\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': self.model.state_dict(),\n",
        "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "            'scaler_state_dict': self.scaler.state_dict(),\n",
        "            'train_losses': self.train_losses,\n",
        "            'val_metrics': self.val_metrics,\n",
        "            'flood_metrics': flood_metrics,\n",
        "            'damage_metrics': damage_metrics,\n",
        "            'best_flood_iou': self.best_flood_iou,\n",
        "            'best_damage_iou': self.best_damage_iou,\n",
        "            'config': self.config.__dict__\n",
        "        }\n",
        "\n",
        "        # Save regular checkpoint\n",
        "        checkpoint_path = os.path.join(\n",
        "            self.config.Checkpoint_Dir,\n",
        "            f'checkpoint_epoch_{epoch:03d}.pth'\n",
        "        )\n",
        "        torch.save(checkpoint, checkpoint_path)\n",
        "        print(f\"Saved checkpoint: {checkpoint_path}\")\n",
        "\n",
        "        # Save best model\n",
        "        if is_best:\n",
        "            best_path = os.path.join(\n",
        "                self.config.Checkpoint_Dir,\n",
        "                'best_model.pth'\n",
        "            )\n",
        "            torch.save(checkpoint, best_path)\n",
        "            print(f\"Saved best model: {best_path}\")\n",
        "\n",
        "        # Keep only last 5 checkpoints to save space\n",
        "        self._cleanup_old_checkpoints(keep_last=5)\n",
        "\n",
        "    def load_checkpoint(self, checkpoint_path):\n",
        "        \"\"\"Load checkpoint for resuming training\"\"\"\n",
        "        print(f\"Loading checkpoint from {checkpoint_path}\")\n",
        "\n",
        "        checkpoint = torch.load(checkpoint_path, map_location=self.device)\n",
        "\n",
        "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        self.scaler.load_state_dict(checkpoint['scaler_state_dict'])\n",
        "\n",
        "        self.train_losses = checkpoint.get('train_losses', {'flood': [], 'damage': [], 'total': []})\n",
        "        self.val_metrics = checkpoint.get('val_metrics', {'flood_iou': [], 'damage_iou': []})\n",
        "        self.best_flood_iou = checkpoint.get('best_flood_iou', 0.0)\n",
        "        self.best_damage_iou = checkpoint.get('best_damage_iou', 0.0)\n",
        "\n",
        "        print(f\"Resumed from epoch {checkpoint['epoch']}\")\n",
        "        print(f\"Best Flood IoU: {self.best_flood_iou:.4f}\")\n",
        "        print(f\"Best Damage IoU: {self.best_damage_iou:.4f}\")\n",
        "\n",
        "        return checkpoint['epoch']\n",
        "\n",
        "    def _cleanup_old_checkpoints(self, keep_last=5):\n",
        "        \"\"\"Remove old checkpoints to save disk space\"\"\"\n",
        "        checkpoint_files = sorted([\n",
        "            f for f in os.listdir(self.config.Checkpoint_Dir)\n",
        "            if f.startswith('checkpoint_epoch_') and f.endswith('.pth')\n",
        "        ])\n",
        "\n",
        "        if len(checkpoint_files) > keep_last:\n",
        "            for f in checkpoint_files[:-keep_last]:\n",
        "                os.remove(os.path.join(self.config.Checkpoint_Dir, f))\n",
        "\n",
        "    def plot_training_history(self):\n",
        "        \"\"\"Generate training history plots\"\"\"\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "        # Loss plot\n",
        "        ax = axes[0, 0]\n",
        "        epochs = range(1, len(self.train_losses['total']) + 1)\n",
        "        ax.plot(epochs, self.train_losses['flood'], 'b-', label='Flood Loss')\n",
        "        ax.plot(epochs, self.train_losses['damage'], 'r-', label='Damage Loss')\n",
        "        ax.plot(epochs, self.train_losses['total'], 'g--', label='Total Loss')\n",
        "        ax.set_xlabel('Epoch')\n",
        "        ax.set_ylabel('Loss')\n",
        "        ax.set_title('Training Loss History')\n",
        "        ax.legend()\n",
        "        ax.grid(True)\n",
        "\n",
        "        # IoU plot\n",
        "        ax = axes[0, 1]\n",
        "        if self.val_metrics['flood_iou']:\n",
        "            epochs_val = range(1, len(self.val_metrics['flood_iou']) + 1)\n",
        "            ax.plot(epochs_val, self.val_metrics['flood_iou'], 'b-o', label='Flood IoU')\n",
        "            ax.plot(epochs_val, self.val_metrics['damage_iou'], 'r-o', label='Damage IoU')\n",
        "            ax.set_xlabel('Epoch')\n",
        "            ax.set_ylabel('IoU')\n",
        "            ax.set_title('Validation IoU History')\n",
        "            ax.legend()\n",
        "            ax.grid(True)\n",
        "\n",
        "        # Learning rate plot\n",
        "        ax = axes[1, 0]\n",
        "        lrs = [group['lr'] for group in self.optimizer.param_groups]\n",
        "        ax.plot([lrs[0]] * len(epochs), 'b-', label='Backbone LR')\n",
        "        ax.plot([lrs[-1]] * len(epochs), 'r-', label='Head LR')\n",
        "        ax.set_xlabel('Epoch')\n",
        "        ax.set_ylabel('Learning Rate')\n",
        "        ax.set_title('Learning Rate Schedule')\n",
        "        ax.legend()\n",
        "        ax.set_yscale('log')\n",
        "        ax.grid(True)\n",
        "\n",
        "        # Summary text\n",
        "        ax = axes[1, 1]\n",
        "        ax.axis('off')\n",
        "\n",
        "        # Fix the string formatting\n",
        "        final_flood_iou = self.val_metrics['flood_iou'][-1] if self.val_metrics['flood_iou'] else 'N/A'\n",
        "        final_damage_iou = self.val_metrics['damage_iou'][-1] if self.val_metrics['damage_iou'] else 'N/A'\n",
        "\n",
        "        summary_text = f\"\"\"\n",
        "        Training Summary:\n",
        "\n",
        "        Total Epochs: {len(self.train_losses['total'])}\n",
        "        Best Flood IoU: {self.best_flood_iou:.4f}\n",
        "        Best Damage IoU: {self.best_damage_iou:.4f}\n",
        "\n",
        "        Final Training Loss:\n",
        "        - Flood: {self.train_losses['flood'][-1]:.4f if self.train_losses['flood'] else 'N/A'}\n",
        "        - Damage: {self.train_losses['damage'][-1]:.4f if self.train_losses['damage'] else 'N/A'}\n",
        "\n",
        "        Final Validation IoU:\n",
        "        - Flood: {final_flood_iou}\n",
        "        - Damage: {final_damage_iou}\n",
        "        \"\"\"\n",
        "        ax.text(0.1, 0.5, summary_text, fontsize=12,\n",
        "              verticalalignment='center', fontfamily='monospace')\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Save plot\n",
        "        plot_path = os.path.join(self.config.Results_Dir, 'training_history.png')\n",
        "        plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "        print(f\"Training history plot saved to {plot_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIn9Yb5zoET2"
      },
      "source": [
        "## Main Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "FWiSNbJ9oCeW"
      },
      "outputs": [],
      "source": [
        "def train_complete_model(config):\n",
        "    \"\"\"\n",
        "    Complete training pipeline\n",
        "    \"\"\"\n",
        "    # Set device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    if device.type == 'cuda':\n",
        "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "        print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "\n",
        "    # Create datasets\n",
        "    print(\"\\nCreating datasets...\")\n",
        "\n",
        "\n",
        "    # Create augmentation pipelines\n",
        "    train_transform = get_training_augmentation()\n",
        "    val_transform = get_validation_augmentation()\n",
        "\n",
        "    # Create datasets\n",
        "    flood_train_dataset = FloodNetDataset(FloodNet_train_img_dir, FloodNet_train_mask_dir, train_transform, binary_flood=True)\n",
        "    flood_val_dataset = FloodNetDataset(FloodNet_val_img_dir, FloodNet_val_mask_dir, val_transform, binary_flood=True)\n",
        "    flood_test_dataset = FloodNetDataset(FloodNet_test_img_dir, FloodNet_test_mask_dir, val_transform, binary_flood=True)\n",
        "\n",
        "    damage_train_dataset = RescueNetDataset(RescueNet_train_img_dir, RescueNet_train_mask_dir, train_transform)\n",
        "    damage_val_dataset = RescueNetDataset(RescueNet_val_img_dir, RescueNet_val_mask_dir, val_transform)\n",
        "    damage_test_dataset = RescueNetDataset(RescueNet_test_img_dir, RescueNet_test_mask_dir, val_transform)\n",
        "\n",
        "    space_train_dataset = SpaceNet8Dataset(sn8_train_manifest, augment=train_transform, cache_size=config.spacenet_cache_size)\n",
        "    space_val_dataset = SpaceNet8Dataset(sn8_val_manifest, augment=val_transform, cache_size=config.spacenet_cache_size)\n",
        "\n",
        "    space_train_dataset = SpaceNet8CompatWrapper(space_train_dataset)\n",
        "    space_val_dataset = SpaceNet8CompatWrapper(space_val_dataset)\n",
        "\n",
        "\n",
        "    # Concatenate FloodNet & SpaceNet 8  datasets\n",
        "    flood_train_combined = ConcatDataset([flood_train_dataset, space_train_dataset])\n",
        "    flood_val_combined = ConcatDataset([flood_val_dataset, space_val_dataset])\n",
        "\n",
        "\n",
        "    # Create dataloaders\n",
        "    print(\"Creating dataloaders...\")\n",
        "\n",
        "    flood_train_loader = DataLoader(\n",
        "        flood_train_combined,\n",
        "        batch_size=config.batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=config.num_workers,\n",
        "        pin_memory=config.pin_memory,\n",
        "        prefetch_factor=config.prefetch_factor,\n",
        "        persistent_workers=config.persistent_workers\n",
        "    )\n",
        "\n",
        "    flood_val_loader = DataLoader(\n",
        "        flood_val_combined,\n",
        "        batch_size=config.batch_size * 2,\n",
        "        shuffle=False,\n",
        "        num_workers=config.num_workers,\n",
        "        pin_memory=config.pin_memory,\n",
        "        prefetch_factor=config.prefetch_factor,\n",
        "        persistent_workers=config.persistent_workers\n",
        "    )\n",
        "\n",
        "    flood_test_loader = DataLoader(\n",
        "        flood_test_dataset,\n",
        "        batch_size=config.batch_size * 2,\n",
        "        shuffle=False,\n",
        "        num_workers=config.num_workers,\n",
        "        pin_memory=config.pin_memory,\n",
        "        prefetch_factor=config.prefetch_factor,\n",
        "        persistent_workers=config.persistent_workers\n",
        "    )\n",
        "\n",
        "    damage_train_loader = DataLoader(\n",
        "        damage_train_dataset,\n",
        "        batch_size=config.batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=config.num_workers,\n",
        "        pin_memory=config.pin_memory,\n",
        "        prefetch_factor=config.prefetch_factor,\n",
        "        persistent_workers=config.persistent_workers\n",
        "    )\n",
        "\n",
        "    damage_val_loader = DataLoader(\n",
        "        damage_val_dataset,\n",
        "        batch_size=config.batch_size * 2,\n",
        "        shuffle=False,\n",
        "        num_workers=config.num_workers,\n",
        "        pin_memory=config.pin_memory,\n",
        "        prefetch_factor=config.prefetch_factor,\n",
        "        persistent_workers=config.persistent_workers\n",
        "    )\n",
        "\n",
        "    damage_test_loader = DataLoader(\n",
        "        damage_test_dataset,\n",
        "        batch_size=config.batch_size * 2,\n",
        "        shuffle=False,\n",
        "        num_workers=config.num_workers,\n",
        "        pin_memory=config.pin_memory,\n",
        "        prefetch_factor=config.prefetch_factor,\n",
        "        persistent_workers=config.persistent_workers\n",
        "    )\n",
        "\n",
        "    # Initialize model\n",
        "    print(\"\\nInitializing model...\")\n",
        "    model = EnhancedDisasterModel(\n",
        "        num_classes_flood=config.num_classes_flood,\n",
        "        num_classes_damage=config.num_classes_damage,\n",
        "        backbone=config.backbone\n",
        "    )\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Count parameters\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"Total parameters: {total_params:,}\")\n",
        "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "    # Initialize trainer\n",
        "    trainer = OptimizedTrainer(model, config, device)\n",
        "\n",
        "    # Optional: Resume from checkpoint\n",
        "    start_epoch = 0\n",
        "    if os.path.exists(os.path.join(config.Checkpoint_Dir, 'best_model.pth')):\n",
        "        response = input(\"Found existing checkpoint. Resume training? (y/n): \")\n",
        "        if response.lower() == 'y':\n",
        "            start_epoch = trainer.load_checkpoint(\n",
        "                os.path.join(config.Checkpoint_Dir, 'best_model.pth')\n",
        "            )\n",
        "\n",
        "    # Training loop\n",
        "    print(f\"\\nStarting training from epoch {start_epoch + 1}...\")\n",
        "    print(f\"Total epochs: {config.num_epochs}\")\n",
        "    print(f\"Batch size: {config.batch_size}\")\n",
        "    print(f\"Effective batch size: {config.batch_size * config.accumulation_steps}\")\n",
        "\n",
        "    best_combined_score = 0.0\n",
        "\n",
        "    for epoch in range(start_epoch + 1, config.num_epochs + 1):\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"EPOCH {epoch}/{config.num_epochs}\")\n",
        "        print('='*60)\n",
        "\n",
        "        # Training\n",
        "        flood_loss, damage_loss = trainer.train_epoch(\n",
        "            flood_train_loader, damage_train_loader, epoch\n",
        "        )\n",
        "\n",
        "        # Validation (every 2 epochs to save time)\n",
        "        if epoch % 2 == 0 or epoch == config.num_epochs:\n",
        "            print(\"\\nRunning validation...\")\n",
        "\n",
        "            # Validate flood task\n",
        "            flood_val_loss, flood_iou, flood_class_ious = trainer.validate(\n",
        "                flood_val_loader, 'flood', config.num_classes_flood\n",
        "            )\n",
        "\n",
        "            # Validate damage task\n",
        "            damage_val_loss, damage_iou, damage_class_ious = trainer.validate(\n",
        "                damage_val_loader, 'damage', config.num_classes_damage\n",
        "            )\n",
        "\n",
        "            # Store metrics\n",
        "            trainer.val_metrics['flood_iou'].append(flood_iou)\n",
        "            trainer.val_metrics['damage_iou'].append(damage_iou)\n",
        "\n",
        "            # Check if best model\n",
        "            combined_score = (flood_iou + damage_iou) / 2\n",
        "            is_best = combined_score > best_combined_score\n",
        "\n",
        "            if is_best:\n",
        "                best_combined_score = combined_score\n",
        "                trainer.best_flood_iou = flood_iou\n",
        "                trainer.best_damage_iou = damage_iou\n",
        "\n",
        "            # Save checkpoint\n",
        "            flood_metrics = {'iou': flood_iou, 'loss': flood_val_loss, 'class_ious': flood_class_ious}\n",
        "            damage_metrics = {'iou': damage_iou, 'loss': damage_val_loss, 'class_ious': damage_class_ious}\n",
        "\n",
        "            trainer.save_checkpoint(epoch, flood_metrics, damage_metrics, is_best)\n",
        "\n",
        "        # Plot training history\n",
        "        if epoch % 5 == 0:\n",
        "            trainer.plot_training_history()\n",
        "\n",
        "    # Final evaluation on test set\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"FINAL EVALUATION ON TEST SET\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Test flood detection\n",
        "    print(\"\\nFlood Detection Test Results:\")\n",
        "    flood_test_loss, flood_test_iou, flood_test_class_ious = trainer.test(\n",
        "        flood_test_loader, 'flood', config.num_classes_flood\n",
        "    )\n",
        "\n",
        "    # Test damage assessment\n",
        "    print(\"\\nDamage Assessment Test Results:\")\n",
        "    damage_test_loss, damage_test_iou, damage_test_class_ious = trainer.test(\n",
        "        damage_test_loader, 'damage', config.num_classes_damage\n",
        "    )\n",
        "\n",
        "    # Save final results\n",
        "    final_results = {\n",
        "        'flood_test': {\n",
        "            'mean_iou': flood_test_iou,\n",
        "            'class_ious': flood_test_class_ious.tolist(),\n",
        "            'loss': flood_test_loss\n",
        "        },\n",
        "        'damage_test': {\n",
        "            'mean_iou': damage_test_iou,\n",
        "            'class_ious': damage_test_class_ious.tolist(),\n",
        "            'loss': damage_test_loss\n",
        "        },\n",
        "        'training_config': config.__dict__\n",
        "    }\n",
        "\n",
        "    import json\n",
        "    with open(os.path.join(config.Results_Dir, 'final_results.json'), 'w') as f:\n",
        "        json.dump(final_results, f, indent=2)\n",
        "\n",
        "    print(\"\\nTraining complete!\")\n",
        "    print(f\"Best Flood IoU: {trainer.best_flood_iou:.4f}\")\n",
        "    print(f\"Best Damage IoU: {trainer.best_damage_iou:.4f}\")\n",
        "    print(f\"Results saved to {config.Results_Dir}\")\n",
        "\n",
        "    return model, trainer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38QRHhtnXXjQ"
      },
      "source": [
        "Duration estimation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7dEssuHlXKDq"
      },
      "outputs": [],
      "source": [
        "# class TrainingTimeEstimator:\n",
        "#     \"\"\"Estimates training time for disaster mapping model on A100\"\"\"\n",
        "\n",
        "#     def __init__(self, config):\n",
        "#         self.config = config\n",
        "#         self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "#         # A100 performance benchmarks (empirically derived)\n",
        "#         self.a100_benchmarks = {\n",
        "#             'resnet50_fps': 85,   # Images per second for ResNet50 backbone\n",
        "#             'resnet101_fps': 65,  # Images per second for ResNet101 backbone\n",
        "#             'memory_overhead': 0.3,  # 30% overhead for caching/misc\n",
        "#             'validation_factor': 0.2,  # Validation takes 20% of training time\n",
        "#             'io_overhead': 0.15,     # 15% overhead for data loading\n",
        "#         }\n",
        "\n",
        "#     def estimate_training_time(self, dataset_sizes, sample_batch=True):\n",
        "#         \"\"\"\n",
        "#         Estimate total training time for disaster mapping pipeline\n",
        "\n",
        "#         Args:\n",
        "#             dataset_sizes: Dict with 'flood_train', 'flood_val', 'damage_train', 'damage_val'\n",
        "#             sample_batch: Whether to run a sample batch for accurate timing\n",
        "\n",
        "#         Returns:\n",
        "#             Dict with detailed time estimates\n",
        "#         \"\"\"\n",
        "#         print(\"=\" * 60)\n",
        "#         print(\"HERMES 0.3 - A100 Training Time Estimation\")\n",
        "#         print(\"=\" * 60)\n",
        "\n",
        "#         # System checks\n",
        "#         self._check_system_resources()\n",
        "\n",
        "#         # Dataset analysis\n",
        "#         total_samples = sum(dataset_sizes.values())\n",
        "#         train_samples = dataset_sizes.get('flood_train', 0) + dataset_sizes.get('damage_train', 0)\n",
        "#         val_samples = dataset_sizes.get('flood_val', 0) + dataset_sizes.get('damage_val', 0)\n",
        "\n",
        "#         print(f\"\\nDataset Analysis:\")\n",
        "#         print(f\"  Total training samples: {train_samples:,}\")\n",
        "#         print(f\"  Total validation samples: {val_samples:,}\")\n",
        "#         print(f\"  Total samples: {total_samples:,}\")\n",
        "\n",
        "#         # Model complexity analysis\n",
        "#         model_complexity = self._analyze_model_complexity()\n",
        "\n",
        "#         # Batch processing estimates\n",
        "#         batch_estimates = self._estimate_batch_processing()\n",
        "\n",
        "#         # Optional: Run sample batch for accuracy\n",
        "#         if sample_batch and torch.cuda.is_available():\n",
        "#             print(f\"\\nRunning sample batch for accurate timing...\")\n",
        "#             sample_timing = self._run_sample_batch()\n",
        "#             # Use sample timing to calibrate estimates\n",
        "#             batch_estimates['time_per_batch'] = sample_timing['time_per_batch']\n",
        "#             batch_estimates['memory_usage'] = sample_timing['peak_memory']\n",
        "\n",
        "#         # Calculate epoch timing\n",
        "#         epoch_timing = self._calculate_epoch_timing(dataset_sizes, batch_estimates)\n",
        "\n",
        "#         # Calculate total training time\n",
        "#         total_timing = self._calculate_total_timing(epoch_timing)\n",
        "\n",
        "#         # Generate detailed report\n",
        "#         report = self._generate_timing_report(\n",
        "#             dataset_sizes, model_complexity, batch_estimates,\n",
        "#             epoch_timing, total_timing\n",
        "#         )\n",
        "\n",
        "#         return report\n",
        "\n",
        "#     def _check_system_resources(self):\n",
        "#         \"\"\"Check system resources and warn about potential issues\"\"\"\n",
        "#         print(f\"\\nSystem Resource Check:\")\n",
        "\n",
        "#         # GPU check\n",
        "#         if torch.cuda.is_available():\n",
        "#             gpu_name = torch.cuda.get_device_name(0)\n",
        "#             gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "#             print(f\"  GPU: {gpu_name}\")\n",
        "#             print(f\"  GPU Memory: {gpu_memory:.1f} GB\")\n",
        "\n",
        "#             if 'A100' not in gpu_name:\n",
        "#                 print(f\"    WARNING: Not using A100, estimates may be inaccurate\")\n",
        "\n",
        "#             if gpu_memory < 70:\n",
        "#                 print(f\"    WARNING: Limited GPU memory, may need smaller batch sizes\")\n",
        "\n",
        "#         else:\n",
        "#             print(f\"   ERROR: No CUDA GPU available\")\n",
        "#             return False\n",
        "\n",
        "#         # CPU and RAM check\n",
        "#         cpu_count = psutil.cpu_count()\n",
        "#         ram_gb = psutil.virtual_memory().total / 1e9\n",
        "#         print(f\"  CPU Cores: {cpu_count}\")\n",
        "#         print(f\"  RAM: {ram_gb:.1f} GB\")\n",
        "\n",
        "#         if cpu_count < 8:\n",
        "#             print(f\"    WARNING: Limited CPU cores, data loading may be slow\")\n",
        "\n",
        "#         if ram_gb < 32:\n",
        "#             print(f\"    WARNING: Limited RAM, may impact data caching\")\n",
        "\n",
        "#         return True\n",
        "\n",
        "#     def _analyze_model_complexity(self):\n",
        "#         \"\"\"Analyze model complexity for timing estimates\"\"\"\n",
        "#         print(f\"\\nModel Complexity Analysis:\")\n",
        "\n",
        "#         # Create a minimal model to count parameters\n",
        "#         try:\n",
        "#             model = EnhancedDisasterModel(\n",
        "#                 num_classes_flood=self.config.num_classes_flood,\n",
        "#                 num_classes_damage=self.config.num_classes_damage,\n",
        "#                 backbone=self.config.backbone\n",
        "#             )\n",
        "\n",
        "#             total_params = sum(p.numel() for p in model.parameters())\n",
        "#             trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "#             print(f\"  Backbone: {self.config.backbone}\")\n",
        "#             print(f\"  Total parameters: {total_params:,}\")\n",
        "#             print(f\"  Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "#             # Memory estimate\n",
        "#             param_memory = total_params * 4 / 1e9  # 4 bytes per float32\n",
        "#             activation_memory = self._estimate_activation_memory()\n",
        "#             total_memory = param_memory + activation_memory\n",
        "\n",
        "#             print(f\"  Parameter memory: {param_memory:.2f} GB\")\n",
        "#             print(f\"  Activation memory: {activation_memory:.2f} GB\")\n",
        "#             print(f\"  Total model memory: {total_memory:.2f} GB\")\n",
        "\n",
        "#             del model  # Free memory\n",
        "#             torch.cuda.empty_cache()\n",
        "\n",
        "#             return {\n",
        "#                 'total_params': total_params,\n",
        "#                 'backbone': self.config.backbone,\n",
        "#                 'memory_gb': total_memory,\n",
        "#                 'complexity_factor': self._get_complexity_factor()\n",
        "#             }\n",
        "\n",
        "#         except Exception as e:\n",
        "#             print(f\"    Could not analyze model: {e}\")\n",
        "#             return {\n",
        "#                 'total_params': 60_000_000,  # Rough estimate\n",
        "#                 'backbone': self.config.backbone,\n",
        "#                 'memory_gb': 8.0,\n",
        "#                 'complexity_factor': 1.2\n",
        "#             }\n",
        "\n",
        "#     def _estimate_activation_memory(self):\n",
        "#         \"\"\"Estimate activation memory based on config\"\"\"\n",
        "#         batch_size = self.config.batch_size\n",
        "#         img_size = self.config.img_height * self.config.img_width\n",
        "\n",
        "#         # Rough estimate based on typical CNN activations\n",
        "#         activation_memory = (\n",
        "#             batch_size * img_size * 3 * 4 +  # Input\n",
        "#             batch_size * img_size * 256 * 4 +  # Feature maps\n",
        "#             batch_size * img_size * 64 * 4     # Output layers\n",
        "#         ) / 1e9  # Convert to GB\n",
        "\n",
        "#         return activation_memory\n",
        "\n",
        "#     def _get_complexity_factor(self):\n",
        "#         \"\"\"Get complexity factor based on backbone\"\"\"\n",
        "#         factors = {\n",
        "#             'resnet50': 1.0,\n",
        "#             'resnet101': 1.6,\n",
        "#             'resnet152': 2.2\n",
        "#         }\n",
        "#         return factors.get(self.config.backbone, 1.2)\n",
        "\n",
        "#     def _estimate_batch_processing(self):\n",
        "#         \"\"\"Estimate batch processing time\"\"\"\n",
        "#         print(f\"\\nBatch Processing Analysis:\")\n",
        "\n",
        "#         # Base throughput from A100 benchmarks\n",
        "#         if 'resnet101' in self.config.backbone:\n",
        "#             base_fps = self.a100_benchmarks['resnet101_fps']\n",
        "#         else:\n",
        "#             base_fps = self.a100_benchmarks['resnet50_fps']\n",
        "\n",
        "#         # Adjust for multi-task learning\n",
        "#         multi_task_factor = 1.8  # Processing both flood and damage tasks\n",
        "#         effective_fps = base_fps / multi_task_factor\n",
        "\n",
        "#         # Adjust for image size\n",
        "#         size_factor = (self.config.img_height * self.config.img_width) / (512 * 512)\n",
        "#         effective_fps = effective_fps / size_factor\n",
        "\n",
        "#         # Adjust for batch size\n",
        "#         batch_efficiency = min(1.0, self.config.batch_size / 16)  # Optimal around batch size 16\n",
        "#         effective_fps = effective_fps * batch_efficiency\n",
        "\n",
        "#         time_per_batch = self.config.batch_size / effective_fps\n",
        "\n",
        "#         print(f\"  Base FPS ({self.config.backbone}): {base_fps}\")\n",
        "#         print(f\"  Multi-task factor: {multi_task_factor:.1f}x\")\n",
        "#         print(f\"  Image size factor: {size_factor:.1f}x\")\n",
        "#         print(f\"  Effective FPS: {effective_fps:.1f}\")\n",
        "#         print(f\"  Time per batch: {time_per_batch:.3f} seconds\")\n",
        "\n",
        "#         return {\n",
        "#             'effective_fps': effective_fps,\n",
        "#             'time_per_batch': time_per_batch,\n",
        "#             'estimated_memory': self._estimate_batch_memory()\n",
        "#         }\n",
        "\n",
        "#     def _estimate_batch_memory(self):\n",
        "#         \"\"\"Estimate memory usage per batch\"\"\"\n",
        "#         batch_size = self.config.batch_size\n",
        "#         img_pixels = self.config.img_height * self.config.img_width * 3\n",
        "\n",
        "#         # Input batch + gradients + activations\n",
        "#         batch_memory = (\n",
        "#             batch_size * img_pixels * 4 * 3  # Input, gradients, activations\n",
        "#         ) / 1e9\n",
        "\n",
        "#         return batch_memory\n",
        "\n",
        "#     def _run_sample_batch(self):\n",
        "#         \"\"\"Run a sample batch to get accurate timing\"\"\"\n",
        "#         try:\n",
        "#             # Create minimal model\n",
        "#             model = EnhancedDisasterModel(\n",
        "#                 num_classes_flood=self.config.num_classes_flood,\n",
        "#                 num_classes_damage=self.config.num_classes_damage,\n",
        "#                 backbone=self.config.backbone\n",
        "#             ).cuda()\n",
        "\n",
        "#             model.train()\n",
        "#             optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
        "#             scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "#             # Create sample data\n",
        "#             batch_size = self.config.batch_size\n",
        "#             sample_images = torch.randn(\n",
        "#                 batch_size, 3, self.config.img_height, self.config.img_width\n",
        "#             ).cuda()\n",
        "#             sample_masks = torch.randint(\n",
        "#                 0, 2, (batch_size, self.config.img_height, self.config.img_width)\n",
        "#             ).cuda()\n",
        "\n",
        "#             # Warm up\n",
        "#             for _ in range(3):\n",
        "#                 with torch.cuda.amp.autocast():\n",
        "#                     flood_out, damage_out = model(sample_images)\n",
        "#                     loss = torch.nn.functional.cross_entropy(flood_out, sample_masks)\n",
        "\n",
        "#                 scaler.scale(loss).backward()\n",
        "#                 scaler.step(optimizer)\n",
        "#                 scaler.update()\n",
        "#                 optimizer.zero_grad()\n",
        "\n",
        "#             torch.cuda.synchronize()\n",
        "\n",
        "#             # Actual timing\n",
        "#             num_runs = 10\n",
        "#             start_time = time.time()\n",
        "#             start_memory = torch.cuda.memory_allocated()\n",
        "\n",
        "#             for _ in range(num_runs):\n",
        "#                 with torch.cuda.amp.autocast():\n",
        "#                     flood_out, damage_out = model(sample_images)\n",
        "#                     flood_loss = torch.nn.functional.cross_entropy(flood_out, sample_masks)\n",
        "#                     damage_loss = torch.nn.functional.cross_entropy(damage_out, sample_masks)\n",
        "#                     total_loss = flood_loss + damage_loss\n",
        "\n",
        "#                 scaler.scale(total_loss).backward()\n",
        "#                 scaler.step(optimizer)\n",
        "#                 scaler.update()\n",
        "#                 optimizer.zero_grad()\n",
        "\n",
        "#             torch.cuda.synchronize()\n",
        "#             end_time = time.time()\n",
        "#             peak_memory = torch.cuda.max_memory_allocated()\n",
        "\n",
        "#             # Calculate metrics\n",
        "#             total_time = end_time - start_time\n",
        "#             time_per_batch = total_time / num_runs\n",
        "#             peak_memory_gb = peak_memory / 1e9\n",
        "\n",
        "#             print(f\"    Sample batch timing: {time_per_batch:.3f} seconds\")\n",
        "#             print(f\"    Peak GPU memory: {peak_memory_gb:.2f} GB\")\n",
        "\n",
        "#             # Cleanup\n",
        "#             del model, optimizer, scaler, sample_images, sample_masks\n",
        "#             torch.cuda.empty_cache()\n",
        "\n",
        "#             return {\n",
        "#                 'time_per_batch': time_per_batch,\n",
        "#                 'peak_memory': peak_memory_gb\n",
        "#             }\n",
        "\n",
        "#         except Exception as e:\n",
        "#             print(f\"     Sample batch failed: {e}\")\n",
        "#             return {\n",
        "#                 'time_per_batch': 0.5,  # Fallback estimate\n",
        "#                 'peak_memory': 12.0\n",
        "#             }\n",
        "\n",
        "#     def _calculate_epoch_timing(self, dataset_sizes, batch_estimates):\n",
        "#         \"\"\"Calculate timing for one epoch\"\"\"\n",
        "#         print(f\"\\nEpoch Timing Calculation:\")\n",
        "\n",
        "#         # Training steps per epoch\n",
        "#         flood_batches = np.ceil(dataset_sizes.get('flood_train', 0) / self.config.batch_size)\n",
        "#         damage_batches = np.ceil(dataset_sizes.get('damage_train', 0) / self.config.batch_size)\n",
        "\n",
        "#         # Use simultaneous processing (both tasks per iteration)\n",
        "#         steps_per_epoch = max(flood_batches, damage_batches)\n",
        "\n",
        "#         # Training time\n",
        "#         train_time_per_epoch = steps_per_epoch * batch_estimates['time_per_batch']\n",
        "\n",
        "#         # Add gradient accumulation overhead\n",
        "#         if hasattr(self.config, 'accumulation_steps') and self.config.accumulation_steps > 1:\n",
        "#             accumulation_overhead = train_time_per_epoch * 0.1  # 10% overhead\n",
        "#             train_time_per_epoch += accumulation_overhead\n",
        "\n",
        "#         # Validation time (every few epochs)\n",
        "#         val_flood_batches = np.ceil(dataset_sizes.get('flood_val', 0) / (self.config.batch_size * 2))\n",
        "#         val_damage_batches = np.ceil(dataset_sizes.get('damage_val', 0) / (self.config.batch_size * 2))\n",
        "#         val_steps = val_flood_batches + val_damage_batches\n",
        "#         val_time = val_steps * batch_estimates['time_per_batch'] * 0.5  # Validation is faster\n",
        "\n",
        "#         print(f\"  Training steps per epoch: {int(steps_per_epoch):,}\")\n",
        "#         print(f\"  Training time per epoch: {timedelta(seconds=int(train_time_per_epoch))}\")\n",
        "#         print(f\"  Validation steps: {int(val_steps):,}\")\n",
        "#         print(f\"  Validation time: {timedelta(seconds=int(val_time))}\")\n",
        "\n",
        "#         return {\n",
        "#             'train_time': train_time_per_epoch,\n",
        "#             'val_time': val_time,\n",
        "#             'steps_per_epoch': steps_per_epoch,\n",
        "#             'total_epoch_time': train_time_per_epoch + (val_time / 2)  # Val every 2 epochs\n",
        "#         }\n",
        "\n",
        "#     def _calculate_total_timing(self, epoch_timing):\n",
        "#         \"\"\"Calculate total training time\"\"\"\n",
        "#         print(f\"\\nTotal Training Time Calculation:\")\n",
        "\n",
        "#         num_epochs = self.config.num_epochs\n",
        "#         epoch_time = epoch_timing['total_epoch_time']\n",
        "\n",
        "#         # Base training time\n",
        "#         base_training_time = num_epochs * epoch_time\n",
        "\n",
        "#         # Add overhead factors\n",
        "#         io_overhead = base_training_time * self.a100_benchmarks['io_overhead']\n",
        "#         setup_overhead = 300  # 5 minutes for setup/initialization\n",
        "#         checkpoint_overhead = num_epochs * 30  # 30 seconds per checkpoint\n",
        "\n",
        "#         total_time = base_training_time + io_overhead + setup_overhead + checkpoint_overhead\n",
        "\n",
        "#         print(f\"  Base training time: {timedelta(seconds=int(base_training_time))}\")\n",
        "#         print(f\"  I/O overhead: {timedelta(seconds=int(io_overhead))}\")\n",
        "#         print(f\"  Setup/checkpoint overhead: {timedelta(seconds=int(setup_overhead + checkpoint_overhead))}\")\n",
        "#         print(f\"  Total estimated time: {timedelta(seconds=int(total_time))}\")\n",
        "\n",
        "#         # Cost estimation (rough)\n",
        "#         hours = total_time / 3600\n",
        "#         estimated_cost = hours * 2.5  # ~$2.5/hour for Colab Pro A100\n",
        "\n",
        "#         print(f\"  Estimated Colab Pro cost: ${estimated_cost:.2f}\")\n",
        "\n",
        "#         return {\n",
        "#             'total_seconds': total_time,\n",
        "#             'total_hours': hours,\n",
        "#             'estimated_cost': estimated_cost,\n",
        "#             'breakdown': {\n",
        "#                 'training': base_training_time,\n",
        "#                 'io_overhead': io_overhead,\n",
        "#                 'setup_checkpoint': setup_overhead + checkpoint_overhead\n",
        "#             }\n",
        "#         }\n",
        "\n",
        "#     def _generate_timing_report(self, dataset_sizes, model_complexity,\n",
        "#                                batch_estimates, epoch_timing, total_timing):\n",
        "#         \"\"\"Generate comprehensive timing report\"\"\"\n",
        "\n",
        "#         report = {\n",
        "#             'summary': {\n",
        "#                 'total_time_hours': total_timing['total_hours'],\n",
        "#                 'total_time_formatted': str(timedelta(seconds=int(total_timing['total_seconds']))),\n",
        "#                 'estimated_cost_usd': total_timing['estimated_cost'],\n",
        "#                 'epochs': self.config.num_epochs,\n",
        "#                 'batch_size': self.config.batch_size\n",
        "#             },\n",
        "#             'dataset_info': dataset_sizes,\n",
        "#             'model_info': model_complexity,\n",
        "#             'performance': {\n",
        "#                 'time_per_batch_seconds': batch_estimates['time_per_batch'],\n",
        "#                 'effective_fps': batch_estimates['effective_fps'],\n",
        "#                 'steps_per_epoch': epoch_timing['steps_per_epoch'],\n",
        "#                 'epoch_time_minutes': epoch_timing['total_epoch_time'] / 60\n",
        "#             },\n",
        "#             'resources': {\n",
        "#                 'estimated_gpu_memory_gb': batch_estimates.get('estimated_memory', 0) + model_complexity['memory_gb'],\n",
        "#                 'gpu_utilization_percent': 85  # Estimated\n",
        "#             },\n",
        "#             'time_breakdown': total_timing['breakdown'],\n",
        "#             'recommendations': self._generate_recommendations(total_timing, batch_estimates)\n",
        "#         }\n",
        "\n",
        "#         return report\n",
        "\n",
        "#     def _generate_recommendations(self, total_timing, batch_estimates):\n",
        "#         \"\"\"Generate recommendations based on estimates\"\"\"\n",
        "#         recommendations = []\n",
        "\n",
        "#         if total_timing['total_hours'] > 8:\n",
        "#             recommendations.append(\" Training will take >8 hours. Consider reducing epochs or using larger batch size.\")\n",
        "\n",
        "#         if total_timing['estimated_cost'] > 50:\n",
        "#             recommendations.append(\" High cost estimate (>${:.0f}). Consider optimizing hyperparameters.\".format(total_timing['estimated_cost']))\n",
        "\n",
        "#         if batch_estimates['time_per_batch'] > 1.0:\n",
        "#             recommendations.append(\" Slow batch processing. Consider reducing image size or model complexity.\")\n",
        "\n",
        "#         if self.config.batch_size < 16:\n",
        "#             recommendations.append(\" Small batch size. A100 can handle larger batches for better efficiency.\")\n",
        "\n",
        "#         if not hasattr(self.config, 'spacenet_cache_size') or self.config.spacenet_cache_size < 200:\n",
        "#             recommendations.append(\" Increase cache size to 200-500 for A100 to reduce I/O overhead.\")\n",
        "\n",
        "#         if not recommendations:\n",
        "#             recommendations.append(\" Configuration looks well-optimized for A100!\")\n",
        "\n",
        "#         return recommendations\n",
        "\n",
        "\n",
        "# def estimate_training_time_before_run(config):\n",
        "#     \"\"\"\n",
        "#     Main function to estimate training time before starting training\n",
        "#     Place this before your main training loop\n",
        "#     \"\"\"\n",
        "\n",
        "#     # Initialize estimator\n",
        "#     estimator = TrainingTimeEstimator(config)\n",
        "\n",
        "#     # Define dataset sizes (you'll need to update these with actual sizes)\n",
        "#     # You can get these from your dataset objects\n",
        "#     dataset_sizes = {\n",
        "#         'flood_train': 1445,  # Update with actual FloodNet training size\n",
        "#         'flood_val': 450,     # Update with actual FloodNet validation size\n",
        "#         'damage_train': 4500, # Update with actual RescueNet training size\n",
        "#         'damage_val': 1000,   # Update with actual RescueNet validation size\n",
        "#     }\n",
        "\n",
        "#     # Get time estimates\n",
        "#     report = estimator.estimate_training_time(\n",
        "#         dataset_sizes=dataset_sizes,\n",
        "#         sample_batch=True  # Set to False to skip sample batch (faster but less accurate)\n",
        "#     )\n",
        "\n",
        "#     # Display summary\n",
        "#     print(f\"\\n\" + \"=\"*60)\n",
        "#     print(\"TRAINING TIME ESTIMATE SUMMARY\")\n",
        "#     print(\"=\"*60)\n",
        "#     print(f\"Estimated Training Time: {report['summary']['total_time_formatted']}\")\n",
        "#     print(f\"Estimated Cost: ${report['summary']['estimated_cost_usd']:.2f}\")\n",
        "#     print(f\"Time per Epoch: {report['performance']['epoch_time_minutes']:.1f} minutes\")\n",
        "#     print(f\"GPU Memory Usage: {report['resources']['estimated_gpu_memory_gb']:.1f} GB\")\n",
        "\n",
        "#     print(f\"\\n RECOMMENDATIONS:\")\n",
        "#     for rec in report['recommendations']:\n",
        "#         print(f\"  {rec}\")\n",
        "\n",
        "#     # Ask user for confirmation\n",
        "#     print(f\"\\n\" + \"=\"*60)\n",
        "#     response = input(\"Continue with training? (y/n): \").lower().strip()\n",
        "\n",
        "#     if response != 'y':\n",
        "#         print(\"Training cancelled by user.\")\n",
        "#         return False, report\n",
        "\n",
        "#     print(\"Proceeding with training...\")\n",
        "#     return True, report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XTkBezz8aUVJ"
      },
      "outputs": [],
      "source": [
        "# if __name__ == \"__main__\":\n",
        "#     config = Config()\n",
        "\n",
        "#     # NEW: Estimate training time first\n",
        "#     should_continue, time_report = estimate_training_time_before_run(config)\n",
        "\n",
        "#     if should_continue:\n",
        "#         model, trainer = train_complete_model(config)  # Your existing code\n",
        "#     else:\n",
        "#         print(\"Training cancelled. Adjust config and try again.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqiyYmzSXYRG"
      },
      "source": [
        "## Initiate Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uo4oz6kitqtH",
        "outputId": "2ea0c51e-11df-428a-98dc-eeb6ab91f2f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "GPU: NVIDIA A100-SXM4-80GB\n",
            "Memory: 85.2 GB\n",
            "\n",
            "Creating datasets...\n",
            "FloodNet dataset: 1445 images, 1445 with masks\n",
            "FloodNet dataset: 450 images, 450 with masks\n",
            "FloodNet dataset: 448 images, 448 with masks\n",
            "Prioritized 158 samples by disaster content likelihood\n",
            "Preloading top 100 disaster samples to A100 GPU memory...\n"
          ]
        }
      ],
      "source": [
        "if __name__ == '__main__':\n",
        "  config = Config()\n",
        "  model, trainer = train_complete_model(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NP1v1bOYXNVK"
      },
      "source": [
        "##Model Demonstration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9W7AkS77xDtf"
      },
      "outputs": [],
      "source": [
        "# def test_on_new_image_with_tiff(model_path, image_path):\n",
        "#     import torch\n",
        "#     import cv2\n",
        "#     import numpy as np\n",
        "#     import matplotlib.pyplot as plt\n",
        "#     from PIL import Image\n",
        "#     import os\n",
        "\n",
        "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "#     try:\n",
        "#         # Load model\n",
        "#         checkpoint = torch.load(model_path, map_location=device, weights_only=False)\n",
        "\n",
        "#         model = EnhancedDisasterModel(\n",
        "#             num_classes_flood=2,\n",
        "#             num_classes_damage=4,\n",
        "#             backbone='resnet101'\n",
        "#         )\n",
        "#         model.load_state_dict(checkpoint['model_state_dict'])\n",
        "#         model.to(device)\n",
        "#         model.eval()\n",
        "\n",
        "#         # Check file extension and load accordingly\n",
        "#         file_ext = os.path.splitext(image_path)[1].lower()\n",
        "#         print(f\"   Processing {file_ext} file: {os.path.basename(image_path)}\")\n",
        "\n",
        "#         if file_ext in ['.tif', '.tiff']:\n",
        "#             # Use PIL for TIFF files (better TIFF support)\n",
        "#             print(\"   Using PIL for TIFF loading...\")\n",
        "#             try:\n",
        "#                 image_pil = Image.open(image_path)\n",
        "\n",
        "#                 # Handle different TIFF modes\n",
        "#                 if image_pil.mode == 'RGB':\n",
        "#                     image = np.array(image_pil)\n",
        "#                 elif image_pil.mode == 'RGBA':\n",
        "#                     # Convert RGBA to RGB\n",
        "#                     image = np.array(image_pil.convert('RGB'))\n",
        "#                 elif image_pil.mode in ['L', 'P']:\n",
        "#                     # Convert grayscale or palette to RGB\n",
        "#                     image = np.array(image_pil.convert('RGB'))\n",
        "#                 elif image_pil.mode == 'I' or image_pil.mode == 'F':\n",
        "#                     # Handle 32-bit integer or float images\n",
        "#                     arr = np.array(image_pil)\n",
        "#                     # Normalize to 0-255 range\n",
        "#                     arr = ((arr - arr.min()) / (arr.max() - arr.min()) * 255).astype(np.uint8)\n",
        "#                     image = np.stack([arr, arr, arr], axis=-1)  # Convert to RGB\n",
        "#                 else:\n",
        "#                     print(f\"   Converting from mode {image_pil.mode} to RGB\")\n",
        "#                     image = np.array(image_pil.convert('RGB'))\n",
        "\n",
        "#                 print(f\"   TIFF image loaded: {image.shape}, dtype: {image.dtype}\")\n",
        "\n",
        "#             except Exception as e:\n",
        "#                 print(f\"   PIL failed, trying OpenCV for TIFF: {e}\")\n",
        "#                 # Fallback to OpenCV\n",
        "#                 image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
        "#                 if image is None:\n",
        "#                     raise ValueError(f\"Could not load TIFF image from {image_path}\")\n",
        "#                 image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "#         else:\n",
        "#             # Use OpenCV for standard formats (PNG, JPG, JPEG)\n",
        "#             print(\"   Using OpenCV for standard image loading...\")\n",
        "#             image = cv2.imread(image_path)\n",
        "#             if image is None:\n",
        "#                 raise ValueError(f\"Could not load image from {image_path}\")\n",
        "#             image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "#         print(f\"   Final image shape: {image.shape}, dtype: {image.dtype}\")\n",
        "\n",
        "#         # Ensure image is in correct format (0-255, uint8)\n",
        "#         if image.dtype != np.uint8:\n",
        "#             if image.max() <= 1.0:\n",
        "#                 # Image is in 0-1 range, convert to 0-255\n",
        "#                 image = (image * 255).astype(np.uint8)\n",
        "#             else:\n",
        "#                 # Image might be in different range, normalize\n",
        "#                 image = ((image - image.min()) / (image.max() - image.min()) * 255).astype(np.uint8)\n",
        "\n",
        "#         # Apply transforms\n",
        "#         transform = get_validation_augmentation()\n",
        "#         input_tensor = transform(image=image)['image']\n",
        "#         input_tensor = input_tensor.unsqueeze(0).to(device)\n",
        "\n",
        "#         # Make predictions\n",
        "#         with torch.no_grad():\n",
        "#             flood_out, damage_out = model(input_tensor)\n",
        "\n",
        "#         flood_pred = torch.argmax(flood_out, dim=1)[0].cpu().numpy()\n",
        "#         damage_pred = torch.argmax(damage_out, dim=1)[0].cpu().numpy()\n",
        "\n",
        "#         # Visualize results\n",
        "#         fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "#         axes[0].imshow(image)\n",
        "#         axes[0].set_title(f'Original Image ({file_ext})')\n",
        "#         axes[0].axis('off')\n",
        "\n",
        "#         axes[1].imshow(flood_pred, cmap='Blues')\n",
        "#         axes[1].set_title('Flood Prediction')\n",
        "#         axes[1].axis('off')\n",
        "\n",
        "#         axes[2].imshow(damage_pred, cmap='Reds')\n",
        "#         axes[2].set_title('Damage Prediction')\n",
        "#         axes[2].axis('off')\n",
        "\n",
        "#         plt.tight_layout()\n",
        "#         plt.show()\n",
        "\n",
        "#         return flood_pred, damage_pred\n",
        "\n",
        "#     except Exception as e:\n",
        "#         print(f\"   ERROR processing {image_path}: {str(e)}\")\n",
        "#         return None, None\n",
        "\n",
        "\n",
        "# import os\n",
        "# import random\n",
        "\n",
        "# def test_on_images_with_tiff(model_path, folder_path, num_img=10):\n",
        "#     \"\"\"\n",
        "#     Test the disaster model on multiple images including TIFF files.\n",
        "\n",
        "#     Supported formats: PNG, JPG, JPEG, TIF, TIFF\n",
        "#     \"\"\"\n",
        "#     try:\n",
        "#         # Updated file extension list to include TIFF\n",
        "#         image_extensions = (\".png\", \".jpg\", \".jpeg\", \".tif\", \".tiff\")\n",
        "#         image_files = [f for f in os.listdir(folder_path)\n",
        "#                       if f.lower().endswith(image_extensions)]\n",
        "\n",
        "#         if not image_files:\n",
        "#             print(f\"No supported image files found in {folder_path}\")\n",
        "#             print(f\"Looking for: {image_extensions}\")\n",
        "#             all_files = os.listdir(folder_path)[:10]  # Show first 10 files\n",
        "#             print(f\"Files in folder: {all_files}\")\n",
        "#             return []\n",
        "\n",
        "#         print(f\"Found {len(image_files)} supported images\")\n",
        "\n",
        "#         # Group by file type for info\n",
        "#         file_types = {}\n",
        "#         for f in image_files:\n",
        "#             ext = os.path.splitext(f)[1].lower()\n",
        "#             file_types[ext] = file_types.get(ext, 0) + 1\n",
        "\n",
        "#         print(f\"File type breakdown: {file_types}\")\n",
        "\n",
        "#         # Sample random images\n",
        "#         sample_files = random.sample(image_files, min(num_img, len(image_files)))\n",
        "#         print(f\"Processing {len(sample_files)} images...\")\n",
        "\n",
        "#         results = []\n",
        "#         successful_predictions = 0\n",
        "\n",
        "#         for i, file_name in enumerate(sample_files, 1):\n",
        "#             file_path = os.path.join(folder_path, file_name)\n",
        "#             print(f\"\\n[{i}/{len(sample_files)}] Processing: {file_name}\")\n",
        "\n",
        "#             flood_pred, damage_pred = test_on_new_image_with_tiff(model_path, file_path)\n",
        "\n",
        "#             if flood_pred is not None and damage_pred is not None:\n",
        "#                 results.append({\n",
        "#                     \"file\": file_name,\n",
        "#                     \"flood_mask\": flood_pred,\n",
        "#                     \"damage_mask\": damage_pred\n",
        "#                 })\n",
        "#                 successful_predictions += 1\n",
        "#                 print(f\"   Success!\")\n",
        "#             else:\n",
        "#                 print(f\"   Failed\")\n",
        "\n",
        "#         print(f\"\\nCompleted: {successful_predictions}/{len(sample_files)} images processed successfully\")\n",
        "#         return results\n",
        "\n",
        "#     except Exception as e:\n",
        "#         print(f\"Error in test_on_images_with_tiff: {str(e)}\")\n",
        "#         return []\n",
        "\n",
        "\n",
        "# # Quick check function to see what file types are in your folder\n",
        "# def check_file_types(folder_path):\n",
        "#     \"\"\"Check what file types are present in the folder\"\"\"\n",
        "#     print(f\"=== File Analysis for: {folder_path} ===\")\n",
        "\n",
        "#     if not os.path.exists(folder_path):\n",
        "#         print(f\"Folder does not exist!\")\n",
        "#         return\n",
        "\n",
        "#     all_files = os.listdir(folder_path)\n",
        "#     print(f\"Total files: {len(all_files)}\")\n",
        "\n",
        "#     # Group by extension\n",
        "#     extensions = {}\n",
        "#     for f in all_files:\n",
        "#         ext = os.path.splitext(f)[1].lower()\n",
        "#         if not ext:\n",
        "#             ext = \"(no extension)\"\n",
        "#         extensions[ext] = extensions.get(ext, 0) + 1\n",
        "\n",
        "#     print(f\"\\nFile extensions found:\")\n",
        "#     for ext, count in sorted(extensions.items()):\n",
        "#         print(f\"  {ext}: {count} files\")\n",
        "\n",
        "#     # Check for TIFF specifically\n",
        "#     tiff_files = [f for f in all_files if f.lower().endswith(('.tif', '.tiff'))]\n",
        "#     if tiff_files:\n",
        "#         print(f\"\\nTIFF files found: {len(tiff_files)}\")\n",
        "#         print(f\"First few TIFF files: {tiff_files[:5]}\")\n",
        "#     else:\n",
        "#         print(f\"\\nNo TIFF files found.\")\n",
        "\n",
        "\n",
        "# # Usage example\n",
        "# if __name__ == \"__main__\":\n",
        "#     model_path = \"/content/checkpoints/best_model.pth\"\n",
        "#     folder_path = \"/content/drive/MyDrive/G.E.M.S./FloodNet/FloodNet-Supervised_v1.0/test/test-label-img/\"\n",
        "\n",
        "#     # First check what file types you have\n",
        "#     check_file_types(folder_path)\n",
        "\n",
        "#     # Then run with TIFF support\n",
        "#     if os.path.exists(model_path) and os.path.exists(folder_path):\n",
        "#         results = test_on_images_with_tiff(model_path, folder_path, num_img=10)\n",
        "#         print(f\"Processing complete. Got results for {len(results)} images.\")\n",
        "#     else:\n",
        "#         print(\"Missing model or folder path\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "SgqmnrpqkY_w"
      },
      "outputs": [],
      "source": [
        "#!pip install nbconvert\n",
        "#!jupyter nbconvert --to html Hermes.0.3.6.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ZslDks6aWCEq"
      },
      "outputs": [],
      "source": [
        "# print(f\"Flood pixels: {(flood_mask == 1).sum()}\")\n",
        "# print(f\"Damage distribution: {np.bincount(damage_mask.flatten())}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}